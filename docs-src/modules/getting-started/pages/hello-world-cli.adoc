= Hello World (CLI)

== Installation

To use Kaskada on the command line, you'll need to install three components:

* The Kaskada command-line executable
* The Kaskada manager, which serves the Kaskada API
* The Kaskada engine, which executes queries

Each Kaskada release has pre-compiled binaries for each component. 
You can visit the https://github.com/kaskada-ai/kaskada/releases[Releases] page on Github to obtain the latest Kaskada release version binaries for your platform.
The example commands below will download the latest Kaskada binaries and applies to Linux and OSX.


[source,bash]
----
curl -s https://api.github.com/repos/kaskada-ai/kaskada/releases/latest |\
grep "browser_download_url.*" |\
grep $(uname -m | sed 's/x86_64/amd64/') |\
grep $(uname -s | tr '[:upper:]' '[:lower:]') |\
cut -d ':' -f2,3 |\ 
tr -d \" |\ 
xargs -I {} sh -c 'curl -L {} -o $(basename {}| cut -d '-' -f1,2)'

chmod +x kaskada-*
----

Print a colon-separated list of the directories in your `PATH`.

[source,bash]
----
echo PATH
----

Move the Kaskada binaries to one of the listed locations. 
This command assumes that the binaries are currently in your working directory and that your `PATH`` includes `/usr/local/bin`, but you can customize it if your locations are different.

[source,bash]
----
mv kaskada-* /usr/local/bin/
----

[TIP]
.Authorizing applications on OSX
====
If you're using OSX, you may need to unblock the applications.
OSX prevents applications you download from running as a security feature.
You can remove the block placed on the file when it was downloaded with the following command:

[source,bash]
----
xattr -dr com.apple.quarantine <path to file>
----
====

You can start a local instance of the Kaskada service by running the manager and engine:

[source,bash]
----
kaskada-manager 2>&1 > manager.log 2>&1 &
kaskada-engine serve > engine.log 2>&1 &
----

[TIP]
.Allowing services to listen on OSX
====
When using OSX, you may need to allow these services to create an API listener the first time you run these commands.
This is normal, and indicates the services are working as expected - the API allows services to communicate between themselves.
====

To verify they're installed correctly and executable, try running the following command (which lists any resources you've created):

[source,bash]
----
kaskada-cli sync export --all
----

You should see output similar to the following:

[source,bash]
----
10:18AM INF starting export
{}  
10:18AM INF Success!
----


== Managing resources

The CLI manages Kaskada resources declaratively by managing spec files. 
A spec file is a YAML file describing a set of Kaskada resources, for examples tables and views.

We'll begin by creating a table.
The first step is to create a spec file containing the table's definition.

Every table is associated with a schema which defines the structure of each event in the table.
Schemas are inferred from the data you load into a table, however, some columns are required by Kaskada's data model.
Every table must include a column identifying the xref:fenl:temporal-aggregation.adoc[time] and xref:fenl:entities.adoc[entity] associated with each row. 

When creating a table, you must tell Kaskada which columns contain the time and entity of each row:

* The xref:fenl:temporal-aggregation.adoc[time] column is specified using the `time_column_name` parameter.
  This parameter must identify a column name in the table's data which contains time values.
  The time should refer to when the event occurred.
* The xref:fenl:entities.adoc[entity] key is specified using the `entity_key_column_name` parameter.
  This parameter must identify a column name in the table's data which contains the entity key value.
  The entity key should identify a _thing_ in the world that each event is associated with.
  Don't worry too much about picking the "right" value - it's easy to change the entity using the xref:fenl:catalog.adoc#with-key[`with_key()`] function.

For more information about configuring tables, see xref:developing:tables.adoc#creating-a-table[].
For more information about the expected structure of input files, see xref:ROOT:loading-data.adoc#file-format[Expected File Format]

[source,yaml]
.spec.yaml
----
tables:
  # The name of the table
- tableName: Purchase               

  # A field containing the time associated with each event
  timeColumnName: purchase_time     

  # An initial entity key associated with each event
  entityKeyColumnName: customer_id  

  # Where the table's data will be stored
  # The default storage location is 'kaskada', and uses local files to store events.
  source:                          
    kaskada: {}
----

To create this table, we must sync the state of the Kaskada service with the contents of the file.

[source,bash]
----
./kaskada-cli sync apply -f spec.yaml

# > 2:18PM INF starting plan
# > 2:18PM INF resource not found on system, will create it kind=*kaskadav1alpha.Table name=GamePlay
# > 2:18PM INF resource not found on system, will create it kind=*kaskadav1alpha.Table name=Purchase
# > 2:18PM INF Success!
----

This creates a table named `Purchase`. Any data loaded into this table
must have a timestamp field named `purchase_time`, a field named
`customer_id`, and a field named `subsort_id`.

[TIP]
.Idiomatic Kaskada
====
We like to use CamelCase to name tables because it
helps distinguish data sources from transformed values and function
names.
====

You've now created your first table! 

== Loading data into a table

Now that we've created a table, we're ready to load some data into it.

[IMPORTANT]
====
A table must be xref:#creating-a-table[created] before data can be loaded into it.
====

Data can be loaded into a table in multiple ways. In this example we'll
load the contents of a Parquet file into the table. To learn about
the different ways data can be loaded into a table, see the
xref:developing:tables.adoc#uploading-data["Uploading Data"
section of the "Tables"] page.

[source,bash]
----
# Download a file to load and save it to path 'purchase.parquet'
curl -L "https://drive.google.com/uc?export=download&id=1SLdIw9uc0RGHY-eKzS30UBhN0NJtslkk" -o purchase.parquet

# Load the file into the Purchase table (which was created in the previous step)
./kaskada-cli table load \
    --table Purchase \
    --file-path file://${PWD}/purchase.parquet
----

The file's content is added to the table.

For more help with tables and loading data, see xref:developing:tables.adoc[Reference -
Tables]

== Querying data

You can write queries in a number of ways with Kaskada. As you are
iterating it can be helpful to build up your queries as components
as you go.  Once you'd like to persist a query, check out our
article on sharing queries with xref:developing:views.adoc[Views].

Let's start by looking at the Purchase table without any filters.
Begin by creating a text file with the following query:

[source,Fenl]
.query.fenl
----
Purchase
----

This query will return all of the columns and rows contained in a table.
Run it by sending the query to `kaskada-cli query run`:

[source,bash]
----
cat query.fenl | ./kaskada-cli query run --stdout --response-as csv
----

[NOTE]
====
This table is intentionally small so that you can get to know queries with Kaskada. 
When working with larger data sets, you may want to write the results to a file rather than `stdout` (by omitting the `--stdout` flag).
====

Let's walk through this command.

1. We begin by using the CLI to run a query: `kaskada-cli query run`.
CLI commands are organized into groups like `query`, `load`, and `sync` - each group contains a set of related sub-commands.
2. In order to see the results of our query, we used the command flag `--stdout`; the default behavior is to write results to a file and return a JSON object describing the result of the query.
3. Finally, we specified how to encode the results with the flag `--response-as csv`.
CSV is a good format for writing results to STDOUT because it's relatively human-readible. The default is to encode results in Parquet, which is a much more efficient encoding for larger datasets.

It can be helpful to limit your results to a single entity.
This makes it easier to see how a single entity changes over time.

[source,Fenl]
.query.fenl
----
Purchase | when(Purchase.customer_id == "patrick")
----

In this example, we build a pipeline of functions using the `|` character.
We begin with the timeline produced by the table `Purchase`, then filter it to the set of times where the purchase's customer is `"patrick"` using the `xref:fenl:catalog.adoc#when[when()]` function.
Execute the query using the CLI as before.

[source,bash]
----
cat query.fenl | kaskada-cli query run --stdout --response-as csv
----

As you begin to better understand your data you can start using
aggregations over your data such as the `max()` function:

[source,Fenl]
.query.fenl
----
{ max_purchase: Purchase.amount | max() } 
| when(Purchase.customer_id == "patrick")
----

Here we begin by constructing a record with a single field named `max_purchase` using curly brackets (i.e., `{key: value, key2: value2}`).
The timeline describing how this record changes is filtered to the times associated with Patrick's purchases.
Execute the query using the CLI as before.

[source,bash]
----
cat query.fenl | kaskada-cli query run --stdout --response-as csv
----


[IMPORTANT]
====
These results may be surprising if you were expecting a single value,
this is a feature, not a bug!

Computations in Fenl are temporal: they produce a time-series of values
describing the full history of a computation's results. Temporal
computation allows Fenl to capture what an expression's value would have
been at arbitrary times in the past.

Fenl values can time-travel forward through time. Time travel allows
combining the result of different computations at different points in
time. Because values can only travel forward in time, Fenl prevents
information about the future from "leaking" into the past.

Read more in the xref:fenl:fenl-quick-start.adoc[Fenl
Language Guide]
====

Now we can start building up more complex queries. To reduce the set of columns
output in your query, you can define a record with the curly braces
`{ }` and name the columns with a label shown on the left of the `:` in
the below query. In order to debug your features, we recommend including
the time and the entity with each query so that you can walk through the
results in time:

[source,Fenl]
.query.fenl
----
{
    time: Purchase.purchase_time,
    entity: Purchase.customer_id,
    max_amount: Purchase.amount | max(),
    min_amount: Purchase.amount | min(),
} | when(Purchase.customer_id == "patrick")
----

This example adds fields to the record using the `min` and `max` aggregations.
Execute the query using the CLI as before.

[source,bash]
----
cat query.fenl | kaskada-cli query run --stdout --response-as csv
----

For more help writing queries, see xref:developing:queries.adoc[Reference -
Writing Queries]

== Cleaning up

When you're done with this tutorial, you can delete the table you created in order to free up resources.

[source,bash]
----
# Delete the Purchase table (which was created in the previous step)
kaskada-cli table delete --table Purchase
----

The table is deleted and any files loaded into it are removed from the system.

For more help with tables, see xref:developing:tables.adoc[Reference -
Tables]