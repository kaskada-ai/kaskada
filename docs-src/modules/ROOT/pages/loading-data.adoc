= Loading Data into a Table

Kaskada stores data in _tables_. Tables consist of multiple rows, and
each row is a value of the same type.
The xref:developing:tables.adoc[Tables] section of the reference docs has more information about managing tables.

[IMPORTANT]
====
A table must be created before data can be loaded into it.
====

== File Format

Any data loaded into a table must include all columns used in the table definition.
The full schema of a table is inferred from the data loaded into it.
At the moment, all data loaded into a table must have the same schema.

Additionally, it expects the following:

* The `time_column` (and all other date-time columns in your dataset)
should be a type that can be xref:fenl:data-model.adoc#type-coercion[cast] to a xref:fenl:data-model.adoc#scalars[timestamp], for example an integer or RFC3330-formatted string.
* If a subsort column is defined, the combination of the entity key column, time column and subsort column should guarantee that each row is unique.  

=== Loading data from Parquet files into a table

Events stored as a Parquet file can be loaded into a table by providing the file's xref:#File Location[location].
The given location must be accessible to the Kaskada instance you wish to load the file into.

.Loading a Parquet file using Python
[source,python]
----
from kaskada import table
from kaskada.api.session import LocalBuilder

session = LocalBuilder().build()

table.load(
  table_name = "Purchase",
  file = "/path/to/file.parquet", 
)
----

.Loading a Parquet file using the CLI
[source,bash]
----
kaskada-cli load \
    --table Purchase \
    --file-path file:///path/to/file.parquet \
    --file-type parquet
----

=== Loading data from CSV files into a table

Events stored as a CSV file can be loaded into a table by providing the file's xref:#File Location[location].
The given location must be accessible to the Kaskada instance you wish to load the file into.
CSV files must include a header row, which is used to infer the file's schema.

CSV type inference occurs when data is loaded.
The schema is inferred by reading the first 1000 rows.

.Loading a CSV file using Python
[source,python]
----
from kaskada import table
from kaskada.api.session import LocalBuilder

session = LocalBuilder().build()

table.load(
  table_name = "Purchase",
  file = "/path/to/file.csv", 
)
----

.Loading a CSV file using the CLI
[source,bash]
----
kaskada-cli load \
    --table Purchase \
    --file-path file:///path/to/file.csv \
    --file-type csv
----

== File Location

Kaskada supports loading files from different kinds of file storage.
The file storage system is specified with the file path protocol prefix, for example `file:` or `s3:`.

=== Local Storage

Data can be loaded from the local disk using the `file:` protocol.
When loading from disk, the file path must identify a file accessible to the Kaskada service.
Local storage is not reccommended when using a remote Kaskada service.

=== AWS S3 Storage

Data can be loaded from AWS S3 (or compatible stores such as Minio) using the `s3:` protocol.
When loading non-public objects from S3, the Kaskada service must be configured with credentials.
Credentials are configured using environment variables.

The following environment variables are used to configure credentials:

* `AWS_ACCESS_KEY_ID`: AWS credential key.
* `AWS_SECRET_ACCESS_KEY`: AWS credential secret.
* `AWS_DEFAULT_REGION`: The AWS S3 region to use if not specified.
* `AWS_ENDPOINT`: The S3 endpoint to connect to.
* `AWS_SESSION_TOKEN`: The session token. Session tokens are required for credentials created by assuming an IAM role.
* `AWS_CONTAINER_CREDENTIALS_RELATIVE_URI`: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html
* `AWS_ALLOW_HTTP`: Set to “true” to permit HTTP connections without TLS

== Loading data into a table with Python

Data can be loaded into a table using the `table.load()` function

[source,python]
----
from kaskada import table
from kaskada.api.session import LocalBuilder

session = LocalBuilder().build()

fullPathToFile = "/content/drive/place/thing/purchases.parquet"
table.load(table_name = "Purchases", file = fullPathToFile)
----

[TIP]
.Required format
The files must be in a specific format. 
See xref:reference:expected-file-format[Expected File Format] for details.

This loads the contents of the file to the Purchases table.

The result of running `load` is a `data_token_id`. 
The data token ID is a unique reference to the data currently stored in the system. 
Data tokens enable repeatable queries: queries performed against the same data token always run on the same input data.


[source,bash]
----
data_token_id: "aa2***a6b9"
----

== Loading data into a table with the CLI

Files can be loaded into a table using the CLI.
When loading files with the CLI, the given path must be local to the Kaskada service.

The file must be encoded as either CSV or Parquet.

[source,bash]
----
kaskada-cli load \
  --table Purchase \
  --file-type csv \
  --file-path file://path/to/purchases.csv
----

The result of running `cli load` is a `data_token_id``. The data token ID is a unique reference to the data currently stored in the system.

[source,bash]
----
data_token_id: "aa2***a6b9"
----