{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a20a51f",
   "metadata": {
    "id": "5a20a51f"
   },
   "source": [
    "# Kaskada Demo\n",
    "## Let's use a common ML challenge: Customer Retention. \n",
    "\n",
    "Companies collect massive amounts of data using platforms like Splunk, Heap, Segment, or even basic event logs describing user behavior. How can you use this data to predict user retention, revenue targets, and identify which customers are likely to be the most successful?\n",
    "\n",
    "Then how do we make that information available to the customer success reps to attempt to save accounts, to sales reps to help predict if a new customer might be successful, and to revenue leaders to predict quarterly and annual revenue targets.\n",
    "\n",
    "**Note**: Due to the terms and conditions by which the data used in this notebook is made avaialble, anyone interested in recreating this work will need to download the files from Kaggle and follow the instructions below to create your own Kaskada account and upload the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BJ2EE9mSGtGB",
   "metadata": {
    "id": "BJ2EE9mSGtGB"
   },
   "source": [
    "# Step 1: Setup Kaskada Client\n",
    "\n",
    "## Session Builder\n",
    "The next version of Kaskada will use an API Session Builder to follow closely to PySpark's approach to local connections.\n",
    "\n",
    "###  Local Session Builder\n",
    "The default local session builder (`LocalBuilder`) by default assumes:\n",
    "* Endpoint: `localhost:50051` for the API server\n",
    "* Is Secure: `False`\n",
    "* Will spin up the API server and Compute Server binaries.\n",
    "  * Assumes Kaskada root is **~/.cache/kaskada**. Override by setting *KASKADA_PATH*\n",
    "  * Assumes the binaries are stored in *KASKADA_PATH/bin*. Override by setting *KASKADA_BIN_PATH* (default is bin)\n",
    "  * Assumes the logs are stored in *KASKADA_PATH/logs*. Override by setting *KASKADA_LOG_PATH* (default is logs)\n",
    "  \n",
    "Most people running locally will want to spin up the server locally by just using: `LocalBuilder().build()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d818e1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaskada.api.release as release\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[release.ReleaseClient.GITHUB_ACCESS_TOKEN_ENV] = getpass(prompt='Github Access Token:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37db47ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaskada.api.session import LocalBuilder\n",
    "\n",
    "session = LocalBuilder().build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d10uDMpYOd",
   "metadata": {
    "id": "a0d10uDMpYOd"
   },
   "source": [
    "# Step 2: Prepare the data\n",
    "Download the data and agree to the terms and conditions of this [research prediction competition](https://www.kaggle.com/c/kkbox-churn-prediction-challenge/data). \n",
    "\n",
    "The files you'll need are titled:\n",
    "\n",
    "\n",
    "*   user_logs_v2.csv.7z\n",
    "*   transactions.csv.7z\n",
    "*   members_v3.csv.7z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8435f9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r19vCeE4kta0",
   "metadata": {
    "id": "r19vCeE4kta0"
   },
   "source": [
    "Now we're ready to create a table for the data and load it into Kaskada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6559db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaskada import table\n",
    "\n",
    "for t in table.list_tables().tables:\n",
    "    table.delete_table(t.table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QFvMhEthlMFq",
   "metadata": {
    "id": "QFvMhEthlMFq"
   },
   "outputs": [],
   "source": [
    "# Create Transaction table and load data into it\n",
    "table.create_table(\n",
    "    table_name = \"Transaction\",\n",
    "    time_column_name = \"transaction_date\",\n",
    "    entity_key_column_name = \"msno\",\n",
    "    grouping_id=\"User\"\n",
    ")\n",
    "table.load(\"Transaction\", \"kkbox-churn-prediction-challenge/transactions.csv\")\n",
    "\n",
    "# Create Member table and load data into it\n",
    "table.create_table(\n",
    "    table_name = \"Member\",\n",
    "    time_column_name = \"registration_init_time\",\n",
    "    entity_key_column_name = \"msno\",\n",
    "    grouping_id=\"User\"\n",
    ")\n",
    "table.load(\"Member\", \"kkbox-churn-prediction-challenge/members_v3.csv\")\n",
    "\n",
    "table.list_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3939a7-1e62-4ed1-9dd2-468d14ba71c1",
   "metadata": {
    "id": "3a3939a7-1e62-4ed1-9dd2-468d14ba71c1"
   },
   "source": [
    "# Step 3: Feature Engineering\n",
    "## Data scientists love Jupyter\n",
    "### With Kaskada's python library and FENL magic they can keep all their favorite parts\n",
    "- Iterative exploration, drill down and manipulation of data \n",
    "- Data cleaning\n",
    "- Statistical modeling\n",
    "- Visualization story telling with words, graphs and code\n",
    "- Training ML models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8a2d0d",
   "metadata": {
    "id": "ab8a2d0d"
   },
   "source": [
    "### Iterative exploration, drill down and manipulation of data with Kaskada\n",
    "\n",
    "#### Connect your event-based data directly to Kaskada\n",
    "\n",
    "Let's take a look at the transaction events and membership information associated with a single member to understand the columns available, `msno=LWekcgcnUIqi22v63xuIMX4GYbxapmPMoDnLMVLFSTs=`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c855846",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext fenlmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0a2f4a-013a-46a8-ba85-9d851c157c33",
   "metadata": {
    "id": "9b0a2f4a-013a-46a8-ba85-9d851c157c33"
   },
   "outputs": [],
   "source": [
    "%%fenl\n",
    "Transaction | when(Transaction.msno == \"LWekcgcnUIqi22v63xuIMX4GYbxapmPMoDnLMVLFSTs=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "epnDDH_nEMGF",
   "metadata": {
    "id": "epnDDH_nEMGF"
   },
   "outputs": [],
   "source": [
    "%%fenl\n",
    "Member | when(Member.msno == \"LWekcgcnUIqi22v63xuIMX4GYbxapmPMoDnLMVLFSTs=\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd446942-2f84-4a04-9e5f-a22487ab0efc",
   "metadata": {
    "id": "dd446942-2f84-4a04-9e5f-a22487ab0efc"
   },
   "source": [
    "### Data Cleaning and Visualizing with Kaskada\n",
    "Visualizations help to not miss the forest (distribution) for the trees (individual data points). Jupyter allows for the crafting of visualizations, that can then be used to inform the decisions being made by the data scientist in regards to further feature engineering and selection.\n",
    "\n",
    "#### With Kaskada Data Scientists can define features directly from the event based data even when:\n",
    "\n",
    "- The transaction log is quite busy, often with multiple entries recorded on given transaction date (transaction_date) as can be observed for this customer on several dates including 2016-09-25. \n",
    "- Some records have a value of zero or less for payment plan days (payment_plan_days) \n",
    "- Many transaction entries are changing the subscription's expiration date (membership_expire_date). \n",
    "- There are backdated records due to some kind of subscription management activity such as a change in auto-renewal status or the like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a116c35d-f9b8-41b5-a69c-0a62940ddf9d",
   "metadata": {
    "id": "a116c35d-f9b8-41b5-a69c-0a62940ddf9d"
   },
   "outputs": [],
   "source": [
    "%%fenl --var df_explore\n",
    "\n",
    "{\n",
    "    payment_plan_days: Transaction.payment_plan_days,\n",
    "    payment_method_id: Transaction.payment_method_id,\n",
    "    trans_at: Transaction.transaction_date,\n",
    "    membership_expire_date: Transaction.membership_expire_date,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea682d62-a8e7-4eac-95e1-d703d80e8318",
   "metadata": {
    "id": "ea682d62-a8e7-4eac-95e1-d703d80e8318"
   },
   "outputs": [],
   "source": [
    "df_explore.dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bedba85-d567-452e-94d0-db19d7bc1880",
   "metadata": {
    "id": "5bedba85-d567-452e-94d0-db19d7bc1880"
   },
   "outputs": [],
   "source": [
    "plt.hist(df_explore.dataframe.payment_plan_days, bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fde0f7f-0ed3-4f12-b17b-8faf856c70fd",
   "metadata": {
    "id": "3fde0f7f-0ed3-4f12-b17b-8faf856c70fd",
    "tags": []
   },
   "source": [
    "We can simply add logic around these events to select the correct examples for prediction time and label time. But first let's try and see if this gets us what we want:\n",
    "\n",
    "- Eliminate 0 or fewer plan days\n",
    "- Select the maximum expiration date\n",
    "- Handle backdated records\n",
    "- Complete a query over all transactions to see how many we have left. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2234c8-bf8b-49f9-89da-9259a51d3dcc",
   "metadata": {
    "id": "ff2234c8-bf8b-49f9-89da-9259a51d3dcc"
   },
   "outputs": [],
   "source": [
    "%%fenl\n",
    "\n",
    "# 1. Data Cleaning\n",
    "\n",
    "let meaningful_txns = Transaction | if(Transaction.payment_plan_days > 0)\n",
    "        \n",
    "let max_expires_at = max(meaningful_txns.membership_expire_date)\n",
    "let expiration_is_previous = (max_expires_at < meaningful_txns.transaction_date)\n",
    "        \n",
    "let subscription_expires_at =  max_expires_at | if(not(expiration_is_previous)) | else(meaningful_txns.transaction_date)\n",
    "\n",
    "in {\n",
    "    payment_plan_days: meaningful_txns.payment_plan_days,\n",
    "    payment_method_id: meaningful_txns.payment_method_id,\n",
    "    trans_at: meaningful_txns.transaction_date,\n",
    "    membership_expire_date: meaningful_txns.membership_expire_date,\n",
    "    expires_at: subscription_expires_at\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac92c9c-043c-434d-a57f-a77749380b27",
   "metadata": {
    "id": "bac92c9c-043c-434d-a57f-a77749380b27"
   },
   "outputs": [],
   "source": [
    "plt.hist(_.dataframe.payment_plan_days, bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba09e77-0fdf-43f4-960b-50a126262ec7",
   "metadata": {
    "id": "2ba09e77-0fdf-43f4-960b-50a126262ec7"
   },
   "source": [
    "The below example shows computing the target feature, churn at a data dependent prediction time plus 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XVOINrwqzFMT",
   "metadata": {
    "id": "XVOINrwqzFMT"
   },
   "outputs": [],
   "source": [
    "%%fenl --var df_training\n",
    "\n",
    "# 1. Data Cleaning\n",
    "\n",
    "let meaningful_txns = Transaction | if(Transaction.payment_plan_days > 0)\n",
    "        \n",
    "let max_expires_at = max(meaningful_txns.membership_expire_date)\n",
    "let expiration_is_previous = (max_expires_at < meaningful_txns.transaction_date)\n",
    "        \n",
    "let subscription_expires_at =  max_expires_at | if(not(expiration_is_previous)) | else(meaningful_txns.transaction_date)\n",
    "\n",
    "let cleaned_transactions = {\n",
    "    msno: meaningful_txns.msno,\n",
    "    payment_plan_days: meaningful_txns.payment_plan_days,\n",
    "    payment_method_id: meaningful_txns.payment_method_id,\n",
    "    trans_at: meaningful_txns.transaction_date,\n",
    "    membership_expire_date: meaningful_txns.membership_expire_date,\n",
    "    expires_at: subscription_expires_at\n",
    "}\n",
    "\n",
    "# 2. Churned Transactions\n",
    "\n",
    "let shifted_txn = cleaned_transactions \n",
    "    | shift_to($input.membership_expire_date | add_time(days(30)))\n",
    "\n",
    "let last_txn = last(cleaned_transactions)\n",
    "\n",
    "let membership_history = {\n",
    "    trans_at: shifted_txn.trans_at,\n",
    "    expires_at: shifted_txn.membership_expire_date,\n",
    "    churned: shifted_txn.trans_at == last_txn.trans_at,\n",
    "} | when(is_valid(shifted_txn))\n",
    "\n",
    "let initial_txn = membership_history.trans_at | first()\n",
    "let churn_txn = membership_history | if(membership_history.churned) | first()\n",
    "\n",
    "let churn_subscription = {\n",
    "    starts_at: initial_txn,\n",
    "    ends_at: churn_txn.trans_at,\n",
    "    churned: true\n",
    "} \n",
    "let active_subscription = {\n",
    "    starts_at: initial_txn,\n",
    "    ends_at: null,\n",
    "    churned: false\n",
    "}\n",
    "\n",
    "# 4. Features\n",
    "\n",
    "let current_subscription = churn_subscription | if(is_valid(churn_txn)) | else(active_subscription)\n",
    "\n",
    "let first_transaction = cleaned_transactions | first()\n",
    "\n",
    "in {\n",
    "    churned: current_subscription.churned,\n",
    "    duration_days: days_between(current_subscription.ends_at, current_subscription.starts_at) as i32,\n",
    "    payment_plan_days: first_transaction.payment_plan_days,\n",
    "    payment_method_id: first_transaction.payment_method_id,\n",
    "    registered_via: first(Member).registered_via | else(-1)\n",
    "} | when(is_valid(current_subscription))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293e31bc-dc5c-4530-ac1a-b998004d776a",
   "metadata": {
    "id": "293e31bc-dc5c-4530-ac1a-b998004d776a"
   },
   "source": [
    "## To summarize, a data scientist can with Kaskada compute feature values at arbitrary data dependent points in time, train a model and make feature values available in production for their data engineer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6924ca3e-28b3-4f93-b0cf-5f8afddc11d8",
   "metadata": {
    "id": "6924ca3e-28b3-4f93-b0cf-5f8afddc11d8"
   },
   "source": [
    "##### 1. Build predictor and target features with Kaskada\n",
    "\n",
    "1.   Write the predictors in a single record computed at their prediction time\n",
    "2.   Shift the features forward to label time and compute the label value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936700a9-e042-401c-9156-7bb18652e109",
   "metadata": {
    "id": "936700a9-e042-401c-9156-7bb18652e109"
   },
   "source": [
    "##### 2. Compute train and test sets by specifying model context with Prediction and Label Times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f5921d-36dc-41d1-a2a6-ae800b7a11de",
   "metadata": {
    "id": "08f5921d-36dc-41d1-a2a6-ae800b7a11de"
   },
   "source": [
    "##### 3. Train, score and compare models with your favorite libraries\n",
    "##### 4. Iterate and select final features and model, handoff "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8196fdc-0e01-46b8-b21a-428dd120fe56",
   "metadata": {
    "id": "a8196fdc-0e01-46b8-b21a-428dd120fe56"
   },
   "source": [
    "#### Etc we can use any library to \n",
    "- Compute prediction probabilities\n",
    "- Do a naive model comparison\n",
    "- Compute ROC and AUC\n",
    "- Compute the average precision\n",
    "- Balance the class weights\n",
    "- Compute the class weights\n",
    "- Train additional models such as RandomForrest Classifiers\n",
    "- Test, evaluate and compare model performance before iterating on features and selecting the final features and models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b400634-fa26-4202-9fc7-951f2d431532",
   "metadata": {
    "id": "9b400634-fa26-4202-9fc7-951f2d431532"
   },
   "source": [
    "# Step 4: Going to Production\n",
    "## But, Data and Machine Learning Engineers Hate Jupyter\n",
    "The non-linearity that was so beneficial as a data scientist turns into a nightmarish choose-your-own adventure book for the machine learning engineer trying to recreate the final path that a Data Scientist took. Whereas all the failures, mistakes, and errors that were made are useful for a data scientist to hold onto, it makes the machine learning engineers job closer to archaeology: trying to discover and interpret the hidden meaning behind for loops, boolean statements and drop cols.\n",
    "\n",
    "### With Kaskada you can bridge the gap to production\n",
    "- Kaskada connects directly the event-based data available in production\n",
    "- Data scientists define the predictor features used to power training sets\n",
    "- Data and ML Engineers call Kaskada to compute the **same** features at the time of now() in production\n",
    "- Kaskada provides production grade targets such as Redis for feature and model serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b31a82b-bcc2-468c-a624-3b8d94e12e12",
   "metadata": {
    "id": "3b31a82b-bcc2-468c-a624-3b8d94e12e12"
   },
   "source": [
    "### Make Features as Code available for airflow jobs etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4157fa-7a57-4403-ad1f-0b22f4223232",
   "metadata": {
    "id": "ec4157fa-7a57-4403-ad1f-0b22f4223232"
   },
   "outputs": [],
   "source": [
    "%%fenl --var feature_vector\n",
    "\n",
    "{\n",
    "    payment_plan_days: Transaction.payment_plan_days,\n",
    "    payment_method_id: Transaction.payment_method_id,\n",
    "    trans_at: Transaction.transaction_date,\n",
    "    membership_expire_date: Transaction.membership_expire_date,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea827f1-f2f6-429a-934a-f00d2ad98bfd",
   "metadata": {
    "id": "eea827f1-f2f6-429a-934a-f00d2ad98bfd"
   },
   "outputs": [],
   "source": [
    "from kaskada import view\n",
    "\n",
    "view.create_view(\n",
    "  view_name = \"Features\", \n",
    "  expression = feature_vector.query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab9ac0-1d8b-464d-a85d-4780a36d1eb5",
   "metadata": {
    "id": "b0ab9ac0-1d8b-464d-a85d-4780a36d1eb5"
   },
   "outputs": [],
   "source": [
    "view.list_views(search = \"Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd67666b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6924ca3e-28b3-4f93-b0cf-5f8afddc11d8",
    "936700a9-e042-401c-9156-7bb18652e109",
    "08f5921d-36dc-41d1-a2a6-ae800b7a11de"
   ],
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
