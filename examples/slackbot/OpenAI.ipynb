{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e90573",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b20a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import openai\n",
    "\n",
    "openai.api_key = getpass.getpass('OpenAI: API Key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152523e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class SlackBotMessage(ABC):\n",
    "    \"\"\"Defines a base slack bot message. This class is intended to be extended from rather than used.\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def to_request(self):\n",
    "        pass\n",
    "\n",
    "class SystemContent(SlackBotMessage):\n",
    "    \"\"\"SystemContent are messages that are prefixed to a conversation and provide context to the LLM.\n",
    "    \"\"\"\n",
    "    def __init__(self, content: str):\n",
    "        self.content = content\n",
    "\n",
    "    def to_request(self):\n",
    "        return {\"role\": \"system\", \"content\": self.content}\n",
    "\n",
    "class QueryContent(SlackBotMessage):\n",
    "    \"\"\"QueryContent is a message that specifies the users topic to the chat bot for reaching out.\n",
    "    \"\"\"\n",
    "    def __init__(self, content: str):\n",
    "        self.content = content\n",
    "        \n",
    "    def to_request(self):\n",
    "        return {\"role\": \"user\", \"content\": f\"Who should I reach out to about: {self.content}\"}\n",
    "\n",
    "class SlackMessage(SlackBotMessage):\n",
    "    \"\"\"SlackMessage are the messages from Slack that provide additional context to the query.\n",
    "    \"\"\"\n",
    "    def __init__(self, username: str, content: str):\n",
    "        self.username = username\n",
    "        self.content = content\n",
    "\n",
    "    def to_request(self):\n",
    "        return {\"role\": \"system\", \"content\": f\"{self.username}: {self.content}\"}\n",
    "\n",
    "class PostProcessor:\n",
    "    \"\"\"PostProcessor parses the response from OpenAI.\n",
    "    \"\"\"\n",
    "    def __init__(self, users: Set[str]):\n",
    "        self.users = users\n",
    "\n",
    "    def get_users_from_message(self, message: str) -> Set[str]:\n",
    "        print(message)\n",
    "        poc_users = set()\n",
    "        for user in self.users:\n",
    "            if user in message:\n",
    "                poc_users.add(user)\n",
    "        return poc_users\n",
    "\n",
    "class SlackBot:\n",
    "    \"\"\"SlackBot is the entry-point to the example of Kaskada + OpenAI + LLMs.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"gpt-3.5-turbo\", max_tokens: int = 25):\n",
    "        self.model_name = model_name\n",
    "        self.max_tokens = max_tokens\n",
    "        self.users = set()\n",
    "        self.intro_message = SystemContent(\"You are a helpful assistant designed to suggest the names of people who would best be points of contacts for a specific topic based on messages.\")\n",
    "        self.messages = []\n",
    "        self.post_processor = PostProcessor(self.users)\n",
    "    \n",
    "    def get_subset_users_message(self):\n",
    "        return SystemContent(f\"Only respond as a JSON object with any subset of these usernames who would be very interested, or return an empty set if no one would be interested: {self.users}\".replace(\"[\", \"\").replace(\"]\", \"\")).to_request()\n",
    "    \n",
    "    def get_format_message(self):\n",
    "        return SystemContent(\"Messages are formatted as username: topic of message\").to_request()\n",
    "    \n",
    "    def add_message(self, username: str, content: str):\n",
    "        message = SlackMessage(username, content)\n",
    "        self.users.add(username)\n",
    "        self.messages.append(message.to_request())\n",
    "        \n",
    "    def __create_conversion(self, query: QueryContent):\n",
    "        return [self.intro_message.to_request(),\\\n",
    "                self.get_subset_users_message(),\\\n",
    "                self.get_format_message()] + self.messages + [query.to_request()]\n",
    "    \n",
    "    def query(self, query: str) -> Set[str]:\n",
    "        messages = self.__create_conversion(QueryContent(query))\n",
    "        conversation = openai.ChatCompletion.create(\n",
    "          model=self.model_name,\n",
    "          messages=messages,\n",
    "          max_tokens=self.max_tokens,\n",
    "          temperature=0\n",
    "        )\n",
    "        response = conversation.choices[0].message.content\n",
    "        return self.post_processor.get_users_from_message(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ec8ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_bot = SlackBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19269fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some sample messages\n",
    "sample_bot.add_message(\"@kevin.nguyen\", \"Vector search databases are the future for LLMs. They enable to growth and optimizations of queries\")\n",
    "sample_bot.add_message(\"@ryan.michael\", \"Kaskada with DataStax enables faster-streaming LLMs than traditional lang-chain models\")\n",
    "sample_bot.add_message(\"@eric.pinzur\", \"Helm charts are how we should deploy the future of architecture of microservices\")\n",
    "sample_bot.add_message(\"@ben.chambers\", \"Python FFIs and Rust compilation give us a much better experience than our current implementation\")\n",
    "sample_bot.add_message(\"@jordan.frazier\", \"here’s list in type inference and index support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbc6362",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_bot.query(\"FFIs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b85475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_bot.query(\"The database I am using is a vector based implementation derived from Cassandra on Astra. There appears to be a problem with the scale.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81e8cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_bot.query(\"Vector search databases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe14d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_bot.query(\"I want to know more about Kaskadas ML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b465e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_bot.query(\"How do I onboard?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7196d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_bot.query(\"Awkward Tacos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587b6a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from the last few messages from the Slack Kaskada Eng\n",
    "# https://datastax.slack.com/archives/C04J75DMUSG/p1690824490676389\n",
    "kaskada_eng_bot = SlackBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caa4554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all the messages from the recent thread\n",
    "kaskada_eng_bot.add_message(\"@ben.chambers\", \"If yes: then since count(E) ignores null elements, then collect(E) needs to ignore elements\")\n",
    "kaskada_eng_bot.add_message(\"@ben.chambers\", \"count(E) == len(collect(E, max=null)) <- should this be true?\")\n",
    "kaskada_eng_bot.add_message(\"@jordan.frazier\", \"(i.e. @Ryan Michael Should collect() collect null values into the list?\")\n",
    "kaskada_eng_bot.add_message(\"@ryan.michael\", \"That’s an interesting question\")\n",
    "kaskada_eng_bot.add_message(\"@ben.chambers\", \"It’s also interesting, because right now most aggregations produce null if they haven’t seen any non-null values. But count produces 0 and collect will produce the empty list. So it feels like we may want a relationship between them.\")\n",
    "kaskada_eng_bot.add_message(\"@ben.chambers\", '''That’s also nice because it lets us do something like:\n",
    "E.value | if(E.interesting) | collect(max=10)\n",
    "To collect “10 interesting thnigs”''')\n",
    "kaskada_eng_bot.add_message(\"@ben.chambers\", '''And we can always put a null value in a non-null struct:\n",
    "({ E.maybe_null } | collect(max=10)).maybe_null if we want to collect 10 potentially-null values.''')\n",
    "kaskada_eng_bot.add_message(\"@jordan.frazier\", '''count produces 0 if it only sees null values (since it doesn’t count null).\n",
    "len(collect()) produces null if it skips null values (contradictory — count(E) != len(collect(E))''')\n",
    "kaskada_eng_bot.add_message(\"@ben.chambers\", '''Why? Why not have it produces an empty list if it hasn’t seen any values? It doesn’t take any space, and makes it relate to count?''')\n",
    "kaskada_eng_bot.add_message(\"@jordan.frazier\", '''That’s right — I was thinking of “skipping” as “ignoring” the input entirely, but that doesn’t make sense. If it sees a null it will either produce the current list or the empty list if none exists''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e195c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_message_on_thread = '''If anybody wants to comment (maybe @Brian Godsey), I added the question to the doc.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07791f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaskada_eng_bot.query(last_message_on_thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d193bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaskada_eng_bot.query(\"I think skipping the enitre input is necessary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1a9a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaskada_eng_bot.query(\"another random octopus tacos vector me search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e4e8d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68545536",
   "metadata": {},
   "source": [
    "1. Providing the list of users is not a scalable approach\n",
    "2. We are not going to provide the whole chat history (this is what fine tuning is for)\n",
    "3. Do we want to allow the model to choose whether or not to return a person or empty set? Should someone always be notified? \"Only return a name if you're very confident\"\n",
    "4. Validation metric? Condition in which we create training examples. E.g. if we know a specific history resulted in a choice, then we can rank it. Recognize if there are important people or it just doesn't know."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
