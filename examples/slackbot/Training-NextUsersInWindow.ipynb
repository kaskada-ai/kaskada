{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Users in Window\n",
    "\n",
    "### For a set of recent messages, try to predict the set of users that might interact next.\n",
    "\n",
    "For each set of messages, build a user set from the following properties:\n",
    "* users that replied to the most-recent message\n",
    "* users that reacted to the most-recent message\n",
    "* any users that wrote a message in the next X minutes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade openai pip install file-read-backwards  -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, openai, random, time, os\n",
    "from openai import cli\n",
    "from types import SimpleNamespace\n",
    "from file_read_backwards import FileReadBackwards\n",
    "\n",
    "work_dir = \"\"\n",
    "\n",
    "# Remember to remove your key from your code when you're done.\n",
    "openai.api_key = \"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Output Stage\n",
    "\n",
    "Pass through the dataset and for each message, find the set of users that write another message inside some time window\n",
    "\n",
    "The code below reads the file backwards to make the algorithm easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New channel: team-compute\n",
      "New channel: team-api\n",
      "New channel: sales-team\n",
      "New channel: rust\n",
      "New channel: random\n",
      "New channel: product\n",
      "New channel: on-call\n",
      "New channel: inbound-leads\n",
      "New channel: general\n",
      "New channel: games\n",
      "New channel: fluff-posting\n",
      "New channel: dev-ops\n",
      "New channel: dev\n",
      "New channel: conferences\n",
      "New channel: articles\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "current_channel = \"\"\n",
    "ts_user_map = {}\n",
    "window_seconds = 300\n",
    "\n",
    "out_file = open(f'{work_dir}/next_users_in_window_temp.jsonl', 'w')\n",
    "\n",
    "with FileReadBackwards(f'{work_dir}/messages.jsonl') as in_file:\n",
    "    for line in in_file:\n",
    "        data = json.loads(line)\n",
    "\n",
    "        channel = data[\"channel\"]\n",
    "        msg_user = data[\"user\"]\n",
    "        msg_ts = data[\"ts\"]\n",
    "\n",
    "        channel = data[\"channel\"]\n",
    "\n",
    "        # restart if the channel changes in the file\n",
    "        if channel != current_channel:\n",
    "            current_channel = channel\n",
    "            ts_user_map = {}\n",
    "            print(f'New channel: {current_channel}')\n",
    "\n",
    "        # remove too old timestamps from the window\n",
    "        next_ts_user_map = {}\n",
    "        #print(ts_user_map)\n",
    "        for ts in ts_user_map:\n",
    "            if ts < msg_ts + window_seconds:\n",
    "                user = ts_user_map[ts]\n",
    "                next_ts_user_map[ts] = user\n",
    "        ts_user_map = next_ts_user_map\n",
    "\n",
    "        # get the list of users in the window and de-duplicate it\n",
    "        users_in_window = list(ts_user_map.values())\n",
    "        users_in_window = list(dict.fromkeys(users_in_window))\n",
    "        \n",
    "\n",
    "        out_file.write(json.dumps({ \"channel\": channel, \"ts\": msg_ts, \"users_in_window\": users_in_window })+\"\\n\")\n",
    "        #print({ \"channel\": channel, \"ts\": msg_ts, \"users_in_window\": users_in_window })\n",
    "        #count += 1\n",
    "        #if count > 10:\n",
    "        #    break\n",
    "\n",
    "        # add the current timestamp to the map\n",
    "        ts_user_map[msg_ts] = msg_user\n",
    "\n",
    "out_file.close()\n",
    "\n",
    "# reverse the file to get the actual output we want\n",
    "out_file = open(f'{work_dir}/next_users_in_window_pre.jsonl', 'w')\n",
    "with FileReadBackwards(f'{work_dir}/next_users_in_window_temp.jsonl') as in_file:\n",
    "    for line in in_file:\n",
    "        out_file.write(line+\"\\n\")\n",
    "out_file.close()\n",
    "\n",
    "# delete temp file\n",
    "os.remove(f'{work_dir}/next_users_in_window_temp.jsonl')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Output Stage\n",
    "\n",
    "This is a Generative problem, so the goals for training aren't as strict.\n",
    "\n",
    "Goals:\n",
    "* prompt and completion length must not be longer than 2048 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New channel: articles\n",
      "New channel: conferences\n",
      "New channel: dev\n",
      "New channel: dev-ops\n",
      "New channel: fluff-posting\n",
      "New channel: games\n",
      "New channel: general\n",
      "New channel: inbound-leads\n",
      "New channel: on-call\n",
      "New channel: product\n",
      "New channel: random\n",
      "New channel: rust\n",
      "New channel: sales-team\n",
      "New channel: team-api\n",
      "New channel: team-compute\n"
     ]
    }
   ],
   "source": [
    "in_file = open(f'{work_dir}/messages.jsonl', 'r')\n",
    "in_pre = open(f'{work_dir}/next_users_in_window_pre.jsonl', 'r')\n",
    "out_file = open(f'{work_dir}/next_users_in_window.jsonl', 'w')\n",
    "\n",
    "current_channel = \"\"\n",
    "previous_training_example = \"\"\n",
    "recent_messages = []\n",
    "count = 0\n",
    "\n",
    "min_messages = 2\n",
    "max_messages = 5\n",
    "between_message_separator = \" \\n\\n\\n \"\n",
    "prompt_separator = \"\\n\\n###\\n\\n\"\n",
    "completion_separator = \" END\"\n",
    "reverse_messages = True\n",
    "\n",
    "# gets a list of all users that reacted to a message\n",
    "def get_reaction_users(reactions):\n",
    "    users = []\n",
    "    if reactions:\n",
    "        for reaction in reactions:\n",
    "            users.extend(reaction[\"users\"])\n",
    "    return users\n",
    "\n",
    "while True:\n",
    "    line = in_file.readline()\n",
    "\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "    pre_line = in_pre.readline()\n",
    "\n",
    "    if not pre_line:\n",
    "        break\n",
    "\n",
    "    data = json.loads(line)\n",
    "    text = data[\"text\"].strip()\n",
    "    channel = data[\"channel\"]\n",
    "    ts = data[\"ts\"]\n",
    "\n",
    "    pre_data = json.loads(pre_line)\n",
    "    pre_channel = pre_data[\"channel\"]\n",
    "    pre_ts = pre_data[\"ts\"]\n",
    "\n",
    "    if pre_ts != ts or pre_channel != channel:\n",
    "        print(\"Pre data and data are mis-aligned. Something went wrong. Stopping\")\n",
    "        break\n",
    "\n",
    "    # skip message if empty\n",
    "    if text == \"\":\n",
    "        continue\n",
    "\n",
    "    # skip message if it contains a code block\n",
    "    if text.find(\"```\") >= 0:\n",
    "        continue\n",
    "\n",
    "    channel = data[\"channel\"]\n",
    "\n",
    "    # restart message history count if the channel changes in the file\n",
    "    if channel != current_channel:\n",
    "        current_channel = channel\n",
    "        recent_messages = []\n",
    "        print(f'New channel: {current_channel}')\n",
    "\n",
    "    # add the recent message to the history.\n",
    "    # remove the oldest message if the size window is exceeded\n",
    "    recent_messages.append(text)\n",
    "    if len(recent_messages) > max_messages:\n",
    "        recent_messages.pop(0)\n",
    "\n",
    "    # if we are in the message history size window, prep to output an example\n",
    "    if len(recent_messages) > min_messages:\n",
    "        messages = recent_messages.copy()\n",
    "        if reverse_messages:\n",
    "            messages.reverse()\n",
    "        \n",
    "        prompt = between_message_separator.join(messages) \n",
    "\n",
    "        # build up user set for the completion\n",
    "        users = []\n",
    "\n",
    "        reactions = data[\"reactions\"]\n",
    "        if reactions:\n",
    "            for reaction in reactions:\n",
    "                users.extend(reaction[\"users\"])\n",
    "\n",
    "        reply_users = data[\"reply_users\"]\n",
    "        if reply_users:\n",
    "            users.extend(reply_users)\n",
    "\n",
    "        users_in_window = pre_data[\"users_in_window\"]\n",
    "        if users_in_window:\n",
    "            users.extend(users_in_window)\n",
    "        \n",
    "        # de-duplicate & sort users in the list.\n",
    "        users = list(dict.fromkeys(users))\n",
    "        users.sort()\n",
    "\n",
    "        # remove the message writer from the user list\n",
    "        msg_user = data[\"user\"]\n",
    "        if msg_user in users:\n",
    "            users.remove(msg_user)\n",
    "\n",
    "        # make the completion\n",
    "        completion = json.dumps(users)\n",
    "\n",
    "        training_example = { \"prompt\": f'{prompt}{prompt_separator}', \"completion\": f' {completion}{completion_separator}' }\n",
    "        training_example = json.dumps(training_example)\n",
    "\n",
    "        # if training example doesn't match the previous one, then output\n",
    "        if previous_training_example != training_example:\n",
    "            out_file.write(training_example + \"\\n\")\n",
    "\n",
    "        previous_training_example = training_example\n",
    "\n",
    "in_file.close()\n",
    "in_pre.close()\n",
    "out_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Verification Stage\n",
    "\n",
    "* make sure prompts end with same suffix\n",
    "* make sure tokens per example are less than 2048\n",
    "* ignore all other analysis\n",
    "  * we are training for *conditional generation*, but the data prep tool incorreclty assumes we are fine-tuning for *classifaction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Your file contains 19672 prompt-completion pairs\n",
      "- Based on your data it seems like you're trying to fine-tune a model for classification\n",
      "- For classification, we recommend you try one of the faster and cheaper models, such as `ada`\n",
      "- For classification, you can estimate the expected model performance by keeping a held out dataset, which is not used for training\n",
      "- More than a third of your `completion` column/key is uppercase. Uppercase completions tends to perform worse than a mixture of case encountered in normal language. We recommend to lower case the data if that makes sense in your domain. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
      "- All prompts end with suffix `\\n\\n###\\n\\n`\n",
      "\n",
      "Based on the analysis we will perform the following actions:\n",
      "- [Recommended] Lowercase all your data in column/key `completion` [Y/n]: Y\n",
      "- [Recommended] Would you like to split into training and validation set? [Y/n]: Y\n",
      "\n",
      "\n",
      "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
      "\n",
      "Wrote modified files to `/Users/eric.pinzur/Documents/slackbot2000/next_users_in_window_prepared_train.jsonl` and `/Users/eric.pinzur/Documents/slackbot2000/next_users_in_window_prepared_valid.jsonl`\n",
      "Feel free to take a look!\n",
      "\n",
      "Now use that file when fine-tuning:\n",
      "> openai api fine_tunes.create -t \"/Users/eric.pinzur/Documents/slackbot2000/next_users_in_window_prepared_train.jsonl\" -v \"/Users/eric.pinzur/Documents/slackbot2000/next_users_in_window_prepared_valid.jsonl\" --compute_classification_metrics --classification_n_classes 283\n",
      "\n",
      "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string `\\n\\n###\\n\\n` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\"] end\"]` so that the generated texts ends at the expected place.\n",
      "Once your model starts training, it'll approximately take 7.91 hours to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
     ]
    }
   ],
   "source": [
    "args = SimpleNamespace(file=f'{work_dir}/next_users_in_window.jsonl', quiet=True)\n",
    "cli.FineTune.prepare_data(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Stage\n",
    "\n",
    "* First need to upload the training & validation files to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 15.1M/15.1M [00:00<00:00, 18.9Git/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/eric.pinzur/Documents/slackbot2000/next_users_in_window.jsonl: file-NjHHSOB0TJZK7iODaT2Da6Us\n",
      "Status (training_file): uploaded \n",
      "Status (training_file): uploaded \n",
      "Status (training_file): uploaded \n",
      "Status (training_file): uploaded \n",
      "Status (training_file): uploaded \n",
      "Status (training_file): uploaded \n",
      "Status (training_file): processed \n"
     ]
    }
   ],
   "source": [
    "training_file_name = f'{work_dir}/next_users_in_window.jsonl'\n",
    "\n",
    "def check_status(training_id):\n",
    "    train_status = openai.File.retrieve(training_id)[\"status\"]\n",
    "    print(f'Status (training_file): {train_status} ')\n",
    "    return (train_status)\n",
    "\n",
    "# Upload the training and validation dataset files to Azure OpenAI.\n",
    "training_id = cli.FineTune._get_or_upload(training_file_name, True)\n",
    "\n",
    "# Check on the upload status of the training dataset file.\n",
    "(train_status) = check_status(training_id)\n",
    "\n",
    "# Poll and display the upload status once a second until both files have either\n",
    "# succeeded or failed to upload.\n",
    "while train_status not in [\"succeeded\", \"failed\", \"processed\"]:\n",
    "    time.sleep(1)\n",
    "    (train_status) = check_status(training_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a fine-tuning Job\n",
    "\n",
    "* no validation file since this is not a classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning model with job ID: ft-MqX1fWkyyyX3ZpwTrDo34rU2\n"
     ]
    }
   ],
   "source": [
    "# This example defines a fine-tune job that creates a customized model based on curie, \n",
    "# with just a single pass through the training data. The job also provides classification-\n",
    "# specific metrics, using our validation data, at the end of that epoch.\n",
    "create_args = {\n",
    "    \"training_file\": training_id,\n",
    "    \"model\": \"ada\",\n",
    "    \"n_epochs\": 1,\n",
    "    \"suffix\": \"next_users_in_window_full_kaskada\"\n",
    "}\n",
    "# Create the fine-tune job and retrieve the job ID\n",
    "# and status from the response.\n",
    "resp = openai.FineTune.create(**create_args)\n",
    "job_id = resp[\"id\"]\n",
    "status = resp[\"status\"]\n",
    "\n",
    "# You can use the job ID to monitor the status of the fine-tune job.\n",
    "# The fine-tune job may take some time to start and complete.\n",
    "print(f'Fine-tuning model with job ID: {job_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = \"ft-MqX1fWkyyyX3ZpwTrDo34rU2\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for the fine-tuning to start\n",
    "\n",
    "* Note that it can take several hours for the job to move from the `pending` state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the status of our fine-tune job.\n",
    "status = openai.FineTune.retrieve(id=job_id)[\"status\"]\n",
    "\n",
    "# If the job isn't yet done, poll it every 2 seconds.\n",
    "if status not in [\"succeeded\", \"failed\"]:\n",
    "    print(f'Job not in terminal status: {status}. Waiting.')\n",
    "    while status not in [\"succeeded\", \"failed\"]:\n",
    "        time.sleep(5)\n",
    "        status = openai.FineTune.retrieve(id=job_id)[\"status\"]\n",
    "        print(f'Status: {status}')\n",
    "else:\n",
    "    print(f'Fine-tune job {job_id} finished with status: {status}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check fine-tuning events\n",
    "\n",
    "* Lets us know specifics about the fine-tuning job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the events of our fine-tune job.\n",
    "events = openai.FineTune.stream_events(id=job_id)\n",
    "\n",
    "for event in events:\n",
    "    print(event)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at Training Results\n",
    "\n",
    "* download the results file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputted results to: /Users/eric.pinzur/Documents/slackbot2000/next_users_in_window_results_0.csv\n"
     ]
    }
   ],
   "source": [
    "file_prefix = \"next_users_in_window_results\"\n",
    "\n",
    "result = openai.FineTune.retrieve(id=job_id)\n",
    "count = 0\n",
    "for result_file in result[\"result_files\"]:\n",
    "    file_name = f'{work_dir}/{file_prefix}_{count}.csv'\n",
    "    file = open(file_name, 'wb')\n",
    "    file.write(openai.File.download(id=result_file[\"id\"]))\n",
    "    file.close()\n",
    "    print(f'Outputted results to: {file_name}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
