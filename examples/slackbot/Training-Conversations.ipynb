{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversations\n",
    "\n",
    "### For a set of recent messages in a \"conversation\", try to predict the set of users that might interact next.\n",
    "\n",
    "A \"conversation\" is defined as either:\n",
    "* All the messages in a thread\n",
    "* A collection of messages from a single channel that occur in succession. If no response is made for 10 minutes, the conversation has ended. The next message outside this window is the start of a new conversation.\n",
    "\n",
    "\n",
    "For each set of messages in a \"conversation\", build a user set from the following properties:\n",
    "* users that reacted to the conversation\n",
    "* users that participated in the conversation\n",
    "* exclude the conversation starter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade openai pip install file-read-backwards  -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, openai, time, pandas, random, getpass\n",
    "from openai import cli\n",
    "from types import SimpleNamespace\n",
    "from sklearn.model_selection import train_test_split\n",
    "from file_read_backwards import FileReadBackwards\n",
    "\n",
    "work_dir = \"/Users/eric.pinzur/Documents/slackbot2000\"\n",
    "openai.api_key = getpass.getpass(prompt=\"Please enter your OpenAI API Key\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Data Prep\n",
    "\n",
    "Convert the original input data into a set of \"conversations\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into \"threads\" and \"non-threads\"\n",
    "\n",
    "Using DuckDB:\n",
    "\n",
    "```sql\n",
    "copy(\n",
    "    select * from \n",
    "    read_json_auto('messages.jsonl', format='newline_delimited') \n",
    "    where thread_ts is not null \n",
    "    order by channel, thread_ts, ts\n",
    ") to 'message_threads.jsonl' (FORMAT JSON);\n",
    "\n",
    "copy(\n",
    "    select * from \n",
    "    read_json_auto('messages.jsonl', format='newline_delimited') \n",
    "    where thread_ts is null \n",
    "    order by channel, ts\n",
    ") to 'message_non_threads.jsonl' (FORMAT JSON);\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make threads from non-threads\n",
    "\n",
    "For non-threads, artifically group the messages into \"threads\".  Collect messages from a channel.  If there is a 5 minute gap between messages, convert the message collection to a \"thread\" and export.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = open(f'{work_dir}/message_non_threads.jsonl', 'r')\n",
    "out_file = open(f'{work_dir}/message_non_threads_threads.jsonl', 'w')\n",
    "\n",
    "def write_thread(thread):\n",
    "    if len(thread) > 0:\n",
    "        reply_users = []\n",
    "        for message in thread:\n",
    "            if message[\"user\"] not in reply_users:\n",
    "                reply_users.append(message[\"user\"])\n",
    "        thread[0][\"reply_users\"] = reply_users\n",
    "        thread_ts = thread[0][\"ts\"]\n",
    "        for message in thread:\n",
    "            message[\"thread_ts\"] = thread_ts\n",
    "            out_file.write(json.dumps(message)+\"\\n\")\n",
    "\n",
    "next_thread = []\n",
    "current_channel = \"\"\n",
    "last_msg_ts = None\n",
    "while True:\n",
    "    output_thread = False\n",
    "    line = in_file.readline()\n",
    "\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "    message = json.loads(line)\n",
    "\n",
    "    channel = message[\"channel\"]\n",
    "\n",
    "    # output if channel changes in the file\n",
    "    if channel != current_channel:\n",
    "        current_channel = channel\n",
    "        output_thread = True\n",
    "\n",
    "    # output if message timestamp is more than 10 mins beyond last message\n",
    "    if last_msg_ts and message[\"ts\"] > last_msg_ts + 600:\n",
    "        output_thread = True\n",
    "\n",
    "    # output and reset\n",
    "    if output_thread:\n",
    "        write_thread(next_thread)\n",
    "        next_thread = []\n",
    "        last_msg_ts = None\n",
    "\n",
    "    next_thread.append(message)\n",
    "    last_msg_ts = message[\"ts\"]\n",
    "\n",
    "# output final thread\n",
    "write_thread(next_thread)\n",
    "\n",
    "in_file.close()\n",
    "out_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Conversations\n",
    "\n",
    "Re-join the two files into a set of conversations, using duckDB:\n",
    "\n",
    "```sql\n",
    "copy(\n",
    "    select * from \n",
    "    read_json_auto(['message_non_threads_threads.jsonl', 'message_threads.jsonl'], format='newline_delimited') \n",
    "    order by channel, thread_ts, ts\n",
    ") to 'message_conversations.jsonl' (FORMAT JSON);\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Examples\n",
    "\n",
    "This is a Generative problem, so the goals for training aren't as strict.\n",
    "\n",
    "Goals:\n",
    "* prompt and completion length must not be longer than 2048 tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy\n",
    "\n",
    "* Group messages int \"conversations\".\n",
    "* Use a method to write examples for a \"conversation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a user map\n",
    "in_file = open(f'{work_dir}/message_conversations.jsonl', 'r')\n",
    "user_map = {}\n",
    "user_count = 0\n",
    "\n",
    "while True:\n",
    "    line = in_file.readline()\n",
    "\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "    message = json.loads(line)\n",
    "    user = message[\"user\"]\n",
    "\n",
    "    # create a user_id -> user_num map\n",
    "    if user not in user_map:\n",
    "        user_count += 1\n",
    "        user_map[user] = user_count\n",
    "\n",
    "in_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UCZ4VJF6J': 1,\n",
       " 'ULJD5H2A2': 2,\n",
       " 'U017T5TFW58': 3,\n",
       " 'U014ZU49HPT': 4,\n",
       " 'UU5C7MNMA': 5,\n",
       " 'U016TM9NXEY': 6,\n",
       " 'U021KG8NMRQ': 7,\n",
       " 'UCXNQ2MPV': 8,\n",
       " 'U02TCUCA7PU': 9,\n",
       " 'U012CLUV1KJ': 10,\n",
       " 'U0332GKB9J8': 11,\n",
       " 'UT62H53R6': 12,\n",
       " 'U03RNK543HC': 13,\n",
       " 'USLACKBOT': 14,\n",
       " 'U011BDYMEG7': 15,\n",
       " 'U01TZK90VSM': 16,\n",
       " 'U02326Q5BG9': 17,\n",
       " 'U017S2GDXF0': 18,\n",
       " 'U0117AWAAEN': 19}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = open(f'{work_dir}/message_conversations.jsonl', 'r')\n",
    "out_file = open(f'{work_dir}/conversation_user_examples.jsonl', 'w')\n",
    "\n",
    "# gets a list of all users that reacted to a message\n",
    "def get_reaction_users(message):\n",
    "    users = []\n",
    "    if message[\"reactions\"]:\n",
    "        for reaction in message[\"reactions\"]:\n",
    "            users.extend(reaction[\"users\"])\n",
    "    return users\n",
    "\n",
    "prompt_separator = \"\\n\\n###\\n\\n\"\n",
    "completion_separator = \" end\"\n",
    "\n",
    "def output_conversation_example(conversation):\n",
    "    if len(conversation) > 0:\n",
    "        prompt_lines = []\n",
    "        initial_user = conversation[0][\"user\"]\n",
    "        users = []\n",
    "        for message in conversation:\n",
    "            user = message[\"user\"]\n",
    "            users.append(user)\n",
    "            users.extend(get_reaction_users(message))\n",
    "            \n",
    "            text = message[\"text\"]\n",
    "            if text == \"\" or text.find(\"```\") >= 0:\n",
    "                continue\n",
    "\n",
    "            prompt_lines.append(f' {user} --> {text} ')\n",
    "\n",
    "        if len(prompt_lines) == 0:\n",
    "            return\n",
    "        \n",
    "        prompt = \"\\n\\n\".join(prompt_lines)\n",
    "\n",
    "        # de-duplicate users in the list.\n",
    "        users = list(dict.fromkeys(users))\n",
    "\n",
    "        # remove the conversation starter\n",
    "        if initial_user in users:\n",
    "            users.remove(initial_user)\n",
    "\n",
    "        # convert user_ids to user_nums\n",
    "        user_nums = []\n",
    "        for user in users:\n",
    "            if user in user_map:\n",
    "                user_nums.append(f'{user_map[user]}')\n",
    "\n",
    "        completion = \" \".join(user_nums) if len(user_nums) > 0 else \"nil\"\n",
    "        \n",
    "        example = { \"prompt\": f'start -> {prompt}{prompt_separator}', \"completion\": f' {completion}{completion_separator}' }\n",
    "        out_file.write(json.dumps(example) + \"\\n\")\n",
    "\n",
    "           \n",
    "current_channel = \"\"\n",
    "current_conversation = []\n",
    "conversation_ts = None\n",
    "\n",
    "while True:\n",
    "    output_convo = False\n",
    "    line = in_file.readline()\n",
    "\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "    message = json.loads(line)\n",
    "    channel = message[\"channel\"]\n",
    "    thread_ts = message[\"thread_ts\"]\n",
    "    user = message[\"user\"]\n",
    "\n",
    "\n",
    "\n",
    "    # output if channel changes\n",
    "    if channel != current_channel:\n",
    "        current_channel = channel\n",
    "        output_convo = True\n",
    "\n",
    "    # output if different thread_ts\n",
    "    if conversation_ts and conversation_ts != thread_ts:\n",
    "        output_convo = True\n",
    "\n",
    "    # output and reset\n",
    "    if output_convo:\n",
    "        output_conversation_example(current_conversation)\n",
    "        current_conversation = []\n",
    "\n",
    "    current_conversation.append(message)\n",
    "    conversation_ts = thread_ts\n",
    "   \n",
    "\n",
    "# output final conversation\n",
    "output_conversation_example(current_conversation)\n",
    "\n",
    "in_file.close()\n",
    "out_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Verification & Split Stage\n",
    "\n",
    "* make sure prompts end with same suffix\n",
    "* remove too long examples\n",
    "* remove duplicated examples\n",
    "\n",
    "Note, we aren't doing classification, so don't start a fine-tune as suggested by the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Your file contains 4642 prompt-completion pairs\n",
      "- Based on your data it seems like you're trying to fine-tune a model for classification\n",
      "- For classification, we recommend you try one of the faster and cheaper models, such as `ada`\n",
      "- For classification, you can estimate the expected model performance by keeping a held out dataset, which is not used for training\n",
      "- There are 3 duplicated prompt-completion sets. These are rows: [971, 1907, 3985]\n",
      "- There are 2 examples that are very long. These are rows: [111, 1618]\n",
      "For conditional generation, and for classification the examples shouldn't be longer than 2048 tokens.\n",
      "- All prompts end with suffix `\\n\\n###\\n\\n`\n",
      "\n",
      "Based on the analysis we will perform the following actions:\n",
      "- [Recommended] Remove 3 duplicate rows [Y/n]: Y\n",
      "- [Recommended] Remove 2 long examples [Y/n]: Y\n",
      "The indices of the long examples has changed as a result of a previously applied recommendation.\n",
      "The 2 long examples to be dropped are now at the following indices: [111, 1617]\n",
      "- [Recommended] Would you like to split into training and validation set? [Y/n]: Y\n",
      "\n",
      "\n",
      "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
      "\n",
      "Wrote modified files to `/Users/eric.pinzur/Documents/slackbot2000/conversation_user_stripped_examples_prepared_train.jsonl` and `/Users/eric.pinzur/Documents/slackbot2000/conversation_user_stripped_examples_prepared_valid.jsonl`\n",
      "Feel free to take a look!\n",
      "\n",
      "Now use that file when fine-tuning:\n",
      "> openai api fine_tunes.create -t \"/Users/eric.pinzur/Documents/slackbot2000/conversation_user_stripped_examples_prepared_train.jsonl\" -v \"/Users/eric.pinzur/Documents/slackbot2000/conversation_user_stripped_examples_prepared_valid.jsonl\" --compute_classification_metrics --classification_n_classes 325\n",
      "\n",
      "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string `\\n\\n###\\n\\n` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\" end\"]` so that the generated texts ends at the expected place.\n",
      "Once your model starts training, it'll approximately take 1.89 hours to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
     ]
    }
   ],
   "source": [
    "args = SimpleNamespace(file=f'{work_dir}/conversation_user_stripped_examples.jsonl', quiet=True)\n",
    "cli.FineTune.prepare_data(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Stage\n",
    "\n",
    "* First need to upload the training & validation files to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 2.11M/2.11M [00:00<00:00, 2.39Git/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/eric.pinzur/Documents/slackbot2000/conversation_user_stripped_examples_prepared_train.jsonl: file-SRadZhsiDk2ioHlNvIIRk7Ye\n",
      "Status (training_file): uploaded \n",
      "Status (training_file): uploaded \n",
      "Status (training_file): uploaded \n",
      "Status (training_file): uploaded \n",
      "Status (training_file): uploaded \n",
      "Status (training_file): processed \n"
     ]
    }
   ],
   "source": [
    "training_file_name = f'{work_dir}/conversation_user_stripped_examples_prepared_train.jsonl'\n",
    "\n",
    "def check_status(training_id):\n",
    "    train_status = openai.File.retrieve(training_id)[\"status\"]\n",
    "    print(f'Status (training_file): {train_status} ')\n",
    "    return (train_status)\n",
    "\n",
    "# Upload the training and validation dataset files to Azure OpenAI.\n",
    "training_id = cli.FineTune._get_or_upload(training_file_name, True)\n",
    "\n",
    "# Check on the upload status of the training dataset file.\n",
    "(train_status) = check_status(training_id)\n",
    "\n",
    "# Poll and display the upload status once a second until both files have either\n",
    "# succeeded or failed to upload.\n",
    "while train_status not in [\"succeeded\", \"failed\", \"processed\"]:\n",
    "    time.sleep(1)\n",
    "    (train_status) = check_status(training_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a fine-tuning Job\n",
    "\n",
    "* no validation file since this is not a classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning model with job ID: \"ft-JXAXQtHd74y6fIDmeQyti3ZP\"\n"
     ]
    }
   ],
   "source": [
    "# This example defines a fine-tune job that creates a customized model based on curie, \n",
    "# with just a single pass through the training data. The job also provides classification-\n",
    "# specific metrics, using our validation data, at the end of that epoch.\n",
    "create_args = {\n",
    "    \"training_file\": training_id,\n",
    "    \"model\": \"davinci\",\n",
    "    \"n_epochs\": 2,\n",
    "    \"learning_rate_multiplier\": 0.02,\n",
    "    \"suffix\": \"coversation_users_full_kaskada_c\"\n",
    "}\n",
    "# Create the fine-tune job and retrieve the job ID\n",
    "# and status from the response.\n",
    "resp = openai.FineTune.create(**create_args)\n",
    "job_id = resp[\"id\"]\n",
    "status = resp[\"status\"]\n",
    "\n",
    "# You can use the job ID to monitor the status of the fine-tune job.\n",
    "# The fine-tune job may take some time to start and complete.\n",
    "print(f'Fine-tuning model with job ID: \"{job_id}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = \"ft-qnHHxoNViVDplGDgIGn0kEC9\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for the fine-tuning to start\n",
    "\n",
    "* Note that it can take several hours for the job to move from the `pending` state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job not in terminal status: pending. Waiting.\n",
      "Status: pending\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mJob not in terminal status: \u001b[39m\u001b[39m{\u001b[39;00mstatus\u001b[39m}\u001b[39;00m\u001b[39m. Waiting.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39mwhile\u001b[39;00m status \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39msucceeded\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfailed\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m----> 8\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m5\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m     status \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mFineTune\u001b[39m.\u001b[39mretrieve(\u001b[39mid\u001b[39m\u001b[39m=\u001b[39mjob_id)[\u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mStatus: \u001b[39m\u001b[39m{\u001b[39;00mstatus\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get the status of our fine-tune job.\n",
    "status = openai.FineTune.retrieve(id=job_id)[\"status\"]\n",
    "\n",
    "# If the job isn't yet done, poll it every 2 seconds.\n",
    "if status not in [\"succeeded\", \"failed\"]:\n",
    "    print(f'Job not in terminal status: {status}. Waiting.')\n",
    "    while status not in [\"succeeded\", \"failed\"]:\n",
    "        time.sleep(5)\n",
    "        status = openai.FineTune.retrieve(id=job_id)[\"status\"]\n",
    "        print(f'Status: {status}')\n",
    "else:\n",
    "    print(f'Fine-tune job {job_id} finished with status: {status}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check fine-tuning events\n",
    "\n",
    "* Lets us know specifics about the fine-tuning job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"fine-tune-event\",\n",
      "  \"level\": \"info\",\n",
      "  \"message\": \"Created fine-tune: ft-qnHHxoNViVDplGDgIGn0kEC9\",\n",
      "  \"created_at\": 1691662984\n",
      "}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[133], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Get the events of our fine-tune job.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m events \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mFineTune\u001b[39m.\u001b[39mstream_events(\u001b[39mid\u001b[39m\u001b[39m=\u001b[39mjob_id)\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfor\u001b[39;00m event \u001b[39min\u001b[39;00m events:\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(event)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/openai/api_resources/fine_tune.py:158\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    153\u001b[0m response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39mrequest(\n\u001b[1;32m    154\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mget\u001b[39m\u001b[39m\"\u001b[39m, url, params, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, request_id\u001b[39m=\u001b[39mrequest_id\n\u001b[1;32m    155\u001b[0m )\n\u001b[1;32m    157\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)  \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    159\u001b[0m     util\u001b[39m.\u001b[39mconvert_to_openai_object(\n\u001b[1;32m    160\u001b[0m         line,\n\u001b[1;32m    161\u001b[0m         api_key,\n\u001b[1;32m    162\u001b[0m         api_version,\n\u001b[1;32m    163\u001b[0m         organization,\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m response\n\u001b[1;32m    166\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/openai/api_requestor.py:692\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns the response(s) and a bool indicating whether it is a stream.\"\"\"\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[39mif\u001b[39;00m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtext/event-stream\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m result\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mContent-Type\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    700\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39mcontent\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/openai/api_requestor.py:115\u001b[0m, in \u001b[0;36mparse_stream\u001b[0;34m(rbody)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_stream\u001b[39m(rbody: Iterator[\u001b[39mbytes\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m rbody:\n\u001b[1;32m    116\u001b[0m         _line \u001b[39m=\u001b[39m parse_stream_helper(line)\n\u001b[1;32m    117\u001b[0m         \u001b[39mif\u001b[39;00m _line \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/requests/models.py:865\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[39mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \n\u001b[1;32m    860\u001b[0m \u001b[39m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    863\u001b[0m pending \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter_content(\n\u001b[1;32m    866\u001b[0m     chunk_size\u001b[39m=\u001b[39mchunk_size, decode_unicode\u001b[39m=\u001b[39mdecode_unicode\n\u001b[1;32m    867\u001b[0m ):\n\u001b[1;32m    869\u001b[0m     \u001b[39mif\u001b[39;00m pending \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    870\u001b[0m         chunk \u001b[39m=\u001b[39m pending \u001b[39m+\u001b[39m chunk\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/urllib3/response.py:937\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    922\u001b[0m \u001b[39mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \u001b[39m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[39m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunked \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m--> 937\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_chunked(amt, decode_content\u001b[39m=\u001b[39mdecode_content)\n\u001b[1;32m    938\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    939\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/urllib3/response.py:1077\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1077\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_chunk_length()\n\u001b[1;32m   1078\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1079\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/urllib3/response.py:1005\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1004\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1005\u001b[0m line \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline()  \u001b[39m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m line \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39msplit(\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m;\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1007\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1238\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1240\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1241\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1242\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1100\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1101\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get the events of our fine-tune job.\n",
    "events = openai.FineTune.stream_events(id=job_id)\n",
    "\n",
    "for event in events:\n",
    "    print(event)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at Training Results\n",
    "\n",
    "* download the results file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputted results to: /Users/eric.pinzur/Documents/slackbot2000/coversation_users_0.csv\n"
     ]
    }
   ],
   "source": [
    "file_prefix = \"coversation_users\"\n",
    "\n",
    "result = openai.FineTune.retrieve(id=job_id)\n",
    "count = 0\n",
    "for result_file in result[\"result_files\"]:\n",
    "    file_name = f'{work_dir}/{file_prefix}_{count}.csv'\n",
    "    file = open(file_name, 'wb')\n",
    "    file.write(openai.File.download(id=result_file[\"id\"]))\n",
    "    file.close()\n",
    "    print(f'Outputted results to: {file_name}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"davinci:ft-personal:coversation-users-full-kaskada-2023-08-05-14-25-30\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_train = pandas.read_json(f'{work_dir}/conversation_user_examples_prepared_train.jsonl', lines=True)\n",
    "cu_valid = pandas.read_json(f'{work_dir}/conversation_user_examples_prepared_valid.jsonl', lines=True)\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# interesting i's: 6, 8, 78, 79, 80\n",
    "i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i += 1\n",
    "i = 6\n",
    "prompt = cu_train['prompt'][i]\n",
    "completion = cu_train['completion'][i]\n",
    "\n",
    "print(f'Prompt: {prompt}')\n",
    "print(f'Completion: {completion}')\n",
    "print(f'Prediction:')\n",
    "\n",
    "openai.Completion.create(model=model_id, prompt=prompt, max_tokens=1, stop=' end', n=1, logprobs=5, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: start ->  UCZ4VJF6J --> <@UU5C7MNMA> <@U017T5TFW58> Following up on the \"export\" use case -- worst case (if tempo doesn't have it) we could do something like having a little job (maybe skycat?) that finds bug reports with trace IDs and uses the Grafana query API to retrieve the trace (which returns it as OTEL JSON) and then attaches that to the bug. Then we'd have that trace *data* forever (and could always figure out how to visualize it separately):\n",
      "\n",
      "<https://grafana.com/docs/tempo/latest/api_docs/#query> \n",
      "\n",
      " U017T5TFW58 --> love the idea! \n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "Completion:  3 end\n",
      "Prediction:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7lHH5djssRnShCdPoYhxMwqR80ipK at 0x16cec7540> JSON: {\n",
       "  \"id\": \"cmpl-7lHH5djssRnShCdPoYhxMwqR80ipK\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1691502175,\n",
       "  \"model\": \"davinci:ft-personal:coversation-users-full-kaskada-2023-08-05-14-25-30\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \" 5\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": {\n",
       "        \"tokens\": [\n",
       "          \" 5\"\n",
       "        ],\n",
       "        \"token_logprobs\": [\n",
       "          -0.11093157\n",
       "        ],\n",
       "        \"top_logprobs\": [\n",
       "          {\n",
       "            \" 13\": -3.8975945,\n",
       "            \" 3\": -2.7242737,\n",
       "            \" 5\": -0.11093157,\n",
       "            \" 10\": -5.4023495\n",
       "          }\n",
       "        ],\n",
       "        \"text_offset\": [\n",
       "          553\n",
       "        ]\n",
       "      },\n",
       "      \"finish_reason\": \"length\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 164,\n",
       "    \"completion_tokens\": 1,\n",
       "    \"total_tokens\": 165\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#i += 1\n",
    "i = 220\n",
    "prompt = cu_valid['prompt'][i]\n",
    "completion = cu_valid['completion'][i]\n",
    "\n",
    "print(f'Prompt: {prompt}')\n",
    "print(f'Completion: {completion}')\n",
    "print(f'Prediction:')\n",
    "\n",
    "res = openai.Completion.create(model=model_id, prompt=prompt, max_tokens=1, stop=' end', n=1, logprobs=4, temperature=0)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  13, Prob: 2\n",
      "User:  3, Prob: 7\n",
      "User:  5, Prob: 89\n",
      "User:  10, Prob: 0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "logprobs = res[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0]\n",
    "for user in logprobs:\n",
    "    print(f'User: {user}, Prob: {round(math.exp(logprobs[user])*100)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UCZ4VJF6J': 1,\n",
       " 'ULJD5H2A2': 2,\n",
       " 'U017T5TFW58': 3,\n",
       " 'U014ZU49HPT': 4,\n",
       " 'UU5C7MNMA': 5,\n",
       " 'U016TM9NXEY': 6,\n",
       " 'U021KG8NMRQ': 7,\n",
       " 'UCXNQ2MPV': 8,\n",
       " 'U02TCUCA7PU': 9,\n",
       " 'U012CLUV1KJ': 10,\n",
       " 'U0332GKB9J8': 11,\n",
       " 'UT62H53R6': 12,\n",
       " 'U03RNK543HC': 13,\n",
       " 'USLACKBOT': 14,\n",
       " 'U011BDYMEG7': 15,\n",
       " 'U01TZK90VSM': 16,\n",
       " 'U02326Q5BG9': 17,\n",
       " 'U017S2GDXF0': 18,\n",
       " 'U0117AWAAEN': 19}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_map\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>prediction</th>\n",
       "      <th>test</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>start -&gt;  UCZ4VJF6J --&gt; &lt;https://venturebeat.c...</td>\n",
       "      <td>3 4 end</td>\n",
       "      <td>{'id': 'cmpl-7lcs8hQ8cAsW6sUrmkWV5eNfvMHfO', '...</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>[4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>start -&gt;  UCZ4VJF6J --&gt; &lt;https://medium.com/th...</td>\n",
       "      <td>nil end</td>\n",
       "      <td>{'id': 'cmpl-7lcs9Kl5ysRTUHy7myMDpGRc03xjT', '...</td>\n",
       "      <td>[nil]</td>\n",
       "      <td>[nil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>start -&gt;  UU5C7MNMA --&gt; &lt;https://blog.spiceai....</td>\n",
       "      <td>nil end</td>\n",
       "      <td>{'id': 'cmpl-7lcs9TgyNbYUQLqwbLkAfV7TmmjY4', '...</td>\n",
       "      <td>[nil]</td>\n",
       "      <td>[nil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>start -&gt;  U014ZU49HPT --&gt; This blog by Adobe g...</td>\n",
       "      <td>nil end</td>\n",
       "      <td>{'id': 'cmpl-7lcs9tK3fKxzTQRvZ7gvBD6lADmXe', '...</td>\n",
       "      <td>[nil]</td>\n",
       "      <td>[nil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>start -&gt;  ULJD5H2A2 --&gt; Ran across a query lan...</td>\n",
       "      <td>nil end</td>\n",
       "      <td>{'id': 'cmpl-7lcsAgSRXYIUYoj8H8soTGgLHPRll', '...</td>\n",
       "      <td>[nil]</td>\n",
       "      <td>[nil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>start -&gt;  UCZ4VJF6J --&gt; &lt;@UU5C7MNMA&gt; FYI I'm i...</td>\n",
       "      <td>5 end</td>\n",
       "      <td>{'id': 'cmpl-7lcsiVYiHV8TO0ihtRi2f329h9g42', '...</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[nil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>start -&gt;  UU5C7MNMA --&gt; &lt;@UCZ4VJF6J&gt; have you ...</td>\n",
       "      <td>1 end</td>\n",
       "      <td>{'id': 'cmpl-7lcsiiuzLAcE2w40upYhZE3jvxcVq', '...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>start -&gt;  UCZ4VJF6J --&gt; &lt;@ULJD5H2A2&gt; from read...</td>\n",
       "      <td>2 end</td>\n",
       "      <td>{'id': 'cmpl-7lcsjdzewSRjvKlmTQmXTEuz81woz', '...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>start -&gt;  UCZ4VJF6J --&gt; &lt;@U03RNK543HC&gt; &lt;@U012C...</td>\n",
       "      <td>nil end</td>\n",
       "      <td>{'id': 'cmpl-7lcsjiQSb6NRapu3c2jc9lPS2OepS', '...</td>\n",
       "      <td>[nil]</td>\n",
       "      <td>[nil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>start -&gt;  U012CLUV1KJ --&gt; &lt;@UCZ4VJF6J&gt; Tell me...</td>\n",
       "      <td>1 end</td>\n",
       "      <td>{'id': 'cmpl-7lcsj6gdOBvTq7bIjsfG5eXE32c4X', '...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>986 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prompt completion  \\\n",
       "0    start ->  UCZ4VJF6J --> <https://venturebeat.c...    3 4 end   \n",
       "1    start ->  UCZ4VJF6J --> <https://medium.com/th...    nil end   \n",
       "2    start ->  UU5C7MNMA --> <https://blog.spiceai....    nil end   \n",
       "3    start ->  U014ZU49HPT --> This blog by Adobe g...    nil end   \n",
       "4    start ->  ULJD5H2A2 --> Ran across a query lan...    nil end   \n",
       "..                                                 ...        ...   \n",
       "988  start ->  UCZ4VJF6J --> <@UU5C7MNMA> FYI I'm i...      5 end   \n",
       "989  start ->  UU5C7MNMA --> <@UCZ4VJF6J> have you ...      1 end   \n",
       "991  start ->  UCZ4VJF6J --> <@ULJD5H2A2> from read...      2 end   \n",
       "992  start ->  UCZ4VJF6J --> <@U03RNK543HC> <@U012C...    nil end   \n",
       "993  start ->  U012CLUV1KJ --> <@UCZ4VJF6J> Tell me...      1 end   \n",
       "\n",
       "                                            prediction    test   pred  \n",
       "0    {'id': 'cmpl-7lcs8hQ8cAsW6sUrmkWV5eNfvMHfO', '...  [3, 4]    [4]  \n",
       "1    {'id': 'cmpl-7lcs9Kl5ysRTUHy7myMDpGRc03xjT', '...   [nil]  [nil]  \n",
       "2    {'id': 'cmpl-7lcs9TgyNbYUQLqwbLkAfV7TmmjY4', '...   [nil]  [nil]  \n",
       "3    {'id': 'cmpl-7lcs9tK3fKxzTQRvZ7gvBD6lADmXe', '...   [nil]  [nil]  \n",
       "4    {'id': 'cmpl-7lcsAgSRXYIUYoj8H8soTGgLHPRll', '...   [nil]  [nil]  \n",
       "..                                                 ...     ...    ...  \n",
       "988  {'id': 'cmpl-7lcsiVYiHV8TO0ihtRi2f329h9g42', '...     [5]  [nil]  \n",
       "989  {'id': 'cmpl-7lcsiiuzLAcE2w40upYhZE3jvxcVq', '...     [1]    [1]  \n",
       "991  {'id': 'cmpl-7lcsjdzewSRjvKlmTQmXTEuz81woz', '...     [2]    [2]  \n",
       "992  {'id': 'cmpl-7lcsjiQSb6NRapu3c2jc9lPS2OepS', '...   [nil]  [nil]  \n",
       "993  {'id': 'cmpl-7lcsj6gdOBvTq7bIjsfG5eXE32c4X', '...     [1]    [1]  \n",
       "\n",
       "[986 rows x 5 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pandas.read_json(f'{work_dir}/conversation_user_examples_prepared_valid_with_pred.jsonl', lines=True)\n",
    "df[\"test\"] = None\n",
    "df[\"pred\"] = None\n",
    "\n",
    "for i in range(len(df)):\n",
    "    completions = df['completion'][i].removesuffix(\" end\").strip().split()\n",
    "    df.at[i, \"test\"] = completions\n",
    "    prediction = df['prediction'][i]\n",
    "    if \"choices\" in prediction:\n",
    "        predictions = prediction[\"choices\"][0][\"text\"].strip().split()\n",
    "        df.at[i, \"pred\"] = predictions\n",
    "df = df[df.pred.notnull()]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7411962064681381"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([['nil', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19']])\n",
    "y_test_transformed = mlb.transform(df['test'])\n",
    "y_pred_transformed = mlb.transform(df['pred'])\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_test_transformed, y_pred_transformed, average='weighted', zero_division=np.nan)  # Or 'micro', 'weighted' based on need\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['4', 'nil', '3', '6', '5', '1', '2', '10', '11'], dtype=object)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"pred\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19',\n",
       "       '2', '3', '4', '5', '6', '7', '8', '9', 'nil'], dtype=object)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m        set\n",
      "\u001b[0;31mString form:\u001b[0m {'thriller', 'sci-fi'}\n",
      "\u001b[0;31mLength:\u001b[0m      2\n",
      "\u001b[0;31mDocstring:\u001b[0m  \n",
      "set() -> new empty set object\n",
      "set(iterable) -> new set object\n",
      "\n",
      "Build an unordered collection of unique elements."
     ]
    }
   ],
   "source": [
    "x = {'sci-fi', 'thriller'}\n",
    "x?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "conversion_map = {}\n",
    "with open(f'{work_dir}/conversation_users_conversion_map.json') as file:\n",
    "    conversion_map = json.load(file)\n",
    "\n",
    "def convert_users_in_line(line):\n",
    "    for in_user in conversion_map:\n",
    "        out_user = conversion_map[in_user]\n",
    "\n",
    "        line = line.replace(in_user, out_user)\n",
    "    return line\n",
    "\n",
    "\n",
    "with open(f'{work_dir}/conversation_user_examples_prepared_train.jsonl', 'r') as in_file:\n",
    "    with open(f'{work_dir}/conversation_user_examples_prepared_train_converted.jsonl', 'w') as out_file:\n",
    "        while True:\n",
    "            line = in_file.readline()\n",
    "\n",
    "            if not line:\n",
    "                break\n",
    "\n",
    "            out_file.write(convert_users_in_line(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) new train set with prompts without userIds at the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = open(f'{work_dir}/message_conversations.jsonl', 'r')\n",
    "out_file = open(f'{work_dir}/conversation_user_noprompts_examples.jsonl', 'w')\n",
    "\n",
    "# gets a list of all users that reacted to a message\n",
    "def get_reaction_users(message):\n",
    "    users = []\n",
    "    if message[\"reactions\"]:\n",
    "        for reaction in message[\"reactions\"]:\n",
    "            users.extend(reaction[\"users\"])\n",
    "    return users\n",
    "\n",
    "prompt_separator = \"\\n\\n###\\n\\n\"\n",
    "completion_separator = \" end\"\n",
    "\n",
    "def output_conversation_example(conversation):\n",
    "    if len(conversation) > 0:\n",
    "        prompt_lines = []\n",
    "        initial_user = conversation[0][\"user\"]\n",
    "        users = []\n",
    "        for message in conversation:\n",
    "            user = message[\"user\"]\n",
    "            users.append(user)\n",
    "            users.extend(get_reaction_users(message))\n",
    "            \n",
    "            text = message[\"text\"]\n",
    "            if text == \"\" or text.find(\"```\") >= 0:\n",
    "                continue\n",
    "\n",
    "            prompt_lines.append(f' {text} ')\n",
    "\n",
    "        if len(prompt_lines) == 0:\n",
    "            return\n",
    "        \n",
    "        prompt = \"\\n\\n\".join(prompt_lines)\n",
    "\n",
    "        prompt = convert_users_in_line(prompt).strip()\n",
    "\n",
    "        # de-duplicate users in the list.\n",
    "        users = list(dict.fromkeys(users))\n",
    "\n",
    "        # remove the conversation starter\n",
    "        if initial_user in users:\n",
    "            users.remove(initial_user)\n",
    "\n",
    "        # convert user_ids to user_nums\n",
    "        user_nums = []\n",
    "        for user in users:\n",
    "            if user in user_map:\n",
    "                user_nums.append(f'{user_map[user]}')\n",
    "\n",
    "        completion = \" \".join(user_nums) if len(user_nums) > 0 else \"nil\"\n",
    "        \n",
    "        example = { \"prompt\": f'{prompt}{prompt_separator}', \"completion\": f' {completion}{completion_separator}' }\n",
    "        out_file.write(json.dumps(example) + \"\\n\")\n",
    "\n",
    "           \n",
    "current_channel = \"\"\n",
    "current_conversation = []\n",
    "conversation_ts = None\n",
    "\n",
    "while True:\n",
    "    output_convo = False\n",
    "    line = in_file.readline()\n",
    "\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "    message = json.loads(line)\n",
    "    channel = message[\"channel\"]\n",
    "    thread_ts = message[\"thread_ts\"]\n",
    "    user = message[\"user\"]\n",
    "\n",
    "    # output if channel changes\n",
    "    if channel != current_channel:\n",
    "        current_channel = channel\n",
    "        output_convo = True\n",
    "\n",
    "    # output if different thread_ts\n",
    "    if conversation_ts and conversation_ts != thread_ts:\n",
    "        output_convo = True\n",
    "\n",
    "    # output and reset\n",
    "    if output_convo:\n",
    "        output_conversation_example(current_conversation)\n",
    "        current_conversation = []\n",
    "\n",
    "    current_conversation.append(message)\n",
    "    conversation_ts = thread_ts\n",
    "   \n",
    "\n",
    "# output final conversation\n",
    "output_conversation_example(current_conversation)\n",
    "\n",
    "in_file.close()\n",
    "out_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) new train set with all userIds and weblinks removed from prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I faintly remember something about putting metrics into  \\n\\n Something I can help wit'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def strip_links_and_users(line):\n",
    "    return re.sub(r\"<.*?>\", '', line)\n",
    "\n",
    "strip_links_and_users(\"<@U052RFMBRF0> I faintly remember something about putting metrics into <https:\\/\\/gitlab.com\\/kaskada\\/kaskada\\/-\\/blob\\/main\\/wren\\/docker-compose.yml#L61> \\n\\n Something I can help wit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = open(f'{work_dir}/message_conversations.jsonl', 'r')\n",
    "out_file = open(f'{work_dir}/conversation_user_stripped_examples.jsonl', 'w')\n",
    "\n",
    "# gets a list of all users that reacted to a message\n",
    "def get_reaction_users(message):\n",
    "    users = []\n",
    "    if message[\"reactions\"]:\n",
    "        for reaction in message[\"reactions\"]:\n",
    "            users.extend(reaction[\"users\"])\n",
    "    return users\n",
    "\n",
    "prompt_separator = \"\\n\\n###\\n\\n\"\n",
    "completion_separator = \" end\"\n",
    "\n",
    "def output_conversation_example(conversation):\n",
    "    if len(conversation) > 0:\n",
    "        prompt_lines = []\n",
    "        initial_user = conversation[0][\"user\"]\n",
    "        users = []\n",
    "        for message in conversation:\n",
    "            user = message[\"user\"]\n",
    "            users.append(user)\n",
    "            users.extend(get_reaction_users(message))\n",
    "            \n",
    "            text = message[\"text\"]\n",
    "            if text == \"\" or text.find(\"```\") >= 0:\n",
    "                continue\n",
    "\n",
    "            prompt_lines.append(f' {text} ')\n",
    "\n",
    "        if len(prompt_lines) == 0:\n",
    "            return\n",
    "        \n",
    "        prompt = \"\\n\\n\".join(prompt_lines)\n",
    "\n",
    "        prompt = strip_links_and_users(prompt).strip()\n",
    "\n",
    "        if len(prompt) < 25:\n",
    "            return\n",
    "\n",
    "        # de-duplicate users in the list.\n",
    "        users = list(dict.fromkeys(users))\n",
    "\n",
    "        # remove the conversation starter\n",
    "        if initial_user in users:\n",
    "            users.remove(initial_user)\n",
    "\n",
    "        # convert user_ids to user_nums\n",
    "        user_nums = []\n",
    "        for user in users:\n",
    "            if user in user_map:\n",
    "                user_nums.append(f'{user_map[user]}')\n",
    "\n",
    "        completion = \" \".join(user_nums) if len(user_nums) > 0 else \"nil\"\n",
    "        \n",
    "        example = { \"prompt\": f'{prompt}{prompt_separator}', \"completion\": f' {completion}{completion_separator}' }\n",
    "        out_file.write(json.dumps(example) + \"\\n\")\n",
    "\n",
    "           \n",
    "current_channel = \"\"\n",
    "current_conversation = []\n",
    "conversation_ts = None\n",
    "\n",
    "while True:\n",
    "    output_convo = False\n",
    "    line = in_file.readline()\n",
    "\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "    message = json.loads(line)\n",
    "    channel = message[\"channel\"]\n",
    "    thread_ts = message[\"thread_ts\"]\n",
    "    user = message[\"user\"]\n",
    "\n",
    "    # output if channel changes\n",
    "    if channel != current_channel:\n",
    "        current_channel = channel\n",
    "        output_convo = True\n",
    "\n",
    "    # output if different thread_ts\n",
    "    if conversation_ts and conversation_ts != thread_ts:\n",
    "        output_convo = True\n",
    "\n",
    "    # output and reset\n",
    "    if output_convo:\n",
    "        output_conversation_example(current_conversation)\n",
    "        current_conversation = []\n",
    "\n",
    "    current_conversation.append(message)\n",
    "    conversation_ts = thread_ts\n",
    "   \n",
    "\n",
    "# output final conversation\n",
    "output_conversation_example(current_conversation)\n",
    "\n",
    "in_file.close()\n",
    "out_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find High User Probs inside validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "Had some fun with rust over the weekend:  \n",
      "\n",
      " Any thoughts on async and how hard it will be to replace threads? I think getting off channels simplifies a few things. \n",
      "\n",
      " Honestly I haven’t looked any further than the merger. I will say that working with the async syntax is really nice, and streams behave like a regular iterator which is also nice. I spent some time trying to do this with `StreamExt` rather than dropping down to `poll_next` - I don’t think it’s possible but the stream combinators felt very natural. \n",
      "\n",
      " Being able to us rusoto to read from S3 would also be a big improvement \n",
      "\n",
      " Ironically, I built a janky version of streams myself back in the days before async was a thing:  \n",
      "\n",
      " haha \n",
      "\n",
      " did you run into any places that you needed to deal with pinning? \n",
      "\n",
      " I’m still a little hazy on pinning. AFAICT everything needs to be pinned, but it hasn’t caused problems anywhere yet. The merger expects its inputs to be box-pinned. I’m using `pin_mut!` before polling a future which may be a problem, but I think the solution is just to box-pin the future. \n",
      "\n",
      " Makes sense. I think the pinning is just \"this can't move\" \n",
      "\n",
      " (at least is the intuition, but I don't understand what the implications / tradeoffs are there) \n",
      "\n",
      " That’s my understanding. I think it basically means that anything long-lived needs to go on the heap. \n",
      "\n",
      " But then how does a `Pin&lt;T&gt;` differ from a `Box&lt;T&gt;`? If it is just \"goes on the heap\", isn't that a `Box`?\n",
      "\n",
      "Completions: ['ben']\n",
      "Predictions: {'ben': 0.6010349638595963}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "I think you’re asking if sliding windows are continuous or discrete, no? \n",
      "\n",
      " Are there currently cases where aggregations produce discrete values? I remember there was some talk about it but I think we ended up not going that route? \n",
      "\n",
      " Correct, aggregations only produce continuous values now (including windowed aggregations with `since`)\n",
      "\n",
      "Completions: ['ryan']\n",
      "Predictions: {'ben': 0.6691625525284406}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "I guess there is no reason that the `finished` tick *isn't* at `DateTime::MAX`, right? The only benefit of `max(event_time) + 1` is that you would see the `_time` increasing. But since it is \"the end of time\", they're both equally reasonable? \n",
      "\n",
      " Bah. Still has problems getting negative times. Once I'm out of meetings I'll try to debug through and see what's going on. I may need to do some pair debugging this afternoon if it's still busted. We can trade pairing :wink: \n",
      "\n",
      " Haha okay deal\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.7038597017298531}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "re: It would need to be `prev(n, foo)` to expand like that\n",
      "\n",
      "Isn't it possible for us to match on aggregation arguments, and if we see a `prev`, we use the `arg[0]` to expand `prev` into `sliding(n, is_valid(arg[0]))`? \n",
      "\n",
      " But a function can't match on that. It is just a rewrite `prev(n, e) = sliding(n, is_valid(e))` for instance.\n",
      "\n",
      "If we wanted it to happen that way, it would need to be a separate window behavior (at least in the frontend).\n",
      "\n",
      "It also seems that \"previous\" is ambiguous. Eg., if we just right `prev(n)` -- previous `n` what?  It seems like we'd actually want it to be previous `n` *new* values of the input (not previous `n` non-null values of the input, which would cause it to get cleared really early, since it has values at every row) \n",
      "\n",
      " Agreed it would need to be a separate window behavior \n",
      "\n",
      " And yeah, seems like we'd want to have the `is_new` behavior copied here, which would be tough\n",
      "\n",
      "Completions: ['ben']\n",
      "Predictions: {'ben': 0.6016017243965299}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "any new thoughts on this? If not, I think I'm leaning towards saying \"the current (understanding of) behavior is the intended behavior\", and then just reviewing the tests to make sure that is what we were seeing, and calling it done for now. \n",
      "\n",
      " responded with comment. The slicing aspect is interesting \n",
      "\n",
      " This feels like a product question at this point though, since we have arguments for either side \n",
      "\n",
      " The slicing case isn't really a question though, and seems unrelated. \n",
      "\n",
      " Specifically, slicing has already been decined as: when we do a lookup, the \"foreign\" uses are 100% even if the \"primary\" uses are 1% \n",
      "\n",
      " The only point of mentioning slicing was to say, that while saying \"all tables in any uses\" may make sense for the current behavior, it didn't match the decision there \n",
      "\n",
      " Sure, but the outputs are drastically larger \n",
      "\n",
      " (Could be, depending on the size of the file being looked into) \n",
      "\n",
      " which, as you say, we cannot slice \n",
      "\n",
      " Ah. Hmm... that's fair. So your point is:\n",
      "\n",
      "1. For slicing, we need to include all entities in the foreign table in order for the lookup results to be correct.\n",
      "2. But, including all entities there means that if we include tables used only in foreign positions than the entities ticked by the self-lookup will include all entities, not only those in the slice, which may defeat the purpose (at least in terms of output size, but not necessarily compute cost).\n",
      "Which I think may make sense. \n",
      "\n",
      " Correct \n",
      "\n",
      " Actually... I think we can go further -- in that case, the results for those entities may be wrong. Consider an entity that was filtered out of the primary use but not of the foreign use. Such an entity would show up with primary-aggregates of 0 / `null` even though it should have had data. \n",
      "\n",
      " oh yes that's an even better point \n",
      "\n",
      " I think that means that the only \"correct\" answer is definition 1\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.6617092625176884}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "is the ready to review version of the data manager. It *should* allow passing S3 paths to query. I have a small / hacky wren change that tries that and it seems to pass things (except for deadline exceeded, so we'll have to see after we do the prefetch) \n",
      "\n",
      " Cool taking a look. Down to pair now on things, or can start looking at gatherer and we pair up this afternoon \n",
      "\n",
      " Sounds good -- maybe after you've taken a look, I can send that against the merge train and we can look at pairing. Should be pretty straightforward I think \n",
      "\n",
      " I'm gonna run a quick errand -- should be back in under 30 hopefully\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.6908036631215133}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "is the prefetch MR ready to go? I've tested (I think) with a later Wren change and it seems to be working. \n",
      "\n",
      " looks good \n",
      "\n",
      " Cool. Anything we should sync on anything? I'll probably start trying to look at having Sparrow do the upload next (and defer the async Parquet reader) until Monday if the upload gets done. \n",
      "\n",
      " Okay cool, that sounds good to me\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.6395226118172433}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "I have an MR that adds (really hacky) support to Wren and passes a test, so I can confirm that it works. But, I don't want to try to check that in (has problems such as Materialization needing to then download the file, etc.). My plan is to try to get the upload MR in, then send a draft of the Wren changes and see if Eric can help me make sense of things next week. I *suspect* this may simplify a lot of the wren code significantly. \n",
      "\n",
      " Okay nice will take a look  \n",
      "\n",
      " Cool. I'm gonna head out soon, but if there is anything you want me to review I'll probably take a look later tonight, just let me know\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.6625525920805474}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "once both our MRs are in, we should see if there is a way to simplify windowing stuff in the DFG. Ideally, it is either more like the AST or more like the plan, but not the weird special snowflake thing. \n",
      "\n",
      " Yeah - I'm definitely leaning towards flattening/conversion earlier before DFG \n",
      "\n",
      " yeah, I'd like that \n",
      "\n",
      " But - does that reduce the readability of the plan for the user? \n",
      "\n",
      " basically, AST to DFG provides the behaviors \n",
      "\n",
      " hmm... depends on how it shows up in the aggregation (you can hover and see the proto, etc.) \n",
      "\n",
      " let's plan on discussing this week? \n",
      "\n",
      " I think different facets of it are pretty fresh for both of us, so good time to at least figure things out\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.6503924884102247}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "any use discussing ? think any of the gitlab comments should be added as comments in the code? \n",
      "\n",
      " unless we start seeing that error happen \n",
      "\n",
      " that makes sense -- I also suspect that as we flesh out the compilation that may become easier to handle / avoid / discover \n",
      "\n",
      " (specifically, I still need to figure out how to implement `transform` while going to the plan, which I *think* has implications on this) \n",
      "\n",
      " Cool -- I'll get that in the queue \n",
      "\n",
      " I have a draft (not yet connected up) version of execution for an operation but I'm gonna have to hop out for my piano lesson. I'll try to get that out this afternoon, so we can discuss later today / monday. Then I'll probably be thinking about how to implement `transform`. Once that's done, I think the rest is mostly mechanical / copying old code to new / etc. \n",
      "\n",
      " Nice, sounds good\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.6081832770038934}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "free for a bit if there is anything to discuss. Otherwise, my next two or three steps are probably (a) compiling the `select` operation (which will likely bet the general shape of the solution for `transform` in general and (b) executing the compiled `select` operation (and connecting multiple operations). May be worth hoping on a quick discussion of naming re: the other comment on the types MR so that can go in. \n",
      "\n",
      " Going to start looking at some docs today, so if there's any wildly off I'll ping you\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.6792286723437944}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "I refactored the tick stuff, just working on adding unit tests now \n",
      "\n",
      " Cool. I'll take another look. We can sync on next steps on compilation / etc. later? \n",
      "\n",
      " Sounds good - all of my unit tests will pass on the first try(tm) so hopefully have that out in the next hour \n",
      "\n",
      " generally looking good -- I'll look at the tests to make sure we get decent coverage of the complex cases, but otherwise looks like it should be good to go\n",
      "\n",
      "Completions: ['ben']\n",
      "Predictions: {'jordan': 0.6952516379192001}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "should I self approve the spread kernels? Want to get the next step sent out for review (operation refactoring / plumbing) which is probably more useful for review anyway (the spread kernels are a lot of code with a very small API) \n",
      "\n",
      " Almost done looking at it now \n",
      "\n",
      " sounds good -- I'll make a pass over the next MR to see if there is anything to clean up before sending it, and then I'll be ready to send that out\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.6202313223271704}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "Thoughts on changing the format of input/output for tests from CSV to JSON? It would make some things easier/possible to test (nulls, nested structs, lists, etc.) \n",
      "\n",
      " Seems fine to me  \n",
      "\n",
      " Cool. I need it in some of the lookup stuff I'm doing, but I'll just introduce a parallel method for testing with JSON input/output. In a future MR I'll probably tear out the CSV stuff and switch everything over. I don't want there to be code changes in flight then (merge conflicts) and want it to be after we get most other stuff straightened out.\n",
      "\n",
      "Completions: ['theo', 'jordan']\n",
      "Predictions: {'jordan': 0.7132742572522212}\n",
      "\n",
      "Total Examples: 14 of 925 lines.\n",
      "User example count: {'ben': 3, 'jordan': 11}\n"
     ]
    }
   ],
   "source": [
    "min_prob = 0.60\n",
    "\n",
    "user_name = {\"1\": \"ben\", \"2\": \"ryan\", \"3\": \"marcial\", \"4\": \"charna\", \"5\": \"eric\", \"6\": \"kevinn\", \"7\": \"tina\", \"8\": \"davor\", \"9\": \"karina\", \"10\": \"jordan\", \"11\": \"brian\", \"12\": \"janoo\", \"13\": \"theo\", \"15\": \"darci\", \"19\": \"bradley\"}\n",
    "\n",
    "file_b =\"conversation_user_noprompts_examples_prepared_valid_with_pred\"\n",
    "file_c =\"conversation_user_stripped_examples_prepared_valid_with_pred\"\n",
    "\n",
    "example_count = 0\n",
    "line_count = 0\n",
    "user_example_count = {}\n",
    "with open(f'{work_dir}/{file_c}.jsonl') as file:\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "\n",
    "        if not line:\n",
    "            break\n",
    "\n",
    "        data = json.loads(line)\n",
    "        if 'choices' not in data['prediction']:\n",
    "            continue\n",
    "\n",
    "        line_count += 1\n",
    "        logprobs = data['prediction']['choices'][0]['logprobs']['top_logprobs'][0]\n",
    "\n",
    "        high_users = {}\n",
    "        for user in logprobs:\n",
    "            logprob = logprobs[user]\n",
    "            user = user.strip()\n",
    "            if user == 'nil':\n",
    "                continue\n",
    "            prob = math.exp(logprob)\n",
    "            if prob > min_prob:\n",
    "                name = user_name[user]\n",
    "                high_users[name] = prob\n",
    "                if name in user_example_count:\n",
    "                    user_example_count[name] += 1 \n",
    "                else:\n",
    "                    user_example_count[name] = 1\n",
    "\n",
    "\n",
    "        if len(high_users) > 0:\n",
    "            example_count += 1\n",
    "            print(\"\\nFound example with high probability:\\n\")\n",
    "\n",
    "            comp_users = []\n",
    "            completions = data[\"completion\"].removesuffix(\"end\").strip().split(\" \")\n",
    "            for completion in completions:\n",
    "                comp_name = user_name[completion] if completion in user_name else completion\n",
    "                comp_users.append(comp_name)\n",
    "\n",
    "            prompt = data[\"prompt\"].removesuffix('\\n\\n###\\n\\n')\n",
    "\n",
    "            print(f'Prompt:\\n{prompt}\\n')\n",
    "            print(f'Completions: {comp_users}')\n",
    "            print(f'Predictions: {high_users}')\n",
    "\n",
    "print()\n",
    "print(f'Total Examples: {example_count} of {line_count} lines.')\n",
    "print(f'User example count: {user_example_count}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
