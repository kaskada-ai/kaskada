{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversations\n",
    "\n",
    "### For a set of recent messages in a \"conversation\", try to predict the set of users that might interact next.\n",
    "\n",
    "A \"conversation\" is defined as either:\n",
    "* All the messages in a thread\n",
    "* A collection of messages from a single channel that occur in succession. If no response is made for 10 minutes, the conversation has ended. The next message outside this window is the start of a new conversation.\n",
    "\n",
    "\n",
    "For each set of messages in a \"conversation\", build a user set from the following properties:\n",
    "* users that reacted to the conversation\n",
    "* users that participated in the conversation\n",
    "* exclude the conversation starter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade openai pip install file-read-backwards  -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, openai, time, pandas, random, getpass\n",
    "from openai import cli\n",
    "from types import SimpleNamespace\n",
    "from sklearn.model_selection import train_test_split\n",
    "from file_read_backwards import FileReadBackwards\n",
    "\n",
    "work_dir = \"/Users/eric.pinzur/Documents/slackbot2000\"\n",
    "openai.api_key = getpass.getpass(prompt=\"Please enter your OpenAI API Key\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Data Prep\n",
    "\n",
    "Convert the original input data into a set of \"conversations\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into \"threads\" and \"non-threads\"\n",
    "\n",
    "Using DuckDB:\n",
    "\n",
    "```sql\n",
    "copy(\n",
    "    select * from \n",
    "    read_json_auto('messages.jsonl', format='newline_delimited') \n",
    "    where thread_ts is not null \n",
    "    order by channel, thread_ts, ts\n",
    ") to 'message_threads.jsonl' (FORMAT JSON);\n",
    "\n",
    "copy(\n",
    "    select * from \n",
    "    read_json_auto('messages.jsonl', format='newline_delimited') \n",
    "    where thread_ts is null \n",
    "    order by channel, ts\n",
    ") to 'message_non_threads.jsonl' (FORMAT JSON);\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make threads from non-threads\n",
    "\n",
    "For non-threads, artifically group the messages into \"threads\".  Collect messages from a channel.  If there is a 5 minute gap between messages, convert the message collection to a \"thread\" and export.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = open(f'{work_dir}/message_non_threads.jsonl', 'r')\n",
    "out_file = open(f'{work_dir}/message_non_threads_threads.jsonl', 'w')\n",
    "\n",
    "def write_thread(thread):\n",
    "    if len(thread) > 0:\n",
    "        reply_users = []\n",
    "        for message in thread:\n",
    "            if message[\"user\"] not in reply_users:\n",
    "                reply_users.append(message[\"user\"])\n",
    "        thread[0][\"reply_users\"] = reply_users\n",
    "        thread_ts = thread[0][\"ts\"]\n",
    "        for message in thread:\n",
    "            message[\"thread_ts\"] = thread_ts\n",
    "            out_file.write(json.dumps(message)+\"\\n\")\n",
    "\n",
    "next_thread = []\n",
    "current_channel = \"\"\n",
    "last_msg_ts = None\n",
    "while True:\n",
    "    output_thread = False\n",
    "    line = in_file.readline()\n",
    "\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "    message = json.loads(line)\n",
    "\n",
    "    channel = message[\"channel\"]\n",
    "\n",
    "    # output if channel changes in the file\n",
    "    if channel != current_channel:\n",
    "        current_channel = channel\n",
    "        output_thread = True\n",
    "\n",
    "    # output if message timestamp is more than 10 mins beyond last message\n",
    "    if last_msg_ts and message[\"ts\"] > last_msg_ts + 600:\n",
    "        output_thread = True\n",
    "\n",
    "    # output and reset\n",
    "    if output_thread:\n",
    "        write_thread(next_thread)\n",
    "        next_thread = []\n",
    "        last_msg_ts = None\n",
    "\n",
    "    next_thread.append(message)\n",
    "    last_msg_ts = message[\"ts\"]\n",
    "\n",
    "# output final thread\n",
    "write_thread(next_thread)\n",
    "\n",
    "in_file.close()\n",
    "out_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Conversations\n",
    "\n",
    "Re-join the two files into a set of conversations, using duckDB:\n",
    "\n",
    "```sql\n",
    "copy(\n",
    "    select * from \n",
    "    read_json_auto(['message_non_threads_threads.jsonl', 'message_threads.jsonl'], format='newline_delimited') \n",
    "    order by channel, thread_ts, ts\n",
    ") to 'message_conversations.jsonl' (FORMAT JSON);\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Examples\n",
    "\n",
    "This is a Generative problem, so the goals for training aren't as strict.\n",
    "\n",
    "Goals:\n",
    "* prompt and completion length must not be longer than 2048 tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy\n",
    "\n",
    "* Group messages int \"conversations\".\n",
    "* Use a method to write examples for a \"conversation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a user map\n",
    "in_file = open(f'{work_dir}/message_conversations.jsonl', 'r')\n",
    "user_map = {}\n",
    "user_count = 0\n",
    "\n",
    "while True:\n",
    "    line = in_file.readline()\n",
    "\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "    message = json.loads(line)\n",
    "    user = message[\"user\"]\n",
    "\n",
    "    # create a user_id -> user_num map\n",
    "    if user not in user_map:\n",
    "        user_count += 1\n",
    "        user_map[user] = user_count\n",
    "\n",
    "in_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UCZ4VJF6J': 1,\n",
       " 'ULJD5H2A2': 2,\n",
       " 'U017T5TFW58': 3,\n",
       " 'U014ZU49HPT': 4,\n",
       " 'UU5C7MNMA': 5,\n",
       " 'U016TM9NXEY': 6,\n",
       " 'U021KG8NMRQ': 7,\n",
       " 'UCXNQ2MPV': 8,\n",
       " 'U02TCUCA7PU': 9,\n",
       " 'U012CLUV1KJ': 10,\n",
       " 'U0332GKB9J8': 11,\n",
       " 'UT62H53R6': 12,\n",
       " 'U03RNK543HC': 13,\n",
       " 'USLACKBOT': 14,\n",
       " 'U011BDYMEG7': 15,\n",
       " 'U01TZK90VSM': 16,\n",
       " 'U02326Q5BG9': 17,\n",
       " 'U017S2GDXF0': 18,\n",
       " 'U0117AWAAEN': 19}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = open(f'{work_dir}/message_conversations.jsonl', 'r')\n",
    "out_file = open(f'{work_dir}/conversation_user_examples.jsonl', 'w')\n",
    "\n",
    "# gets a list of all users that reacted to a message\n",
    "def get_reaction_users(message):\n",
    "    users = []\n",
    "    if message[\"reactions\"]:\n",
    "        for reaction in message[\"reactions\"]:\n",
    "            users.extend(reaction[\"users\"])\n",
    "    return users\n",
    "\n",
    "prompt_separator = \"\\n\\n###\\n\\n\"\n",
    "completion_separator = \" end\"\n",
    "\n",
    "def output_conversation_example(conversation):\n",
    "    if len(conversation) > 0:\n",
    "        prompt_lines = []\n",
    "        initial_user = conversation[0][\"user\"]\n",
    "        users = []\n",
    "        for message in conversation:\n",
    "            user = message[\"user\"]\n",
    "            users.append(user)\n",
    "            users.extend(get_reaction_users(message))\n",
    "            \n",
    "            text = message[\"text\"]\n",
    "            if text == \"\" or text.find(\"```\") >= 0:\n",
    "                continue\n",
    "\n",
    "            prompt_lines.append(f' {user} --> {text} ')\n",
    "\n",
    "        if len(prompt_lines) == 0:\n",
    "            return\n",
    "        \n",
    "        prompt = \"\\n\\n\".join(prompt_lines)\n",
    "\n",
    "        # de-duplicate users in the list.\n",
    "        users = list(dict.fromkeys(users))\n",
    "\n",
    "        # remove the conversation starter\n",
    "        if initial_user in users:\n",
    "            users.remove(initial_user)\n",
    "\n",
    "        # convert user_ids to user_nums\n",
    "        user_nums = []\n",
    "        for user in users:\n",
    "            if user in user_map:\n",
    "                user_nums.append(f'{user_map[user]}')\n",
    "\n",
    "        completion = \" \".join(user_nums) if len(user_nums) > 0 else \"nil\"\n",
    "        \n",
    "        example = { \"prompt\": f'start -> {prompt}{prompt_separator}', \"completion\": f' {completion}{completion_separator}' }\n",
    "        out_file.write(json.dumps(example) + \"\\n\")\n",
    "\n",
    "           \n",
    "current_channel = \"\"\n",
    "current_conversation = []\n",
    "conversation_ts = None\n",
    "\n",
    "while True:\n",
    "    output_convo = False\n",
    "    line = in_file.readline()\n",
    "\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "    message = json.loads(line)\n",
    "    channel = message[\"channel\"]\n",
    "    thread_ts = message[\"thread_ts\"]\n",
    "    user = message[\"user\"]\n",
    "\n",
    "\n",
    "\n",
    "    # output if channel changes\n",
    "    if channel != current_channel:\n",
    "        current_channel = channel\n",
    "        output_convo = True\n",
    "\n",
    "    # output if different thread_ts\n",
    "    if conversation_ts and conversation_ts != thread_ts:\n",
    "        output_convo = True\n",
    "\n",
    "    # output and reset\n",
    "    if output_convo:\n",
    "        output_conversation_example(current_conversation)\n",
    "        current_conversation = []\n",
    "\n",
    "    current_conversation.append(message)\n",
    "    conversation_ts = thread_ts\n",
    "   \n",
    "\n",
    "# output final conversation\n",
    "output_conversation_example(current_conversation)\n",
    "\n",
    "in_file.close()\n",
    "out_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Verification & Split Stage\n",
    "\n",
    "* make sure prompts end with same suffix\n",
    "* remove too long examples\n",
    "* remove duplicated examples\n",
    "\n",
    "Note, we aren't doing classification, so don't start a fine-tune as suggested by the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Your file contains 5295 prompt-completion pairs\n",
      "- Based on your data it seems like you're trying to fine-tune a model for classification\n",
      "- For classification, we recommend you try one of the faster and cheaper models, such as `ada`\n",
      "- For classification, you can estimate the expected model performance by keeping a held out dataset, which is not used for training\n",
      "- There are 55 duplicated prompt-completion sets. These are rows: [740, 772, 925, 932, 935, 936, 957, 1015, 1016, 1079, 1093, 1131, 1136, 1446, 1646, 1716, 2869, 2903, 2956, 2969, 3070, 3099, 3200, 3225, 3318, 3366, 3424, 3438, 3652, 3840, 3877, 3904, 3914, 4003, 4011, 4134, 4151, 4218, 4254, 4263, 4264, 4323, 4384, 4422, 4502, 4505, 4587, 4662, 4829, 4843, 4885, 4996, 5020, 5054, 5061]\n",
      "- There are 2 examples that are very long. These are rows: [146, 1906]\n",
      "For conditional generation, and for classification the examples shouldn't be longer than 2048 tokens.\n",
      "- All prompts end with suffix `\\n\\n###\\n\\n`\n",
      "\n",
      "Based on the analysis we will perform the following actions:\n",
      "- [Recommended] Remove 55 duplicate rows [Y/n]: Y\n",
      "- [Recommended] Remove 2 long examples [Y/n]: Y\n",
      "The indices of the long examples has changed as a result of a previously applied recommendation.\n",
      "The 2 long examples to be dropped are now at the following indices: [146, 1890]\n",
      "- [Recommended] Would you like to split into training and validation set? [Y/n]: Y\n",
      "\n",
      "\n",
      "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
      "\n",
      "Wrote modified files to `/Users/eric.pinzur/Documents/slackbot2000/conversation_user_noprompts_examples_prepared_train.jsonl` and `/Users/eric.pinzur/Documents/slackbot2000/conversation_user_noprompts_examples_prepared_valid.jsonl`\n",
      "Feel free to take a look!\n",
      "\n",
      "Now use that file when fine-tuning:\n",
      "> openai api fine_tunes.create -t \"/Users/eric.pinzur/Documents/slackbot2000/conversation_user_noprompts_examples_prepared_train.jsonl\" -v \"/Users/eric.pinzur/Documents/slackbot2000/conversation_user_noprompts_examples_prepared_valid.jsonl\" --compute_classification_metrics --classification_n_classes 334\n",
      "\n",
      "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string `\\n\\n###\\n\\n` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\" end\"]` so that the generated texts ends at the expected place.\n",
      "Once your model starts training, it'll approximately take 2.13 hours to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
     ]
    }
   ],
   "source": [
    "args = SimpleNamespace(file=f'{work_dir}/conversation_user_noprompts_examples.jsonl', quiet=True)\n",
    "cli.FineTune.prepare_data(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Stage\n",
    "\n",
    "* First need to upload the training & validation files to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 2.67M/2.67M [00:00<00:00, 3.71Git/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/eric.pinzur/Documents/slackbot2000/conversation_user_examples_prepared_train_converted.jsonl: file-Aay9NHb6yx1rLFlGJpFgkD7f\n",
      "Status (training_file): uploaded \n",
      "Status (training_file): uploaded \n",
      "Status (training_file): uploaded \n",
      "Status (training_file): uploaded \n",
      "Status (training_file): processed \n"
     ]
    }
   ],
   "source": [
    "training_file_name = f'{work_dir}/conversation_user_noprompts_examples_prepared_train.jsonl'\n",
    "\n",
    "def check_status(training_id):\n",
    "    train_status = openai.File.retrieve(training_id)[\"status\"]\n",
    "    print(f'Status (training_file): {train_status} ')\n",
    "    return (train_status)\n",
    "\n",
    "# Upload the training and validation dataset files to Azure OpenAI.\n",
    "training_id = cli.FineTune._get_or_upload(training_file_name, True)\n",
    "\n",
    "# Check on the upload status of the training dataset file.\n",
    "(train_status) = check_status(training_id)\n",
    "\n",
    "# Poll and display the upload status once a second until both files have either\n",
    "# succeeded or failed to upload.\n",
    "while train_status not in [\"succeeded\", \"failed\", \"processed\"]:\n",
    "    time.sleep(1)\n",
    "    (train_status) = check_status(training_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a fine-tuning Job\n",
    "\n",
    "* no validation file since this is not a classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning model with job ID: \"ft-qnHHxoNViVDplGDgIGn0kEC9\"\n"
     ]
    }
   ],
   "source": [
    "# This example defines a fine-tune job that creates a customized model based on curie, \n",
    "# with just a single pass through the training data. The job also provides classification-\n",
    "# specific metrics, using our validation data, at the end of that epoch.\n",
    "create_args = {\n",
    "    \"training_file\": training_id,\n",
    "    \"model\": \"davinci\",\n",
    "    \"n_epochs\": 2,\n",
    "    \"learning_rate_multiplier\": 0.02,\n",
    "    \"suffix\": \"coversation_users_full_kaskada_a\"\n",
    "}\n",
    "# Create the fine-tune job and retrieve the job ID\n",
    "# and status from the response.\n",
    "resp = openai.FineTune.create(**create_args)\n",
    "job_id = resp[\"id\"]\n",
    "status = resp[\"status\"]\n",
    "\n",
    "# You can use the job ID to monitor the status of the fine-tune job.\n",
    "# The fine-tune job may take some time to start and complete.\n",
    "print(f'Fine-tuning model with job ID: \"{job_id}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = \"ft-qnHHxoNViVDplGDgIGn0kEC9\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for the fine-tuning to start\n",
    "\n",
    "* Note that it can take several hours for the job to move from the `pending` state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job not in terminal status: pending. Waiting.\n",
      "Status: pending\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mJob not in terminal status: \u001b[39m\u001b[39m{\u001b[39;00mstatus\u001b[39m}\u001b[39;00m\u001b[39m. Waiting.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39mwhile\u001b[39;00m status \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39msucceeded\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfailed\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m----> 8\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m5\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m     status \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mFineTune\u001b[39m.\u001b[39mretrieve(\u001b[39mid\u001b[39m\u001b[39m=\u001b[39mjob_id)[\u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mStatus: \u001b[39m\u001b[39m{\u001b[39;00mstatus\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get the status of our fine-tune job.\n",
    "status = openai.FineTune.retrieve(id=job_id)[\"status\"]\n",
    "\n",
    "# If the job isn't yet done, poll it every 2 seconds.\n",
    "if status not in [\"succeeded\", \"failed\"]:\n",
    "    print(f'Job not in terminal status: {status}. Waiting.')\n",
    "    while status not in [\"succeeded\", \"failed\"]:\n",
    "        time.sleep(5)\n",
    "        status = openai.FineTune.retrieve(id=job_id)[\"status\"]\n",
    "        print(f'Status: {status}')\n",
    "else:\n",
    "    print(f'Fine-tune job {job_id} finished with status: {status}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check fine-tuning events\n",
    "\n",
    "* Lets us know specifics about the fine-tuning job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"fine-tune-event\",\n",
      "  \"level\": \"info\",\n",
      "  \"message\": \"Created fine-tune: ft-qnHHxoNViVDplGDgIGn0kEC9\",\n",
      "  \"created_at\": 1691662984\n",
      "}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[133], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Get the events of our fine-tune job.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m events \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mFineTune\u001b[39m.\u001b[39mstream_events(\u001b[39mid\u001b[39m\u001b[39m=\u001b[39mjob_id)\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfor\u001b[39;00m event \u001b[39min\u001b[39;00m events:\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(event)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/openai/api_resources/fine_tune.py:158\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    153\u001b[0m response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39mrequest(\n\u001b[1;32m    154\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mget\u001b[39m\u001b[39m\"\u001b[39m, url, params, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, request_id\u001b[39m=\u001b[39mrequest_id\n\u001b[1;32m    155\u001b[0m )\n\u001b[1;32m    157\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)  \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    159\u001b[0m     util\u001b[39m.\u001b[39mconvert_to_openai_object(\n\u001b[1;32m    160\u001b[0m         line,\n\u001b[1;32m    161\u001b[0m         api_key,\n\u001b[1;32m    162\u001b[0m         api_version,\n\u001b[1;32m    163\u001b[0m         organization,\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m response\n\u001b[1;32m    166\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/openai/api_requestor.py:692\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns the response(s) and a bool indicating whether it is a stream.\"\"\"\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[39mif\u001b[39;00m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtext/event-stream\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m result\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mContent-Type\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    700\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39mcontent\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/openai/api_requestor.py:115\u001b[0m, in \u001b[0;36mparse_stream\u001b[0;34m(rbody)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_stream\u001b[39m(rbody: Iterator[\u001b[39mbytes\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m rbody:\n\u001b[1;32m    116\u001b[0m         _line \u001b[39m=\u001b[39m parse_stream_helper(line)\n\u001b[1;32m    117\u001b[0m         \u001b[39mif\u001b[39;00m _line \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/requests/models.py:865\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[39mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \n\u001b[1;32m    860\u001b[0m \u001b[39m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    863\u001b[0m pending \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter_content(\n\u001b[1;32m    866\u001b[0m     chunk_size\u001b[39m=\u001b[39mchunk_size, decode_unicode\u001b[39m=\u001b[39mdecode_unicode\n\u001b[1;32m    867\u001b[0m ):\n\u001b[1;32m    869\u001b[0m     \u001b[39mif\u001b[39;00m pending \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    870\u001b[0m         chunk \u001b[39m=\u001b[39m pending \u001b[39m+\u001b[39m chunk\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/urllib3/response.py:937\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    922\u001b[0m \u001b[39mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \u001b[39m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[39m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunked \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m--> 937\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_chunked(amt, decode_content\u001b[39m=\u001b[39mdecode_content)\n\u001b[1;32m    938\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    939\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/urllib3/response.py:1077\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1077\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_chunk_length()\n\u001b[1;32m   1078\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1079\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/urllib3/response.py:1005\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1004\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1005\u001b[0m line \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline()  \u001b[39m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m line \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39msplit(\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m;\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1007\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1238\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1240\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1241\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1242\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1100\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1101\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get the events of our fine-tune job.\n",
    "events = openai.FineTune.stream_events(id=job_id)\n",
    "\n",
    "for event in events:\n",
    "    print(event)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at Training Results\n",
    "\n",
    "* download the results file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputted results to: /Users/eric.pinzur/Documents/slackbot2000/coversation_users_0.csv\n"
     ]
    }
   ],
   "source": [
    "file_prefix = \"coversation_users\"\n",
    "\n",
    "result = openai.FineTune.retrieve(id=job_id)\n",
    "count = 0\n",
    "for result_file in result[\"result_files\"]:\n",
    "    file_name = f'{work_dir}/{file_prefix}_{count}.csv'\n",
    "    file = open(file_name, 'wb')\n",
    "    file.write(openai.File.download(id=result_file[\"id\"]))\n",
    "    file.close()\n",
    "    print(f'Outputted results to: {file_name}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"davinci:ft-personal:coversation-users-full-kaskada-2023-08-05-14-25-30\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_train = pandas.read_json(f'{work_dir}/conversation_user_examples_prepared_train.jsonl', lines=True)\n",
    "cu_valid = pandas.read_json(f'{work_dir}/conversation_user_examples_prepared_valid.jsonl', lines=True)\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# interesting i's: 6, 8, 78, 79, 80\n",
    "i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i += 1\n",
    "i = 6\n",
    "prompt = cu_train['prompt'][i]\n",
    "completion = cu_train['completion'][i]\n",
    "\n",
    "print(f'Prompt: {prompt}')\n",
    "print(f'Completion: {completion}')\n",
    "print(f'Prediction:')\n",
    "\n",
    "openai.Completion.create(model=model_id, prompt=prompt, max_tokens=1, stop=' end', n=1, logprobs=5, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: start ->  UCZ4VJF6J --> <@UU5C7MNMA> <@U017T5TFW58> Following up on the \"export\" use case -- worst case (if tempo doesn't have it) we could do something like having a little job (maybe skycat?) that finds bug reports with trace IDs and uses the Grafana query API to retrieve the trace (which returns it as OTEL JSON) and then attaches that to the bug. Then we'd have that trace *data* forever (and could always figure out how to visualize it separately):\n",
      "\n",
      "<https://grafana.com/docs/tempo/latest/api_docs/#query> \n",
      "\n",
      " U017T5TFW58 --> love the idea! \n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "Completion:  3 end\n",
      "Prediction:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7lHH5djssRnShCdPoYhxMwqR80ipK at 0x16cec7540> JSON: {\n",
       "  \"id\": \"cmpl-7lHH5djssRnShCdPoYhxMwqR80ipK\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1691502175,\n",
       "  \"model\": \"davinci:ft-personal:coversation-users-full-kaskada-2023-08-05-14-25-30\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \" 5\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": {\n",
       "        \"tokens\": [\n",
       "          \" 5\"\n",
       "        ],\n",
       "        \"token_logprobs\": [\n",
       "          -0.11093157\n",
       "        ],\n",
       "        \"top_logprobs\": [\n",
       "          {\n",
       "            \" 13\": -3.8975945,\n",
       "            \" 3\": -2.7242737,\n",
       "            \" 5\": -0.11093157,\n",
       "            \" 10\": -5.4023495\n",
       "          }\n",
       "        ],\n",
       "        \"text_offset\": [\n",
       "          553\n",
       "        ]\n",
       "      },\n",
       "      \"finish_reason\": \"length\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 164,\n",
       "    \"completion_tokens\": 1,\n",
       "    \"total_tokens\": 165\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#i += 1\n",
    "i = 220\n",
    "prompt = cu_valid['prompt'][i]\n",
    "completion = cu_valid['completion'][i]\n",
    "\n",
    "print(f'Prompt: {prompt}')\n",
    "print(f'Completion: {completion}')\n",
    "print(f'Prediction:')\n",
    "\n",
    "res = openai.Completion.create(model=model_id, prompt=prompt, max_tokens=1, stop=' end', n=1, logprobs=4, temperature=0)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  13, Prob: 2\n",
      "User:  3, Prob: 7\n",
      "User:  5, Prob: 89\n",
      "User:  10, Prob: 0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "logprobs = res[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0]\n",
    "for user in logprobs:\n",
    "    print(f'User: {user}, Prob: {round(math.exp(logprobs[user])*100)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UCZ4VJF6J': 1,\n",
       " 'ULJD5H2A2': 2,\n",
       " 'U017T5TFW58': 3,\n",
       " 'U014ZU49HPT': 4,\n",
       " 'UU5C7MNMA': 5,\n",
       " 'U016TM9NXEY': 6,\n",
       " 'U021KG8NMRQ': 7,\n",
       " 'UCXNQ2MPV': 8,\n",
       " 'U02TCUCA7PU': 9,\n",
       " 'U012CLUV1KJ': 10,\n",
       " 'U0332GKB9J8': 11,\n",
       " 'UT62H53R6': 12,\n",
       " 'U03RNK543HC': 13,\n",
       " 'USLACKBOT': 14,\n",
       " 'U011BDYMEG7': 15,\n",
       " 'U01TZK90VSM': 16,\n",
       " 'U02326Q5BG9': 17,\n",
       " 'U017S2GDXF0': 18,\n",
       " 'U0117AWAAEN': 19}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_map\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>prediction</th>\n",
       "      <th>test</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>start -&gt;  UCZ4VJF6J --&gt; &lt;https://venturebeat.c...</td>\n",
       "      <td>3 4 end</td>\n",
       "      <td>{'id': 'cmpl-7lcs8hQ8cAsW6sUrmkWV5eNfvMHfO', '...</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>[4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>start -&gt;  UCZ4VJF6J --&gt; &lt;https://medium.com/th...</td>\n",
       "      <td>nil end</td>\n",
       "      <td>{'id': 'cmpl-7lcs9Kl5ysRTUHy7myMDpGRc03xjT', '...</td>\n",
       "      <td>[nil]</td>\n",
       "      <td>[nil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>start -&gt;  UU5C7MNMA --&gt; &lt;https://blog.spiceai....</td>\n",
       "      <td>nil end</td>\n",
       "      <td>{'id': 'cmpl-7lcs9TgyNbYUQLqwbLkAfV7TmmjY4', '...</td>\n",
       "      <td>[nil]</td>\n",
       "      <td>[nil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>start -&gt;  U014ZU49HPT --&gt; This blog by Adobe g...</td>\n",
       "      <td>nil end</td>\n",
       "      <td>{'id': 'cmpl-7lcs9tK3fKxzTQRvZ7gvBD6lADmXe', '...</td>\n",
       "      <td>[nil]</td>\n",
       "      <td>[nil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>start -&gt;  ULJD5H2A2 --&gt; Ran across a query lan...</td>\n",
       "      <td>nil end</td>\n",
       "      <td>{'id': 'cmpl-7lcsAgSRXYIUYoj8H8soTGgLHPRll', '...</td>\n",
       "      <td>[nil]</td>\n",
       "      <td>[nil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>start -&gt;  UCZ4VJF6J --&gt; &lt;@UU5C7MNMA&gt; FYI I'm i...</td>\n",
       "      <td>5 end</td>\n",
       "      <td>{'id': 'cmpl-7lcsiVYiHV8TO0ihtRi2f329h9g42', '...</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[nil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>start -&gt;  UU5C7MNMA --&gt; &lt;@UCZ4VJF6J&gt; have you ...</td>\n",
       "      <td>1 end</td>\n",
       "      <td>{'id': 'cmpl-7lcsiiuzLAcE2w40upYhZE3jvxcVq', '...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>start -&gt;  UCZ4VJF6J --&gt; &lt;@ULJD5H2A2&gt; from read...</td>\n",
       "      <td>2 end</td>\n",
       "      <td>{'id': 'cmpl-7lcsjdzewSRjvKlmTQmXTEuz81woz', '...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>start -&gt;  UCZ4VJF6J --&gt; &lt;@U03RNK543HC&gt; &lt;@U012C...</td>\n",
       "      <td>nil end</td>\n",
       "      <td>{'id': 'cmpl-7lcsjiQSb6NRapu3c2jc9lPS2OepS', '...</td>\n",
       "      <td>[nil]</td>\n",
       "      <td>[nil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>start -&gt;  U012CLUV1KJ --&gt; &lt;@UCZ4VJF6J&gt; Tell me...</td>\n",
       "      <td>1 end</td>\n",
       "      <td>{'id': 'cmpl-7lcsj6gdOBvTq7bIjsfG5eXE32c4X', '...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>986 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prompt completion  \\\n",
       "0    start ->  UCZ4VJF6J --> <https://venturebeat.c...    3 4 end   \n",
       "1    start ->  UCZ4VJF6J --> <https://medium.com/th...    nil end   \n",
       "2    start ->  UU5C7MNMA --> <https://blog.spiceai....    nil end   \n",
       "3    start ->  U014ZU49HPT --> This blog by Adobe g...    nil end   \n",
       "4    start ->  ULJD5H2A2 --> Ran across a query lan...    nil end   \n",
       "..                                                 ...        ...   \n",
       "988  start ->  UCZ4VJF6J --> <@UU5C7MNMA> FYI I'm i...      5 end   \n",
       "989  start ->  UU5C7MNMA --> <@UCZ4VJF6J> have you ...      1 end   \n",
       "991  start ->  UCZ4VJF6J --> <@ULJD5H2A2> from read...      2 end   \n",
       "992  start ->  UCZ4VJF6J --> <@U03RNK543HC> <@U012C...    nil end   \n",
       "993  start ->  U012CLUV1KJ --> <@UCZ4VJF6J> Tell me...      1 end   \n",
       "\n",
       "                                            prediction    test   pred  \n",
       "0    {'id': 'cmpl-7lcs8hQ8cAsW6sUrmkWV5eNfvMHfO', '...  [3, 4]    [4]  \n",
       "1    {'id': 'cmpl-7lcs9Kl5ysRTUHy7myMDpGRc03xjT', '...   [nil]  [nil]  \n",
       "2    {'id': 'cmpl-7lcs9TgyNbYUQLqwbLkAfV7TmmjY4', '...   [nil]  [nil]  \n",
       "3    {'id': 'cmpl-7lcs9tK3fKxzTQRvZ7gvBD6lADmXe', '...   [nil]  [nil]  \n",
       "4    {'id': 'cmpl-7lcsAgSRXYIUYoj8H8soTGgLHPRll', '...   [nil]  [nil]  \n",
       "..                                                 ...     ...    ...  \n",
       "988  {'id': 'cmpl-7lcsiVYiHV8TO0ihtRi2f329h9g42', '...     [5]  [nil]  \n",
       "989  {'id': 'cmpl-7lcsiiuzLAcE2w40upYhZE3jvxcVq', '...     [1]    [1]  \n",
       "991  {'id': 'cmpl-7lcsjdzewSRjvKlmTQmXTEuz81woz', '...     [2]    [2]  \n",
       "992  {'id': 'cmpl-7lcsjiQSb6NRapu3c2jc9lPS2OepS', '...   [nil]  [nil]  \n",
       "993  {'id': 'cmpl-7lcsj6gdOBvTq7bIjsfG5eXE32c4X', '...     [1]    [1]  \n",
       "\n",
       "[986 rows x 5 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pandas.read_json(f'{work_dir}/conversation_user_examples_prepared_valid_with_pred.jsonl', lines=True)\n",
    "df[\"test\"] = None\n",
    "df[\"pred\"] = None\n",
    "\n",
    "for i in range(len(df)):\n",
    "    completions = df['completion'][i].removesuffix(\" end\").strip().split()\n",
    "    df.at[i, \"test\"] = completions\n",
    "    prediction = df['prediction'][i]\n",
    "    if \"choices\" in prediction:\n",
    "        predictions = prediction[\"choices\"][0][\"text\"].strip().split()\n",
    "        df.at[i, \"pred\"] = predictions\n",
    "df = df[df.pred.notnull()]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7411962064681381"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([['nil', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19']])\n",
    "y_test_transformed = mlb.transform(df['test'])\n",
    "y_pred_transformed = mlb.transform(df['pred'])\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_test_transformed, y_pred_transformed, average='weighted', zero_division=np.nan)  # Or 'micro', 'weighted' based on need\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['4', 'nil', '3', '6', '5', '1', '2', '10', '11'], dtype=object)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"pred\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19',\n",
       "       '2', '3', '4', '5', '6', '7', '8', '9', 'nil'], dtype=object)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m        set\n",
      "\u001b[0;31mString form:\u001b[0m {'thriller', 'sci-fi'}\n",
      "\u001b[0;31mLength:\u001b[0m      2\n",
      "\u001b[0;31mDocstring:\u001b[0m  \n",
      "set() -> new empty set object\n",
      "set(iterable) -> new set object\n",
      "\n",
      "Build an unordered collection of unique elements."
     ]
    }
   ],
   "source": [
    "x = {'sci-fi', 'thriller'}\n",
    "x?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "conversion_map = {}\n",
    "with open(f'{work_dir}/conversation_users_conversion_map.json') as file:\n",
    "    conversion_map = json.load(file)\n",
    "\n",
    "def convert_users_in_line(line):\n",
    "    for in_user in conversion_map:\n",
    "        out_user = conversion_map[in_user]\n",
    "\n",
    "        line = line.replace(in_user, out_user)\n",
    "    return line\n",
    "\n",
    "\n",
    "with open(f'{work_dir}/conversation_user_examples_prepared_train.jsonl', 'r') as in_file:\n",
    "    with open(f'{work_dir}/conversation_user_examples_prepared_train_converted.jsonl', 'w') as out_file:\n",
    "        while True:\n",
    "            line = in_file.readline()\n",
    "\n",
    "            if not line:\n",
    "                break\n",
    "\n",
    "            out_file.write(convert_users_in_line(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) new train set with prompts without userIds at the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = open(f'{work_dir}/message_conversations.jsonl', 'r')\n",
    "out_file = open(f'{work_dir}/conversation_user_noprompts_examples.jsonl', 'w')\n",
    "\n",
    "# gets a list of all users that reacted to a message\n",
    "def get_reaction_users(message):\n",
    "    users = []\n",
    "    if message[\"reactions\"]:\n",
    "        for reaction in message[\"reactions\"]:\n",
    "            users.extend(reaction[\"users\"])\n",
    "    return users\n",
    "\n",
    "prompt_separator = \"\\n\\n###\\n\\n\"\n",
    "completion_separator = \" end\"\n",
    "\n",
    "def output_conversation_example(conversation):\n",
    "    if len(conversation) > 0:\n",
    "        prompt_lines = []\n",
    "        initial_user = conversation[0][\"user\"]\n",
    "        users = []\n",
    "        for message in conversation:\n",
    "            user = message[\"user\"]\n",
    "            users.append(user)\n",
    "            users.extend(get_reaction_users(message))\n",
    "            \n",
    "            text = message[\"text\"]\n",
    "            if text == \"\" or text.find(\"```\") >= 0:\n",
    "                continue\n",
    "\n",
    "            prompt_lines.append(f' {text} ')\n",
    "\n",
    "        if len(prompt_lines) == 0:\n",
    "            return\n",
    "        \n",
    "        prompt = \"\\n\\n\".join(prompt_lines)\n",
    "\n",
    "        prompt = convert_users_in_line(prompt).strip()\n",
    "\n",
    "        # de-duplicate users in the list.\n",
    "        users = list(dict.fromkeys(users))\n",
    "\n",
    "        # remove the conversation starter\n",
    "        if initial_user in users:\n",
    "            users.remove(initial_user)\n",
    "\n",
    "        # convert user_ids to user_nums\n",
    "        user_nums = []\n",
    "        for user in users:\n",
    "            if user in user_map:\n",
    "                user_nums.append(f'{user_map[user]}')\n",
    "\n",
    "        completion = \" \".join(user_nums) if len(user_nums) > 0 else \"nil\"\n",
    "        \n",
    "        example = { \"prompt\": f'{prompt}{prompt_separator}', \"completion\": f' {completion}{completion_separator}' }\n",
    "        out_file.write(json.dumps(example) + \"\\n\")\n",
    "\n",
    "           \n",
    "current_channel = \"\"\n",
    "current_conversation = []\n",
    "conversation_ts = None\n",
    "\n",
    "while True:\n",
    "    output_convo = False\n",
    "    line = in_file.readline()\n",
    "\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "    message = json.loads(line)\n",
    "    channel = message[\"channel\"]\n",
    "    thread_ts = message[\"thread_ts\"]\n",
    "    user = message[\"user\"]\n",
    "\n",
    "    # output if channel changes\n",
    "    if channel != current_channel:\n",
    "        current_channel = channel\n",
    "        output_convo = True\n",
    "\n",
    "    # output if different thread_ts\n",
    "    if conversation_ts and conversation_ts != thread_ts:\n",
    "        output_convo = True\n",
    "\n",
    "    # output and reset\n",
    "    if output_convo:\n",
    "        output_conversation_example(current_conversation)\n",
    "        current_conversation = []\n",
    "\n",
    "    current_conversation.append(message)\n",
    "    conversation_ts = thread_ts\n",
    "   \n",
    "\n",
    "# output final conversation\n",
    "output_conversation_example(current_conversation)\n",
    "\n",
    "in_file.close()\n",
    "out_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) new train set with all userIds and weblinks removed from prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I faintly remember something about putting metrics into  \\n\\n Something I can help wit'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def strip_links_and_users(line):\n",
    "    return re.sub(r\"<.*?>\", '', line)\n",
    "\n",
    "strip_links_and_users(\"<@U052RFMBRF0> I faintly remember something about putting metrics into <https:\\/\\/gitlab.com\\/kaskada\\/kaskada\\/-\\/blob\\/main\\/wren\\/docker-compose.yml#L61> \\n\\n Something I can help wit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = open(f'{work_dir}/message_conversations.jsonl', 'r')\n",
    "out_file = open(f'{work_dir}/conversation_user_stripped_examples.jsonl', 'w')\n",
    "\n",
    "# gets a list of all users that reacted to a message\n",
    "def get_reaction_users(message):\n",
    "    users = []\n",
    "    if message[\"reactions\"]:\n",
    "        for reaction in message[\"reactions\"]:\n",
    "            users.extend(reaction[\"users\"])\n",
    "    return users\n",
    "\n",
    "prompt_separator = \"\\n\\n###\\n\\n\"\n",
    "completion_separator = \" end\"\n",
    "\n",
    "def output_conversation_example(conversation):\n",
    "    if len(conversation) > 0:\n",
    "        prompt_lines = []\n",
    "        initial_user = conversation[0][\"user\"]\n",
    "        users = []\n",
    "        for message in conversation:\n",
    "            user = message[\"user\"]\n",
    "            users.append(user)\n",
    "            users.extend(get_reaction_users(message))\n",
    "            \n",
    "            text = message[\"text\"]\n",
    "            if text == \"\" or text.find(\"```\") >= 0:\n",
    "                continue\n",
    "\n",
    "            prompt_lines.append(f' {text} ')\n",
    "\n",
    "        if len(prompt_lines) == 0:\n",
    "            return\n",
    "        \n",
    "        prompt = \"\\n\\n\".join(prompt_lines)\n",
    "\n",
    "        prompt = strip_links_and_users(prompt).strip()\n",
    "\n",
    "        if len(prompt) < 25:\n",
    "            return\n",
    "\n",
    "        # de-duplicate users in the list.\n",
    "        users = list(dict.fromkeys(users))\n",
    "\n",
    "        # remove the conversation starter\n",
    "        if initial_user in users:\n",
    "            users.remove(initial_user)\n",
    "\n",
    "        # convert user_ids to user_nums\n",
    "        user_nums = []\n",
    "        for user in users:\n",
    "            if user in user_map:\n",
    "                user_nums.append(f'{user_map[user]}')\n",
    "\n",
    "        completion = \" \".join(user_nums) if len(user_nums) > 0 else \"nil\"\n",
    "        \n",
    "        example = { \"prompt\": f'{prompt}{prompt_separator}', \"completion\": f' {completion}{completion_separator}' }\n",
    "        out_file.write(json.dumps(example) + \"\\n\")\n",
    "\n",
    "           \n",
    "current_channel = \"\"\n",
    "current_conversation = []\n",
    "conversation_ts = None\n",
    "\n",
    "while True:\n",
    "    output_convo = False\n",
    "    line = in_file.readline()\n",
    "\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "    message = json.loads(line)\n",
    "    channel = message[\"channel\"]\n",
    "    thread_ts = message[\"thread_ts\"]\n",
    "    user = message[\"user\"]\n",
    "\n",
    "    # output if channel changes\n",
    "    if channel != current_channel:\n",
    "        current_channel = channel\n",
    "        output_convo = True\n",
    "\n",
    "    # output if different thread_ts\n",
    "    if conversation_ts and conversation_ts != thread_ts:\n",
    "        output_convo = True\n",
    "\n",
    "    # output and reset\n",
    "    if output_convo:\n",
    "        output_conversation_example(current_conversation)\n",
    "        current_conversation = []\n",
    "\n",
    "    current_conversation.append(message)\n",
    "    conversation_ts = thread_ts\n",
    "   \n",
    "\n",
    "# output final conversation\n",
    "output_conversation_example(current_conversation)\n",
    "\n",
    "in_file.close()\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<@U052RFMBRF0> I faintly remember something about putting metrics into a docker container and running it with wren development. Does this ring a bell? \\n\\n yeah I think Eric hooked all that up: <https:\\\\/\\\\/gitlab.com\\\\/kaskada\\\\/kaskada\\\\/-\\\\/blob\\\\/main\\\\/wren\\\\/docker-compose.yml#L61>'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"<@U052RFMBRF0> I faintly remember something about putting metrics into a docker container and running it with wren development. Does this ring a bell? \\n\\n yeah I think Eric hooked all that up: <https://docker-compose.yml#L61>\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
