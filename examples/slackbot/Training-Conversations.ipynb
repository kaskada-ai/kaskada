{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversations\n",
    "\n",
    "### For a set of recent messages in a \"conversation\", try to predict the set of users that might interact next.\n",
    "\n",
    "A \"conversation\" is defined as either:\n",
    "* All the messages in a thread\n",
    "* A collection of messages from a single channel that occur in succession. If no response is made for 10 minutes, the conversation has ended. The next message outside this window is the start of a new conversation.\n",
    "\n",
    "\n",
    "For each set of messages in a \"conversation\", build a user set from the following properties:\n",
    "* users that reacted to the conversation\n",
    "* users that participated in the conversation\n",
    "* exclude the conversation starter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade openai pip install file-read-backwards  -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, openai, time, pandas, random, getpass\n",
    "from openai import cli\n",
    "from types import SimpleNamespace\n",
    "from sklearn.model_selection import train_test_split\n",
    "from file_read_backwards import FileReadBackwards\n",
    "\n",
    "work_dir = \"/Users/eric.pinzur/Documents/slackbot2000\"\n",
    "openai.api_key = getpass.getpass(prompt=\"Please enter your OpenAI API Key\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Data Prep\n",
    "\n",
    "Convert the original input data into a set of \"conversations\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into \"threads\" and \"non-threads\"\n",
    "\n",
    "Using DuckDB:\n",
    "\n",
    "```sql\n",
    "copy(\n",
    "    select * from \n",
    "    read_json_auto('messages.jsonl', format='newline_delimited') \n",
    "    where thread_ts is not null \n",
    "    order by channel, thread_ts, ts\n",
    ") to 'message_threads.jsonl' (FORMAT JSON);\n",
    "\n",
    "copy(\n",
    "    select * from \n",
    "    read_json_auto('messages.jsonl', format='newline_delimited') \n",
    "    where thread_ts is null \n",
    "    order by channel, ts\n",
    ") to 'message_non_threads.jsonl' (FORMAT JSON);\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make threads from non-threads\n",
    "\n",
    "For non-threads, artifically group the messages into \"threads\".  Collect messages from a channel.  If there is a 5 minute gap between messages, convert the message collection to a \"thread\" and export.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = open(f'{work_dir}/message_non_threads.jsonl', 'r')\n",
    "out_file = open(f'{work_dir}/message_non_threads_threads.jsonl', 'w')\n",
    "\n",
    "def write_thread(thread):\n",
    "    if len(thread) > 0:\n",
    "        reply_users = []\n",
    "        for message in thread:\n",
    "            if message[\"user\"] not in reply_users:\n",
    "                reply_users.append(message[\"user\"])\n",
    "        thread[0][\"reply_users\"] = reply_users\n",
    "        thread_ts = thread[0][\"ts\"]\n",
    "        for message in thread:\n",
    "            message[\"thread_ts\"] = thread_ts\n",
    "            out_file.write(json.dumps(message)+\"\\n\")\n",
    "\n",
    "next_thread = []\n",
    "current_channel = \"\"\n",
    "last_msg_ts = None\n",
    "while True:\n",
    "    output_thread = False\n",
    "    line = in_file.readline()\n",
    "\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "    message = json.loads(line)\n",
    "\n",
    "    channel = message[\"channel\"]\n",
    "\n",
    "    # output if channel changes in the file\n",
    "    if channel != current_channel:\n",
    "        current_channel = channel\n",
    "        output_thread = True\n",
    "\n",
    "    # output if message timestamp is more than 10 mins beyond last message\n",
    "    if last_msg_ts and message[\"ts\"] > last_msg_ts + 600:\n",
    "        output_thread = True\n",
    "\n",
    "    # output and reset\n",
    "    if output_thread:\n",
    "        write_thread(next_thread)\n",
    "        next_thread = []\n",
    "        last_msg_ts = None\n",
    "\n",
    "    next_thread.append(message)\n",
    "    last_msg_ts = message[\"ts\"]\n",
    "\n",
    "# output final thread\n",
    "write_thread(next_thread)\n",
    "\n",
    "in_file.close()\n",
    "out_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Conversations\n",
    "\n",
    "Re-join the two files into a set of conversations, using duckDB:\n",
    "\n",
    "```sql\n",
    "copy(\n",
    "    select * from \n",
    "    read_json_auto(['message_non_threads_threads.jsonl', 'message_threads.jsonl'], format='newline_delimited') \n",
    "    order by channel, thread_ts, ts\n",
    ") to 'message_conversations.jsonl' (FORMAT JSON);\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Examples\n",
    "\n",
    "This is a Generative problem, so the goals for training aren't as strict.\n",
    "\n",
    "Goals:\n",
    "* prompt and completion length must not be longer than 2048 tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy\n",
    "\n",
    "* Group messages int \"conversations\".\n",
    "* Use a method to write examples for a \"conversation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a user map\n",
    "in_file = open(f'{work_dir}/message_conversations.jsonl', 'r')\n",
    "user_map = {}\n",
    "user_count = 0\n",
    "\n",
    "while True:\n",
    "    line = in_file.readline()\n",
    "\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "    message = json.loads(line)\n",
    "    user = message[\"user\"]\n",
    "\n",
    "    # create a user_id -> user_num map\n",
    "    if user not in user_map:\n",
    "        user_count += 1\n",
    "        user_map[user] = user_count\n",
    "\n",
    "in_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = open(f'{work_dir}/message_conversations.jsonl', 'r')\n",
    "out_file = open(f'{work_dir}/conversation_user_examples.jsonl', 'w')\n",
    "\n",
    "# gets a list of all users that reacted to a message\n",
    "def get_reaction_users(message):\n",
    "    users = []\n",
    "    if message[\"reactions\"]:\n",
    "        for reaction in message[\"reactions\"]:\n",
    "            users.extend(reaction[\"users\"])\n",
    "    return users\n",
    "\n",
    "prompt_separator = \"\\n\\n###\\n\\n\"\n",
    "completion_separator = \" end\"\n",
    "\n",
    "def output_conversation_example(conversation):\n",
    "    if len(conversation) > 0:\n",
    "        prompt_lines = []\n",
    "        initial_user = conversation[0][\"user\"]\n",
    "        users = []\n",
    "        for message in conversation:\n",
    "            user = message[\"user\"]\n",
    "            users.append(user)\n",
    "            users.extend(get_reaction_users(message))\n",
    "            \n",
    "            text = message[\"text\"]\n",
    "            if text == \"\" or text.find(\"```\") >= 0:\n",
    "                continue\n",
    "\n",
    "            prompt_lines.append(f' {user} --> {text} ')\n",
    "\n",
    "        if len(prompt_lines) == 0:\n",
    "            return\n",
    "        \n",
    "        prompt = \"\\n\\n\".join(prompt_lines)\n",
    "\n",
    "        # de-duplicate users in the list.\n",
    "        users = list(dict.fromkeys(users))\n",
    "\n",
    "        # remove the conversation starter\n",
    "        if initial_user in users:\n",
    "            users.remove(initial_user)\n",
    "\n",
    "        # convert user_ids to user_nums\n",
    "        user_nums = []\n",
    "        for user in users:\n",
    "            if user in user_map:\n",
    "                user_nums.append(f'{user_map[user]}')\n",
    "\n",
    "        completion = \" \".join(user_nums) if len(user_nums) > 0 else \"nil\"\n",
    "        \n",
    "        example = { \"prompt\": f'start -> {prompt}{prompt_separator}', \"completion\": f' {completion}{completion_separator}' }\n",
    "        out_file.write(json.dumps(example) + \"\\n\")\n",
    "\n",
    "           \n",
    "current_channel = \"\"\n",
    "current_conversation = []\n",
    "conversation_ts = None\n",
    "\n",
    "while True:\n",
    "    output_convo = False\n",
    "    line = in_file.readline()\n",
    "\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "    message = json.loads(line)\n",
    "    channel = message[\"channel\"]\n",
    "    thread_ts = message[\"thread_ts\"]\n",
    "    user = message[\"user\"]\n",
    "\n",
    "\n",
    "\n",
    "    # output if channel changes\n",
    "    if channel != current_channel:\n",
    "        current_channel = channel\n",
    "        output_convo = True\n",
    "\n",
    "    # output if different thread_ts\n",
    "    if conversation_ts and conversation_ts != thread_ts:\n",
    "        output_convo = True\n",
    "\n",
    "    # output and reset\n",
    "    if output_convo:\n",
    "        output_conversation_example(current_conversation)\n",
    "        current_conversation = []\n",
    "\n",
    "    current_conversation.append(message)\n",
    "    conversation_ts = thread_ts\n",
    "   \n",
    "\n",
    "# output final conversation\n",
    "output_conversation_example(current_conversation)\n",
    "\n",
    "in_file.close()\n",
    "out_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Verification & Split Stage\n",
    "\n",
    "* make sure prompts end with same suffix\n",
    "* remove too long examples\n",
    "* remove duplicated examples\n",
    "\n",
    "Note, we aren't doing classification, so don't start a fine-tune as suggested by the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Your file contains 5295 prompt-completion pairs\n",
      "- Based on your data it seems like you're trying to fine-tune a model for classification\n",
      "- For classification, we recommend you try one of the faster and cheaper models, such as `ada`\n",
      "- For classification, you can estimate the expected model performance by keeping a held out dataset, which is not used for training\n",
      "- There are 37 duplicated prompt-completion sets. These are rows: [740, 772, 925, 932, 935, 936, 957, 1016, 1079, 1136, 1446, 1646, 1716, 2869, 2903, 3099, 3200, 3225, 3318, 3424, 3438, 3877, 3904, 4218, 4254, 4263, 4264, 4323, 4422, 4505, 4662, 4829, 4843, 4885, 4996, 5020, 5054]\n",
      "- There are 3 examples that are very long. These are rows: [146, 1906, 5035]\n",
      "For conditional generation, and for classification the examples shouldn't be longer than 2048 tokens.\n",
      "- All prompts end with suffix ` \\n\\n###\\n\\n`\n",
      "- All prompts start with prefix `start ->  U`\n",
      "\n",
      "Based on the analysis we will perform the following actions:\n",
      "- [Recommended] Remove 37 duplicate rows [Y/n]: Y\n",
      "- [Recommended] Remove 3 long examples [Y/n]: Y\n",
      "The indices of the long examples has changed as a result of a previously applied recommendation.\n",
      "The 3 long examples to be dropped are now at the following indices: [146, 1893, 4999]\n",
      "- [Recommended] Would you like to split into training and validation set? [Y/n]: Y\n",
      "\n",
      "\n",
      "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
      "\n",
      "Wrote modified files to `/Users/eric.pinzur/Documents/slackbot2000/conversation_user_examples_prepared_train.jsonl` and `/Users/eric.pinzur/Documents/slackbot2000/conversation_user_examples_prepared_valid.jsonl`\n",
      "Feel free to take a look!\n",
      "\n",
      "Now use that file when fine-tuning:\n",
      "> openai api fine_tunes.create -t \"/Users/eric.pinzur/Documents/slackbot2000/conversation_user_examples_prepared_train.jsonl\" -v \"/Users/eric.pinzur/Documents/slackbot2000/conversation_user_examples_prepared_valid.jsonl\" --compute_classification_metrics --classification_n_classes 333\n",
      "\n",
      "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string ` \\n\\n###\\n\\n` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\" end\"]` so that the generated texts ends at the expected place.\n",
      "Once your model starts training, it'll approximately take 2.14 hours to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
     ]
    }
   ],
   "source": [
    "args = SimpleNamespace(file=f'{work_dir}/conversation_user_examples.jsonl', quiet=True)\n",
    "cli.FineTune.prepare_data(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Stage\n",
    "\n",
    "* First need to upload the training & validation files to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 2.65M/2.65M [00:00<00:00, 375Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/eric.pinzur/Documents/slackbot2000/conversation_user_examples_prepared_train.jsonl: file-g9e5FgUEE9jW7U0wNsRRPOOc\n",
      "Status (training_file): uploaded \n",
      "Status (training_file): uploaded \n",
      "Status (training_file): uploaded \n",
      "Status (training_file): uploaded \n",
      "Status (training_file): processed \n"
     ]
    }
   ],
   "source": [
    "training_file_name = f'{work_dir}/conversation_user_examples_prepared_train.jsonl'\n",
    "\n",
    "def check_status(training_id):\n",
    "    train_status = openai.File.retrieve(training_id)[\"status\"]\n",
    "    print(f'Status (training_file): {train_status} ')\n",
    "    return (train_status)\n",
    "\n",
    "# Upload the training and validation dataset files to Azure OpenAI.\n",
    "training_id = cli.FineTune._get_or_upload(training_file_name, True)\n",
    "\n",
    "# Check on the upload status of the training dataset file.\n",
    "(train_status) = check_status(training_id)\n",
    "\n",
    "# Poll and display the upload status once a second until both files have either\n",
    "# succeeded or failed to upload.\n",
    "while train_status not in [\"succeeded\", \"failed\", \"processed\"]:\n",
    "    time.sleep(1)\n",
    "    (train_status) = check_status(training_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a fine-tuning Job\n",
    "\n",
    "* no validation file since this is not a classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning model with job ID: \"ft-2AZO7nL3LW9AlEbuonurYOlR\"\n"
     ]
    }
   ],
   "source": [
    "# This example defines a fine-tune job that creates a customized model based on curie, \n",
    "# with just a single pass through the training data. The job also provides classification-\n",
    "# specific metrics, using our validation data, at the end of that epoch.\n",
    "create_args = {\n",
    "    \"training_file\": training_id,\n",
    "    \"model\": \"davinci\",\n",
    "    \"n_epochs\": 2,\n",
    "    \"learning_rate_multiplier\": 0.02,\n",
    "    \"suffix\": \"coversation_users_full_kaskada\"\n",
    "}\n",
    "# Create the fine-tune job and retrieve the job ID\n",
    "# and status from the response.\n",
    "resp = openai.FineTune.create(**create_args)\n",
    "job_id = resp[\"id\"]\n",
    "status = resp[\"status\"]\n",
    "\n",
    "# You can use the job ID to monitor the status of the fine-tune job.\n",
    "# The fine-tune job may take some time to start and complete.\n",
    "print(f'Fine-tuning model with job ID: \"{job_id}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = \"ft-2AZO7nL3LW9AlEbuonurYOlR\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for the fine-tuning to start\n",
    "\n",
    "* Note that it can take several hours for the job to move from the `pending` state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the status of our fine-tune job.\n",
    "status = openai.FineTune.retrieve(id=job_id)[\"status\"]\n",
    "\n",
    "# If the job isn't yet done, poll it every 2 seconds.\n",
    "if status not in [\"succeeded\", \"failed\"]:\n",
    "    print(f'Job not in terminal status: {status}. Waiting.')\n",
    "    while status not in [\"succeeded\", \"failed\"]:\n",
    "        time.sleep(5)\n",
    "        status = openai.FineTune.retrieve(id=job_id)[\"status\"]\n",
    "        print(f'Status: {status}')\n",
    "else:\n",
    "    print(f'Fine-tune job {job_id} finished with status: {status}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check fine-tuning events\n",
    "\n",
    "* Lets us know specifics about the fine-tuning job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the events of our fine-tune job.\n",
    "events = openai.FineTune.stream_events(id=job_id)\n",
    "\n",
    "for event in events:\n",
    "    print(event)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at Training Results\n",
    "\n",
    "* download the results file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_prefix = \"coversation_users\"\n",
    "\n",
    "result = openai.FineTune.retrieve(id=job_id)\n",
    "count = 0\n",
    "for result_file in result[\"result_files\"]:\n",
    "    file_name = f'{work_dir}/{file_prefix}_{count}.csv'\n",
    "    file = open(file_name, 'wb')\n",
    "    file.write(openai.File.download(id=result_file[\"id\"]))\n",
    "    file.close()\n",
    "    print(f'Outputted results to: {file_name}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
