{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Message User\n",
    "\n",
    "### For a set of recent messages, try to predict the next user that will reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, openai, random, time\n",
    "from openai import cli\n",
    "from types import SimpleNamespace\n",
    "\n",
    "work_dir = \"\"\n",
    "\n",
    "# Remember to remove your key from your code when you're done.\n",
    "openai.api_key = \"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Output Stage\n",
    "\n",
    "Goals:\n",
    "* prompt and completion length must not be longer than 2048 tokens\n",
    "* each completion is a classification class and must be token unique, thus switching the completion to a number is ideal (1-100)\n",
    "* there SHOULD be 100 data points for each completion\n",
    "* both train and valid files need ALL completions present ideally at least 100 in each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New channel: articles\n",
      "New channel: conferences\n",
      "New channel: dev\n",
      "New channel: dev-ops\n",
      "New channel: fluff-posting\n",
      "New channel: games\n",
      "New channel: general\n",
      "New channel: inbound-leads\n",
      "New channel: on-call\n",
      "New channel: product\n",
      "New channel: random\n",
      "New channel: rust\n",
      "New channel: sales-team\n",
      "New channel: team-api\n",
      "New channel: team-compute\n",
      "Total user count: 17\n"
     ]
    }
   ],
   "source": [
    "file1 = open(f'{work_dir}/messages.jsonl', 'r')\n",
    "file2 = open(f'{work_dir}/next_message_user.jsonl', 'w')\n",
    "\n",
    "current_channel = \"\"\n",
    "previous_training_example = \"\"\n",
    "recent_messages = []\n",
    "count = 0\n",
    "user_map = {}\n",
    "user_count = 0\n",
    "user_counts = {}\n",
    "\n",
    "min_messages = 2\n",
    "max_messages = 5\n",
    "between_message_separator = \" \\n\\n\\n \"\n",
    "prompt_separator = \"\\n\\n###\\n\\n\"\n",
    "completion_separator = \"\"\n",
    "reverse_messages = True\n",
    "\n",
    "while True:\n",
    "    line = file1.readline()\n",
    "\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "    data = json.loads(line)\n",
    "    text = data[\"text\"].strip()\n",
    "\n",
    "    # skip message if empty\n",
    "    if text == \"\":\n",
    "        continue\n",
    "\n",
    "    # skip message if it contains a code block\n",
    "    if text.find(\"```\") >= 0:\n",
    "        continue\n",
    "\n",
    "    channel = data[\"channel\"]\n",
    "    user_id = data[\"user\"]\n",
    "    reactions = data[\"reactions\"]\n",
    "\n",
    "    # create a user_id -> user_num map\n",
    "    #\n",
    "    # also initialize a map to count training samples per user_id\n",
    "    # goal for at least 300 samples for each user_id so that \n",
    "    # so that we can ensure at least 100 samples for each user in both\n",
    "    # training and validation files\n",
    "    #\n",
    "    # note that we count by user_num and not user_id, to ease processing\n",
    "    # in the next stage\n",
    "    if user_id not in user_map:\n",
    "        user_count += 1\n",
    "        user_map[user_id] = user_count\n",
    "        user_counts[user_count] = 0\n",
    "\n",
    "    # restart message history count if the channel changes in the file\n",
    "    if channel != current_channel:\n",
    "        current_channel = channel\n",
    "        recent_messages = []\n",
    "        print(f'New channel: {current_channel}')\n",
    "\n",
    "    # if we are in the message history size window, prep to output an example\n",
    "    if len(recent_messages) > min_messages:\n",
    "        messages = recent_messages.copy()\n",
    "        if reverse_messages:\n",
    "            messages.reverse()\n",
    "        prompt = between_message_separator.join(messages)\n",
    "\n",
    "        user_num = user_map[user_id]\n",
    "\n",
    "        training_example = { \"prompt\": f'{prompt}{prompt_separator}', \"completion\": f' {user_num}{completion_separator}' }\n",
    "        training_example = json.dumps(training_example)\n",
    "\n",
    "        # if training example doesn't match the previous one, then output\n",
    "        if previous_training_example != training_example:\n",
    "            file2.write(training_example + \"\\n\")\n",
    "            # also update the example count for the user_id\n",
    "            user_counts[user_num] += 1\n",
    "\n",
    "        previous_training_example = training_example\n",
    "\n",
    "    # add the recent message to the history.\n",
    "    # remove the oldest message if the size window is exceeded\n",
    "    recent_messages.append(text)\n",
    "    if len(recent_messages) > max_messages:\n",
    "        recent_messages.pop(0)\n",
    "\n",
    "file1.close()\n",
    "file2.close()\n",
    "\n",
    "user_map_file = open(f'{work_dir}/user_map.json', 'w')\n",
    "json.dump(user_map, user_map_file)\n",
    "user_map_file.close()\n",
    "\n",
    "user_counts_file = open(f'{work_dir}/next_message_user_counts.json', 'w')\n",
    "json.dump(user_counts, user_counts_file)\n",
    "user_counts_file.close()\n",
    "\n",
    "print(f'Total user count: {user_count}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Stage\n",
    "\n",
    "* Load the files from the previous stage output\n",
    "* remove any examples where the total completion count for the user is less than 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing the following users from the example set:\n",
      "['7', '8', '9', '11', '12', '14', '15', '16', '17']\n",
      "\n",
      "This leaves the remaing users and counts: \n",
      "{'1': 6851, '2': 1488, '3': 786, '4': 3480, '5': 1138, '6': 2100, '10': 2566, '13': 406}\n",
      "Total classes for training are: 8\n"
     ]
    }
   ],
   "source": [
    "min_examples = 300\n",
    "\n",
    "user_counts_file = open(f'{work_dir}/next_message_user_counts.json', 'r')\n",
    "\n",
    "user_counts = json.load(user_counts_file)\n",
    "user_counts_file.close()\n",
    "\n",
    "user_nums_with_too_few_examples = []\n",
    "user_nums_with_enough_examples = {}\n",
    "\n",
    "for user_num in user_counts:\n",
    "    example_count = user_counts[user_num]\n",
    "    if example_count < min_examples:\n",
    "        user_nums_with_too_few_examples.append(user_num)\n",
    "    else:\n",
    "        user_nums_with_enough_examples[user_num] = example_count\n",
    "\n",
    "print(\"Removing the following users from the example set:\")\n",
    "print(user_nums_with_too_few_examples)\n",
    "\n",
    "print(\"\\nThis leaves the remaing users and counts: \")\n",
    "print(user_nums_with_enough_examples)\n",
    "\n",
    "print(f'Total classes for training are: {len(user_nums_with_enough_examples)}')\n",
    "\n",
    "in_file = open(f'{work_dir}/next_message_user.jsonl', 'r')\n",
    "out_file = open(f'{work_dir}/next_message_user_cleaned.jsonl', 'w')\n",
    "\n",
    "completions_to_remove = []\n",
    "for user_num in user_nums_with_too_few_examples:\n",
    "    completions_to_remove.append(f' {user_num}{completion_separator}')\n",
    "\n",
    "while True:\n",
    "    training_example_line = in_file.readline()\n",
    "\n",
    "    if not training_example_line:\n",
    "        break\n",
    "\n",
    "    training_example = json.loads(training_example_line)\n",
    "    completion = training_example[\"completion\"]\n",
    "    if completion not in completions_to_remove:\n",
    "        out_file.write(training_example_line)\n",
    "\n",
    "in_file.close()\n",
    "out_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split Stage\n",
    "\n",
    "* Split the remaining exampes into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Your file contains 18815 prompt-completion pairs\n",
      "- Based on your data it seems like you're trying to fine-tune a model for classification\n",
      "- For classification, we recommend you try one of the faster and cheaper models, such as `ada`\n",
      "- For classification, you can estimate the expected model performance by keeping a held out dataset, which is not used for training\n",
      "- All prompts end with suffix `\\n\\n###\\n\\n`\n",
      "\n",
      "No remediations found.\n",
      "- [Recommended] Would you like to split into training and validation set? [Y/n]: Y\n",
      "\n",
      "\n",
      "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
      "\n",
      "Wrote modified files to `/Users/eric.pinzur/Documents/slackbot2000/next_message_user_cleaned_prepared_train (1).jsonl` and `/Users/eric.pinzur/Documents/slackbot2000/next_message_user_cleaned_prepared_valid (1).jsonl`\n",
      "Feel free to take a look!\n",
      "\n",
      "Now use that file when fine-tuning:\n",
      "> openai api fine_tunes.create -t \"/Users/eric.pinzur/Documents/slackbot2000/next_message_user_cleaned_prepared_train (1).jsonl\" -v \"/Users/eric.pinzur/Documents/slackbot2000/next_message_user_cleaned_prepared_valid (1).jsonl\" --compute_classification_metrics --classification_n_classes 8\n",
      "\n",
      "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string `\\n\\n###\\n\\n` for the model to start generating completions, rather than continuing with the prompt.\n",
      "Once your model starts training, it'll approximately take 7.56 hours to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
     ]
    }
   ],
   "source": [
    "args = SimpleNamespace(file=f'{work_dir}/next_message_user_cleaned.jsonl', quiet=True)\n",
    "cli.FineTune.prepare_data(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Validation Stage\n",
    "* Open the `train` and `valid` files. \n",
    "* Check to make sure there is enough examples for each `user_num` in each file\n",
    "* Ideally a minimum of 100 examples per user, per file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training file has the following completion counts:\n",
      "{' 1': 6502, ' 5': 1082, ' 10': 2444, ' 3': 749, ' 6': 1979, ' 4': 3272, ' 2': 1407, ' 13': 380}\n",
      "\n",
      "The validation file has the following completion counts:\n",
      "{' 4': 208, ' 1': 349, ' 5': 56, ' 2': 81, ' 3': 37, ' 6': 121, ' 10': 122, ' 13': 26}\n"
     ]
    }
   ],
   "source": [
    "def get_completion_counts(file_path):\n",
    "    file = open(file_path)\n",
    "\n",
    "    completion_counts = {}\n",
    "\n",
    "    while True:\n",
    "        training_example_line = file.readline()\n",
    "\n",
    "        if not training_example_line:\n",
    "            break\n",
    "\n",
    "        training_example = json.loads(training_example_line)\n",
    "        completion = training_example[\"completion\"]\n",
    "\n",
    "        if completion not in completion_counts:\n",
    "            completion_counts[completion] = 1\n",
    "        else:\n",
    "            completion_counts[completion] += 1\n",
    "\n",
    "    file.close()\n",
    "    return completion_counts\n",
    "\n",
    "train_completion_counts = get_completion_counts(f'{work_dir}/next_message_user_cleaned_prepared_train.jsonl')\n",
    "valid_completion_counts = get_completion_counts(f'{work_dir}/next_message_user_cleaned_prepared_valid.jsonl')\n",
    "\n",
    "print(\"The training file has the following completion counts:\")\n",
    "print(train_completion_counts)\n",
    "print(\"\\nThe validation file has the following completion counts:\")\n",
    "print(valid_completion_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well, that ^^ is not ideal\n",
    "\n",
    "* Need to split the files myself and re-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_percent = 25\n",
    "random_seed = 41\n",
    "\n",
    "file_in = open(f'{work_dir}/next_message_user_cleaned.jsonl', 'r')\n",
    "file_train = open(f'{work_dir}/next_message_user_cleaned_train.jsonl', 'w')\n",
    "file_valid = open(f'{work_dir}/next_message_user_cleaned_valid.jsonl', 'w')\n",
    "\n",
    "random.seed(random_seed)\n",
    "while True:\n",
    "    line = file_in.readline()\n",
    "\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "    if random.randint(0, 99) < validate_percent:\n",
    "        file_valid.write(line)\n",
    "    else:\n",
    "        file_train.write(line)\n",
    "\n",
    "file_in.close()\n",
    "file_train.close()\n",
    "file_valid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training file has the following completion counts:\n",
      "{' 1': 5093, ' 2': 1097, ' 3': 593, ' 4': 2622, ' 5': 859, ' 6': 1549, ' 10': 1904, ' 13': 304}\n",
      "\n",
      "The validation file has the following completion counts:\n",
      "{' 2': 391, ' 4': 858, ' 1': 1758, ' 6': 551, ' 5': 279, ' 3': 193, ' 10': 662, ' 13': 102}\n"
     ]
    }
   ],
   "source": [
    "## Recheck Files\n",
    "\n",
    "train_completion_counts = get_completion_counts(f'{work_dir}/next_message_user_cleaned_train.jsonl')\n",
    "valid_completion_counts = get_completion_counts(f'{work_dir}/next_message_user_cleaned_valid.jsonl')\n",
    "\n",
    "print(\"The training file has the following completion counts:\")\n",
    "print(train_completion_counts)\n",
    "print(\"\\nThe validation file has the following completion counts:\")\n",
    "print(valid_completion_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fantastic, moving on to model training\n",
    "\n",
    "* First need to upload the training & validation files to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 10.5M/10.5M [00:00<00:00, 10.1Git/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/eric.pinzur/Documents/slackbot2000/next_message_user_cleaned_train.jsonl: file-W2I8PmXypH8bzCPKAwcoXObW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 3.52M/3.52M [00:00<00:00, 3.56Git/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/eric.pinzur/Documents/slackbot2000/next_message_user_cleaned_valid.jsonl: file-sSSVNpj0IUAr1XUplVSpGrSG\n",
      "Status (training_file | validation_file): uploaded | uploaded\n",
      "Status (training_file | validation_file): uploaded | uploaded\n",
      "Status (training_file | validation_file): uploaded | uploaded\n",
      "Status (training_file | validation_file): processed | uploaded\n",
      "Status (training_file | validation_file): processed | uploaded\n",
      "Status (training_file | validation_file): processed | processed\n"
     ]
    }
   ],
   "source": [
    "training_file_name = f'{work_dir}/next_message_user_cleaned_train.jsonl'\n",
    "validation_file_name = f'{work_dir}/next_message_user_cleaned_valid.jsonl'\n",
    "\n",
    "def check_status(training_id, validation_id):\n",
    "    train_status = openai.File.retrieve(training_id)[\"status\"]\n",
    "    valid_status = openai.File.retrieve(validation_id)[\"status\"]\n",
    "    print(f'Status (training_file | validation_file): {train_status} | {valid_status}')\n",
    "    return (train_status, valid_status)\n",
    "\n",
    "# Upload the training and validation dataset files to Azure OpenAI.\n",
    "training_id = cli.FineTune._get_or_upload(training_file_name, True)\n",
    "validation_id = cli.FineTune._get_or_upload(validation_file_name, True)\n",
    "\n",
    "# Check on the upload status of the training and validation dataset files.\n",
    "(train_status, valid_status) = check_status(training_id, validation_id)\n",
    "\n",
    "# Poll and display the upload status once a second until both files have either\n",
    "# succeeded or failed to upload.\n",
    "while train_status not in [\"succeeded\", \"failed\", \"processed\"] or valid_status not in [\"succeeded\", \"failed\", \"processed\"]:\n",
    "    time.sleep(1)\n",
    "    (train_status, valid_status) = check_status(training_id, validation_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a fine-tuning Job\n",
    "\n",
    "* `classification_n_classes` should match the total user count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning model with job ID: ft-kLSvHqK2gc0cF4PG61NjDYcL.\n"
     ]
    }
   ],
   "source": [
    "# This example defines a fine-tune job that creates a customized model based on curie, \n",
    "# with just a single pass through the training data. The job also provides classification-\n",
    "# specific metrics, using our validation data, at the end of that epoch.\n",
    "create_args = {\n",
    "    \"training_file\": training_id,\n",
    "    \"validation_file\": validation_id,\n",
    "    \"model\": \"ada\",\n",
    "    \"compute_classification_metrics\": True,\n",
    "    \"n_epochs\": 1,\n",
    "    \"classification_n_classes\": 8,\n",
    "    \"suffix\": \"next_message_user_full_kaskada\"\n",
    "}\n",
    "# Create the fine-tune job and retrieve the job ID\n",
    "# and status from the response.\n",
    "resp = openai.FineTune.create(**create_args)\n",
    "job_id = resp[\"id\"]\n",
    "status = resp[\"status\"]\n",
    "\n",
    "# You can use the job ID to monitor the status of the fine-tune job.\n",
    "# The fine-tune job may take some time to start and complete.\n",
    "print(f'Fine-tuning model with job ID: {job_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = \"ft-kLSvHqK2gc0cF4PG61NjDYcL\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for the fine-tuning to start\n",
    "\n",
    "* Note that it can take several hours for the job to move from the `pending` state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the status of our fine-tune job.\n",
    "status = openai.FineTune.retrieve(id=job_id)[\"status\"]\n",
    "\n",
    "# If the job isn't yet done, poll it every 2 seconds.\n",
    "if status not in [\"succeeded\", \"failed\"]:\n",
    "    print(f'Job not in terminal status: {status}. Waiting.')\n",
    "    while status not in [\"succeeded\", \"failed\"]:\n",
    "        time.sleep(5)\n",
    "        status = openai.FineTune.retrieve(id=job_id)[\"status\"]\n",
    "        print(f'Status: {status}')\n",
    "else:\n",
    "    print(f'Fine-tune job {job_id} finished with status: {status}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check fine-tuning events\n",
    "\n",
    "* Lets us know specifics about the fine-tuning job\n",
    "* Gives debug info on failure:\n",
    "  Example from a previous attempt using a 1% random sample of the initial dataset before any cleaning:\n",
    "  ```\n",
    "  {\n",
    "    \"object\": \"fine-tune-event\",\n",
    "    \"level\": \"info\",\n",
    "    \"message\": \"Created fine-tune: ft-TUEyMRXlB2es1JaeILlCRH7i\",\n",
    "    \"created_at\": 1690925364\n",
    "  }\n",
    "  {\n",
    "    \"object\": \"fine-tune-event\",\n",
    "    \"level\": \"error\",\n",
    "    \"message\": \"Fine-tune failed. Errors:\\nThe number of classes in file-GG6r3mCVJ6q4uEGsBWewYrq9 does not match the number of classes specified in the hyperparameters.\\nThe number of classes in file-Pyd5oYAo0CkztKjiEvsSdzWO does not match the number of classes specified in the hyperparameters.\",\n",
    "    \"created_at\": 1690934403\n",
    "  }\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the events of our fine-tune job.\n",
    "events = openai.FineTune.stream_events(id=job_id)\n",
    "\n",
    "for event in events:\n",
    "    print(event)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at Training Results\n",
    "\n",
    "* download the results file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputted results to: /Users/eric.pinzur/Documents/slackbot2000/next_message_user_results_0.csv\n"
     ]
    }
   ],
   "source": [
    "file_prefix = \"next_message_user_results\"\n",
    "\n",
    "result = openai.FineTune.retrieve(id=job_id)\n",
    "count = 0\n",
    "for result_file in result[\"result_files\"]:\n",
    "    file_name = f'{work_dir}/{file_prefix}_{count}.csv'\n",
    "    file = open(file_name, 'wb')\n",
    "    file.write(openai.File.download(id=result_file[\"id\"]))\n",
    "    file.close()\n",
    "    print(f'Outputted results to: {file_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
