{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversations\n",
    "\n",
    "### On a per-conversation basis, predict the next person to type, based on the previous messages that weren't theirs.  Excluded userIds and http links.\n",
    "\n",
    "A \"conversation\" is defined as either:\n",
    "* All the messages in a thread\n",
    "* A collection of messages from a single channel that occur in succession. If no response is made for 10 minutes, the conversation has ended. The next message outside this window is the start of a new conversation.\n",
    "\n",
    "For each set of messages in a \"conversation\":\n",
    "* determine the set of users that participated in the conversation\n",
    "* for each user, iterate through the messages:\n",
    "  * collect sets of messages that they didn't write\n",
    "  * export an example before they would have written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade openai pip install file-read-backwards  -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, openai, time, pandas, random, getpass\n",
    "from openai import cli\n",
    "from types import SimpleNamespace\n",
    "from sklearn.model_selection import train_test_split\n",
    "from file_read_backwards import FileReadBackwards\n",
    "\n",
    "work_dir = \"/Users/eric.pinzur/Documents/slackbot2000\"\n",
    "openai.api_key = getpass.getpass(prompt=\"Please enter your OpenAI API Key\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Data Prep\n",
    "\n",
    "Convert the original input data into a set of \"conversations\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into \"threads\" and \"non-threads\"\n",
    "\n",
    "Using DuckDB:\n",
    "\n",
    "```sql\n",
    "copy(\n",
    "    select * from \n",
    "    read_json_auto('messages.jsonl', format='newline_delimited') \n",
    "    where thread_ts is not null \n",
    "    order by channel, thread_ts, ts\n",
    ") to 'message_threads.jsonl' (FORMAT JSON);\n",
    "\n",
    "copy(\n",
    "    select * from \n",
    "    read_json_auto('messages.jsonl', format='newline_delimited') \n",
    "    where thread_ts is null \n",
    "    order by channel, ts\n",
    ") to 'message_non_threads.jsonl' (FORMAT JSON);\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make threads from non-threads\n",
    "\n",
    "For non-threads, artifically group the messages into \"threads\".  Collect messages from a channel.  If there is a 5 minute gap between messages, convert the message collection to a \"thread\" and export.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = open(f'{work_dir}/message_non_threads.jsonl', 'r')\n",
    "out_file = open(f'{work_dir}/message_non_threads_threads.jsonl', 'w')\n",
    "\n",
    "def write_thread(thread):\n",
    "    if len(thread) > 0:\n",
    "        reply_users = []\n",
    "        for message in thread:\n",
    "            if message[\"user\"] not in reply_users:\n",
    "                reply_users.append(message[\"user\"])\n",
    "        thread[0][\"reply_users\"] = reply_users\n",
    "        thread_ts = thread[0][\"ts\"]\n",
    "        for message in thread:\n",
    "            message[\"thread_ts\"] = thread_ts\n",
    "            out_file.write(json.dumps(message)+\"\\n\")\n",
    "\n",
    "next_thread = []\n",
    "current_channel = \"\"\n",
    "last_msg_ts = None\n",
    "while True:\n",
    "    output_thread = False\n",
    "    line = in_file.readline()\n",
    "\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "    message = json.loads(line)\n",
    "\n",
    "    channel = message[\"channel\"]\n",
    "\n",
    "    # output if channel changes in the file\n",
    "    if channel != current_channel:\n",
    "        current_channel = channel\n",
    "        output_thread = True\n",
    "\n",
    "    # output if message timestamp is more than 10 mins beyond last message\n",
    "    if last_msg_ts and message[\"ts\"] > last_msg_ts + 600:\n",
    "        output_thread = True\n",
    "\n",
    "    # output and reset\n",
    "    if output_thread:\n",
    "        write_thread(next_thread)\n",
    "        next_thread = []\n",
    "        last_msg_ts = None\n",
    "\n",
    "    next_thread.append(message)\n",
    "    last_msg_ts = message[\"ts\"]\n",
    "\n",
    "# output final thread\n",
    "write_thread(next_thread)\n",
    "\n",
    "in_file.close()\n",
    "out_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Conversations\n",
    "\n",
    "Re-join the two files into a set of conversations, using duckDB:\n",
    "\n",
    "```sql\n",
    "copy(\n",
    "    select * from \n",
    "    read_json_auto(['message_non_threads_threads.jsonl', 'message_threads.jsonl'], format='newline_delimited') \n",
    "    order by channel, thread_ts, ts\n",
    ") to 'message_conversations.jsonl' (FORMAT JSON);\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Examples\n",
    "\n",
    "This is a Generative problem, so the goals for training aren't as strict.\n",
    "\n",
    "Goals:\n",
    "* prompt and completion length must not be longer than 2048 tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy\n",
    "\n",
    "* Group messages int \"conversations\".\n",
    "* Use a method to write examples for a \"conversation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a user map\n",
    "in_file = open(f'{work_dir}/message_conversations.jsonl', 'r')\n",
    "user_map = {}\n",
    "user_count = 0\n",
    "\n",
    "while True:\n",
    "    line = in_file.readline()\n",
    "\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "    message = json.loads(line)\n",
    "    user = message[\"user\"]\n",
    "\n",
    "    # create a user_id -> user_num map\n",
    "    if user not in user_map:\n",
    "        user_count += 1\n",
    "        user_map[user] = user_count\n",
    "\n",
    "in_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UCZ4VJF6J': 1,\n",
       " 'ULJD5H2A2': 2,\n",
       " 'U017T5TFW58': 3,\n",
       " 'U014ZU49HPT': 4,\n",
       " 'UU5C7MNMA': 5,\n",
       " 'U016TM9NXEY': 6,\n",
       " 'U021KG8NMRQ': 7,\n",
       " 'UCXNQ2MPV': 8,\n",
       " 'U02TCUCA7PU': 9,\n",
       " 'U012CLUV1KJ': 10,\n",
       " 'U0332GKB9J8': 11,\n",
       " 'UT62H53R6': 12,\n",
       " 'U03RNK543HC': 13,\n",
       " 'USLACKBOT': 14,\n",
       " 'U011BDYMEG7': 15,\n",
       " 'U01TZK90VSM': 16,\n",
       " 'U02326Q5BG9': 17,\n",
       " 'U017S2GDXF0': 18,\n",
       " 'U0117AWAAEN': 19}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def strip_links_and_users(line):\n",
    "    return re.sub(r\"<.*?>\", '', line)\n",
    "\n",
    "def strip_emoji(line):\n",
    "    return re.sub(r\":.*?:\", '', line)\n",
    "\n",
    "work_dir = \"/Users/eric.pinzur/Documents/slackbot2000\"\n",
    "\n",
    "in_file = open(f'{work_dir}/message_conversations.jsonl', 'r')\n",
    "out_file = open(f'{work_dir}/conversation_next_message_examples.jsonl', 'w')\n",
    "\n",
    "prompt_separator = \"\\n\\n###\\n\\n\"\n",
    "completion_separator = \"\"\n",
    "\n",
    "max_lines = 5\n",
    "max_prompt_len = 5000\n",
    "\n",
    "user_map = {\n",
    " 'UCZ4VJF6J': 1,\n",
    " 'ULJD5H2A2': 2,\n",
    " 'U017T5TFW58': 3,\n",
    " 'U014ZU49HPT': 4,\n",
    " 'UU5C7MNMA': 5,\n",
    " 'U016TM9NXEY': 6,\n",
    " 'U021KG8NMRQ': 7,\n",
    " 'UCXNQ2MPV': 8,\n",
    " 'U02TCUCA7PU': 9,\n",
    " 'U012CLUV1KJ': 10,\n",
    " 'U0332GKB9J8': 11,\n",
    " 'UT62H53R6': 12,\n",
    " 'U03RNK543HC': 13,\n",
    " 'USLACKBOT': 14,\n",
    " 'U011BDYMEG7': 15,\n",
    " 'U01TZK90VSM': 16,\n",
    " 'U02326Q5BG9': 17,\n",
    " 'U017S2GDXF0': 18,\n",
    " 'U0117AWAAEN': 19\n",
    " }\n",
    "\n",
    "def output_conversation_examples(conversation):\n",
    "    # first clean the conversation\n",
    "    cleaned = []\n",
    "    for message in conversation:\n",
    "        text = message[\"text\"]\n",
    "        text = strip_links_and_users(text)\n",
    "        text = strip_emoji(text)\n",
    "        text = text.strip()\n",
    "        if text == \"\" or text.find(\"```\") >= 0:\n",
    "            continue\n",
    "        message[\"text\"] = text\n",
    "        cleaned.append(message)\n",
    "\n",
    "    if len(cleaned) == 0:\n",
    "        return\n",
    "\n",
    "    # first build up a set of users that particpated in the conversation\n",
    "    users = []\n",
    "    for message in cleaned:\n",
    "        user = message[\"user\"]\n",
    "        users.append(user)\n",
    "    # de-duplicate users\n",
    "    users = list(dict.fromkeys(users))\n",
    "\n",
    "    # iterate on users, messages\n",
    "    for user in users:\n",
    "        user_num = user_map[user]\n",
    "        lines = []\n",
    "        last_example = \"\"\n",
    "        for message in cleaned:\n",
    "            msg_text = message[\"text\"]\n",
    "            msg_user = message[\"user\"]\n",
    "            # if message isn't from the user, append it\n",
    "            if msg_user != user:\n",
    "                lines.append(msg_text)\n",
    "                if len(lines) > max_lines:\n",
    "                    lines.pop(0)\n",
    "            # else output message set, reversed, trimmed\n",
    "            elif len(lines) > 0:\n",
    "                reversed = lines.copy()\n",
    "                reversed.reverse()\n",
    "                prompt = \"\\n\\n\".join(reversed)\n",
    "                if len(prompt) > max_prompt_len:\n",
    "                    prompt = prompt[0:max_prompt_len]\n",
    "\n",
    "                example = { \"prompt\": f'{prompt}{prompt_separator}', \"completion\": f' {user_num}{completion_separator}' }\n",
    "                if example != last_example:\n",
    "                    out_file.write(json.dumps(example) + \"\\n\")\n",
    "                last_example = example\n",
    "\n",
    "           \n",
    "current_channel = \"\"\n",
    "current_conversation = []\n",
    "conversation_ts = None\n",
    "\n",
    "while True:\n",
    "    output_convo = False\n",
    "    line = in_file.readline()\n",
    "\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "    message = json.loads(line)\n",
    "    channel = message[\"channel\"]\n",
    "    thread_ts = message[\"thread_ts\"]\n",
    "    user = message[\"user\"]\n",
    "\n",
    "    # output if channel changes\n",
    "    if channel != current_channel:\n",
    "        current_channel = channel\n",
    "        output_convo = True\n",
    "\n",
    "    # output if different thread_ts\n",
    "    if conversation_ts and conversation_ts != thread_ts:\n",
    "        output_convo = True\n",
    "\n",
    "    # output and reset\n",
    "    if output_convo:\n",
    "        output_conversation_examples(current_conversation)\n",
    "        current_conversation = []\n",
    "\n",
    "    current_conversation.append(message)\n",
    "    conversation_ts = thread_ts\n",
    "   \n",
    "\n",
    "# output final conversation\n",
    "output_conversation_examples(current_conversation)\n",
    "\n",
    "in_file.close()\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Cleanup State\n",
    "\n",
    "Use Chat API with few-shot learning to help determine which examples might be helpful for training, and which are just noise\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First find some examples of helpful (good) and un-helpful (bad) messages.  I wrote the `human.py` script to help with this.\n",
    "\n",
    "Next use the code below to summarize the helpful (good) messages, so they use fewer tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt (411): just read about this as part of Pinterests intfrastructure. It basically ingests Kafka events and guarantees they end up in exactly one partitioned file on S3. Supports writing Parquet, etc.\n",
      "\n",
      "\n",
      "\n",
      "Can take JSON, Proto, Avro, Thrift etc. to columns as well.\n",
      "\n",
      "May not be necessary with the current less-streaming-first approach, but thought it may be useful to have on our radar if/when that becomes important again.\n",
      "Response (292): Pinterest's infrastructure supports ingesting Kafka events and ensuring they are stored in one partitioned file on S3. It can handle various formats like JSON, Proto, Avro, and Thrift. While it may not be immediately necessary for our current approach, it is worth considering for future use.\n",
      "Prompt (203): Good read for some background when we start looking at this\n",
      "\n",
      "This article has a good summary of the difficulties (and how pinterest solved them) of writing a pipeline on both streaming and historic data:\n",
      "Response (135): The article provides insights on the challenges and solutions of building a pipeline for both streaming and historic data at Pinterest.\n",
      "Prompt (172): The idea of using ebs instead of s3 for storage is interesting. It made me wonder if there is a future where we use ebs volumes for each table or something wacky like that.\n",
      "Response (205): The idea of using EBS instead of S3 for storage is intriguing. It raises the possibility of potentially using EBS volumes for individual tables or exploring unconventional storage approaches in the future.\n",
      "Prompt (167): Especially this:\n",
      "\n",
      "doh\n",
      "\n",
      "Seen in iceberg release (integration with this)\n",
      "\n",
      "\"Git-inspired data version control\" (Transactional catalog for data lakes).\n",
      "\n",
      "Seems interesting.\n",
      "Response (164): The iceberg release includes \"Git-inspired data version control,\" which is a transactional catalog for data lakes. It seems interesting and worth exploring further.\n",
      "Prompt (407): These limitations seems concerning:\n",
      "• Cassandra stores data in memtables and commitlogs before flushing to SSTables, so analytics performed via only _sstable-to-arrow_ *will potentially be stale / not real-time.*\n",
      "• The parser can only read tables with up to 64 columns.\n",
      "• The system is set up to scan entire SSTables (not read specific partitions). More work will be needed if we ever do predicate pushdown.\n",
      "Response (493): There are a few limitations to be aware of. First, analytics performed using only the sstable-to-arrow method may not be real-time due to data being stored in memtables and commitlogs before flushing to SSTables in Cassandra.\n",
      "\n",
      "Additionally, the parser can only read tables with up to 64 columns, and the system is currently configured to scan entire SSTables rather than reading specific partitions. This means that further work would be required if predicate pushdown is needed in the future.\n",
      "Prompt (537): I'll post this comment directly in the doc as well\n",
      "\n",
      "Ok, I've read through Abstract N1. At the end of the abstract it says that we need to shorten and tailor our workshop abstract with a current abstract after that. Is the ask for us to tailor the `Creating and operating ML models from event-based data using feature stores and feature engines` to be the workshop abstract fot the three workshops listed.  Or is the ask to edit the talk abstract generally and then edit the workshop abstract you provided for each of the three workshops?\n",
      "Response (218): The ask is to edit the existing workshop abstract (`Creating and operating ML models from event-based data using feature stores and feature engines`) to tailor it for each of the three workshops listed in the document.\n",
      "Prompt (189): Data Lead on American farming, predicting outcomes using weather and field sensor data. They have a huge analytics team but can still predict/explain only 50% of outcomes with their models.\n",
      "Response (205): The American farming industry has a large analytics team working on predicting outcomes using weather and field sensor data. However, their models can currently only predict or explain 50% of the outcomes.\n",
      "Prompt (266): 12 hours for 7 billion rows definitely seems high depending on what they're doing\n",
      "\n",
      "They said they're very frustrated at the amount of time they have to spend managing pipelines. They predict about a billion rows a day and process 7 days data each day. Job takes 12h.\n",
      "Response (224): Processing 7 billion rows in 12 hours does seem like a high processing time. The team is frustrated with the amount of time spent managing pipelines and predicts about a billion rows per day, processing 7 days of data daily.\n",
      "Prompt (381): This should be out of the thread, right?\n",
      "\n",
      "and that sounds like a 12h number\n",
      "\n",
      "(7 billion squared is 4.9e+19)\n",
      "\n",
      "And/or hitting the \"degenerate\" behavior we talk about when working with Spark / SQL and time (eg., \"for every time, consider run a sub-query over the rows up to that time\", leading to N^2)\n",
      "\n",
      "12 hours for 7 billion rows definitely seems high depending on what they're doing\n",
      "Response (214): Moving this discussion out of the thread is a good idea. The estimated time of 12 hours for processing 7 billion rows seems quite high, especially if it involves Spark/SQL and a potential N^2 time complexity issue.\n",
      "Prompt (290): Both of these are from Pushnami and they are pretty compatible with our system. They focus on time based event data and do advertising recommendation. Their main format is Parquet and is stored in S3. They hope to point Kaskada to their bucket and it works out well. Great candidates for us\n",
      "Response (326): Both Pushnami solutions focus on time-based event data and advertising recommendations, which aligns well with our system. They use Parquet as the main format and store the data in S3. It seems promising to point Kaskada to their S3 bucket and integrate with their solutions. These options are worth considering for our needs.\n",
      "Prompt (475): They said they're very frustrated at the amount of time they have to spend managing pipelines. They predict about a billion rows a day and process 7 days data each day. Job takes 12h.\n",
      "\n",
      "Both of these are from Pushnami and they are pretty compatible with our system. They focus on time based event data and do advertising recommendation. Their main format is Parquet and is stored in S3. They hope to point Kaskada to their bucket and it works out well. Great candidates for us\n",
      "Response (372): Pushnami seems to be a promising candidate for our system. They handle large volumes of time-based event data, process 7 days' worth of data in 12 hours, and predict about a billion rows per day. They primarily use Parquet format and store data in S3, making them compatible with our infrastructure. Integrating Kaskada with their data bucket could be a valuable solution.\n",
      "Prompt (510): They could be running on potato's\n",
      "\n",
      "They said they're very frustrated at the amount of time they have to spend managing pipelines. They predict about a billion rows a day and process 7 days data each day. Job takes 12h.\n",
      "\n",
      "Both of these are from Pushnami and they are pretty compatible with our system. They focus on time based event data and do advertising recommendation. Their main format is Parquet and is stored in S3. They hope to point Kaskada to their bucket and it works out well. Great candidates for us\n",
      "Response (332): Pushnami seems like a great fit for our system. They deal with time-based event data and focus on advertising recommendations. Their main format is Parquet, stored in S3. They process about a billion rows a day and can handle processing 7 days of data within 12 hours. We can potentially integrate Kaskada with their existing setup.\n",
      "Prompt (303): the RESTApi for Egret can be dropped, right? cc\n",
      "\n",
      "currently looking at disconnecting Egret from Marlin, just to disable the crashloop; the client threads are a bit entangled all over the place, trying to find the right  to\n",
      "\n",
      "just starting a thread for s as I do prep work for the  changes for tomorrow/wed\n",
      "Response (154): It seems like you are discussing disconnecting Egret from Marlin and making changes for tomorrow. You're also considering dropping the REST API for Egret.\n",
      "Prompt (121): What's your thinking here. Would we want to do something like \"point release every week + minor release every milestone\"?\n",
      "Response (419): Based on the context, it seems like you are considering a release strategy for the project. A possible approach could be to have weekly point releases to address minor changes and bug fixes, while having minor releases at key milestones to introduce more significant features or enhancements. This strategy allows for regular updates and continuous improvement while also providing larger updates at specific intervals.\n",
      "Prompt (282): Huh. That could be a bit painful in rust -- the version goes in the package, which means every merge has to increment all of the maintenance version numbers...\n",
      "\n",
      "What's your thinking here. Would we want to do something like \"point release every week + minor release every milestone\"?\n",
      "Response (276): In Rust, managing versions can be a bit tedious since the version number is tied to the package. One approach could be to do point releases on a weekly basis and minor releases for every milestone. This would provide a balance between regular updates and significant releases.\n",
      "Prompt (1045): Sounds good -- ideally we can let things evolve/move independently, and then just have some mechanism for making those breaking changes (auth, etc.) in a sensible way.\n",
      "\n",
      "What I've seen done is basically we have a page that says \"The Kaskada service supports versions X through Z of the Python Client. Versions X through Y are deprecated and will be unsupported on DATE.\"\n",
      "\n",
      "So then we could do something like:\n",
      "\n",
      "1. Let the Python client evolve as we see fit, as long as the API is evolving in compatible ways.\n",
      "2. When certain things change that we want to eventually support exclusively (like change to auth), we can bump the major version.\n",
      "3. Then we can just list which major versions of the client the service supports.\n",
      "\n",
      "Sure -- some of auth may show up as fields in the protos, but especially things on headers don't fit to that well.\n",
      "\n",
      "(Also happy to discuss more about how I've seen protobufs used)\n",
      "\n",
      "So I *think* I'm saying that we could rely on the protos as the contract and then we don't need to keep the versions themselves as synchronized.\n",
      "Response (412): We can allow independent evolution and handle breaking changes, such as auth, in a sensible way. One approach is to list the supported versions of the Python client and deprecate older versions. Protos can serve as the contract, while some auth changes may be reflected in the fields but not necessarily in the headers. We can rely on the protos as the contract and no longer need to synchronize version numbers.\n",
      "Prompt (1167): Eg., I believe you have to tell EKS when to upgrade, right?\n",
      "\n",
      "I generally like something around how many lagging versions are supported (seems like it could be really useful for Fenl at some point)... but for k8s it's somewhat easier because the user controls when the cluster upgrades.\n",
      "\n",
      "This is a big reason why Google (and others) use things like gRPC to provide forwards/backwards compatibility of the API. If you have a few different client libraries and thousands of different users it is basically impossible to ensure that everyone is using the same version in lockstep. It was maybe possible when the only client was the studio -- but even then you could have old versions in your browser.\n",
      "\n",
      "So instead, you use gRPC and focus on the API version rather than the various evolutions that happen in the backend.\n",
      "\n",
      "The problem with that is we can't control when our users upgrade, etc. So if they're using a client from a year ago and we've only extended the service, the client should still work.\n",
      "\n",
      "Sounds good -- ideally we can let things evolve/move independently, and then just have some mechanism for making those breaking changes (auth, etc.) in a sensible way.\n",
      "Response (379): You're right, in EKS the upgrade process needs to be controlled by the user. Using something like gRPC can help with forwards/backwards compatibility and managing different client versions. It's important to focus on the API version rather than backend evolutions. We should aim to let things evolve independently while handling breaking changes, such as auth, in a sensible way.\n",
      "Prompt (944): you've probably noticed that many of the resources in k8s haven't bumped their version often -- it's because you can generally *add* to the API without incrementing it.\n",
      "\n",
      "I think that's a big part of why it's nice to really think about things as separate components, separately versioned, with the API / protobuf as the primary contract (and then some \"social contract\" around things like auth / semantics / etc.).\n",
      "\n",
      "It seems like coordinating major (or minor) version numbers for changes to things not captured by the API makes sense.\n",
      "\n",
      "Once we get the API stable `v1.0` instead of `v1.0-alpha`, we can do surprisingly a lot without having to break the API itself.\n",
      "\n",
      "Eg., do we keep the last N versions of the service running in case people hit it, and then have a header that routes to the right version?\n",
      "\n",
      "It's harder for a hosted service, where the user doesn't opt in to the upgrade.\n",
      "\n",
      "Eg., I believe you have to tell EKS when to upgrade, right?\n",
      "Response (658): In Kubernetes, the versioning of resources is often not bumped frequently because new features can be added without incrementing the version. It's beneficial to think of components as separate entities with their own versioning, with the API/protobuf serving as the primary contract. Coordinating major or minor version numbers for changes not captured by the API makes sense. Once the API is stable, like transitioning from `v1.0-alpha` to `v1.0`, a lot can be done without breaking the API itself.\n",
      "\n",
      "For a hosted service, it may be more challenging as users don't opt-in to upgrades. For example, in EKS, the user typically has control over when to upgrade.\n",
      "Prompt (958): Yeah, I think many of the services in GCP were basically sitting in their v1alpha or v1 for years, because you can do so much with protobufs.\n",
      "\n",
      "interesting\n",
      "\n",
      "you've probably noticed that many of the resources in k8s haven't bumped their version often -- it's because you can generally *add* to the API without incrementing it.\n",
      "\n",
      "I think that's a big part of why it's nice to really think about things as separate components, separately versioned, with the API / protobuf as the primary contract (and then some \"social contract\" around things like auth / semantics / etc.).\n",
      "\n",
      "It seems like coordinating major (or minor) version numbers for changes to things not captured by the API makes sense.\n",
      "\n",
      "Once we get the API stable `v1.0` instead of `v1.0-alpha`, we can do surprisingly a lot without having to break the API itself.\n",
      "\n",
      "Eg., do we keep the last N versions of the service running in case people hit it, and then have a header that routes to the right version?\n",
      "Response (758): In GCP, services often remain in their initial versions (v1alpha or v1) for a long time because protobufs allow for extensive functionality. Similarly, in Kubernetes, the versioning of resources is infrequent due to the ability to add to the API without incrementing the version number. It is beneficial to treat components as separate entities with their own versioning, relying on the API/protobuf as the main contract. Coordinating version numbers for non-API related changes is sensible. Once the API stabilizes at v1.0, there is flexibility to make significant changes without breaking the API itself. Regarding backward compatibility, an approach could be to keep the last N versions running and utilize a header for routing to the appropriate version.\n",
      "Prompt (1027): I think the scrappy approach here is: some kind of semver but only implement where its required. For now thats only pypi (I think). In this case, it’d be nice if we could at least settle on the form of semver and how to treat it. If all projects at least spoke the same versioning language, we could converge the versioning some other time. Maybe we could reserve `major` for the one point that does tie across services (and make this a manual step). Each project can increment `minor` and `maintenance` whenever.\n",
      "\n",
      "From a deployment perspective, that just means parsing and populating a few more fields for argocd to pick up, so no biggie there.\n",
      "\n",
      "TDD is an option but I seriously doubt our commitment to it\n",
      "\n",
      "Definitely interested to hear if there might be more effective/flexible strategies to accomplish this.\n",
      "\n",
      "The situation I want to avoid is: having to roll back certain specific components and requiring a firedrill to understand intercompatibility.\n",
      "\n",
      "That said, maybe we only need to be strict where it matters (like pypi).\n",
      "Response (675): The suggestion is to implement a form of semantic versioning where it is required, particularly for PyPI. The idea is to reserve the \"major\" version for cross-service compatibility, while allowing each project to increment the \"minor\" and \"maintenance\" versions independently. This approach would involve parsing and populating fields for deployment, but can provide flexibility and avoid compatibility issues. Testing might be a consideration, although commitment to TDD is uncertain. The goal is to prevent a situation where specific components need to be rolled back, causing unnecessary complications. Strict versioning may only be necessary for critical areas like PyPI.\n",
      "Prompt (1090): (in which case … we could just start with serial versioning, single number, keep it simple …)\n",
      "\n",
      "I can dig the use of protos as common contracts between services. Are you saying that maybe by baking this into the python client that we don’t need to rely on the version for compat?\n",
      "\n",
      "I think the scrappy approach here is: some kind of semver but only implement where its required. For now thats only pypi (I think). In this case, it’d be nice if we could at least settle on the form of semver and how to treat it. If all projects at least spoke the same versioning language, we could converge the versioning some other time. Maybe we could reserve `major` for the one point that does tie across services (and make this a manual step). Each project can increment `minor` and `maintenance` whenever.\n",
      "\n",
      "From a deployment perspective, that just means parsing and populating a few more fields for argocd to pick up, so no biggie there.\n",
      "\n",
      "TDD is an option but I seriously doubt our commitment to it\n",
      "\n",
      "Definitely interested to hear if there might be more effective/flexible strategies to accomplish this.\n",
      "Response (650): Using protos as the common contract between services can eliminate the need to rely on the version for compatibility. Implementing a simplified serial versioning approach, such as using semver with only the necessary fields, like `major`, `minor`, and `maintenance`, could be a good approach. Reservation of `major` version as the point that ties across services could be a manual step. It would be helpful to have a consistent versioning language across projects. Test-driven development (TDD) is an option, but it may not be feasible in this case. Exploring more effective and flexible strategies for achieving versioning goals would be beneficial.\n",
      "Prompt (1284): Ok so at least to start with, I’ll just kick off `0.0.1` and increment somehow in CI just for the distro packages, and see where it goes.\n",
      "\n",
      "I do think there will be “things” outside the realm of the protos that the client will have to track (right now I can only think of authentication and authorization), but yeah, I think I see where you’re going with it.\n",
      "\n",
      "(in which case … we could just start with serial versioning, single number, keep it simple …)\n",
      "\n",
      "I can dig the use of protos as common contracts between services. Are you saying that maybe by baking this into the python client that we don’t need to rely on the version for compat?\n",
      "\n",
      "I think the scrappy approach here is: some kind of semver but only implement where its required. For now thats only pypi (I think). In this case, it’d be nice if we could at least settle on the form of semver and how to treat it. If all projects at least spoke the same versioning language, we could converge the versioning some other time. Maybe we could reserve `major` for the one point that does tie across services (and make this a manual step). Each project can increment `minor` and `maintenance` whenever.\n",
      "\n",
      "From a deployment perspective, that just means parsing and populating a few more fields for argocd to pick up, so no biggie there.\n",
      "Response (654): To start, you can kick off version `0.0.1` and increment it in CI for distribution packages. There may be non-proto related aspects, such as authentication and authorization, that the client needs to track. By using protos as common contracts, it may be possible to rely less on versioning for compatibility. It could be beneficial to settle on a form of semver and treat it consistently across projects. The `major` version could be reserved for cross-service ties. Incrementing `minor` and `maintenance` versions manually for each project can happen as needed. From a deployment perspective, this would involve parsing and populating fields for argocd.\n",
      "Prompt (836): Agreed. I like Kubernetes’ use of   for that, but they’re also committed to time-based releases. So thats a pretty different problem i guess.\n",
      "\n",
      "I gotta saywink:\n",
      "\n",
      "For example, if you say `version: 2` in a helm chart, that actually means `use helm 3` which is … less than ideal XD\n",
      "\n",
      "But yeah I think its worth punting on tying everything together for now, it gets real tricky real fast.\n",
      "\n",
      "Ok so at least to start with, I’ll just kick off `0.0.1` and increment somehow in CI just for the distro packages, and see where it goes.\n",
      "\n",
      "I do think there will be “things” outside the realm of the protos that the client will have to track (right now I can only think of authentication and authorization), but yeah, I think I see where you’re going with it.\n",
      "\n",
      "(in which case … we could just start with serial versioning, single number, keep it simple …)\n",
      "Response (467): Agreed. Kubernetes has a different approach with semantic versioning and time-based releases. Using Helm, the version number \"2\" actually means using Helm 3, which can be confusing. We can start with a simple versioning system, like a single number, for the distribution packages and increment in CI. There may be other aspects to track outside of the protos, such as authentication and authorization. Starting with a serial versioning approach could simplify things.\n",
      "Prompt (686): Yeah I hear that. Makes sense.\n",
      "\n",
      "In general, yes … though I think there’s a required upgrade path somewhere for eks (but thats not a k8s contract thing, that’s aws)\n",
      "\n",
      "Agreed. I like Kubernetes’ use of   for that, but they’re also committed to time-based releases. So thats a pretty different problem i guess.\n",
      "\n",
      "I gotta saywink:\n",
      "\n",
      "For example, if you say `version: 2` in a helm chart, that actually means `use helm 3` which is … less than ideal XD\n",
      "\n",
      "But yeah I think its worth punting on tying everything together for now, it gets real tricky real fast.\n",
      "\n",
      "Ok so at least to start with, I’ll just kick off `0.0.1` and increment somehow in CI just for the distro packages, and see where it goes.\n",
      "Response (176): It's a good idea to start with a versioning approach and increment it in CI for the distribution packages. This will allow you to track changes and evolve the system over time.\n",
      "Prompt (636): Once `stable` then yeah it can sit all day. But one cannot easily add functionality to stable without going through the process. Way to heavy for us, but .. to your point heh.\n",
      "\n",
      "Aaaaactually, dunno if you caught this but sometime in the last two years or so they implemented a new policy: you can add things at `v1beta1` but it’s got a lifecycle now. You _must_ bump version to demonstrate commitment to developing the feature over time, or it gets autodeprecated.\n",
      "\n",
      "ick XD\n",
      "\n",
      "Yeah I hear that. Makes sense.\n",
      "\n",
      "In general, yes … though I think there’s a required upgrade path somewhere for eks (but thats not a k8s contract thing, that’s aws)\n",
      "Response (437): Once a version is marked as stable, it can remain unchanged for extended periods. However, adding new functionality to a stable version requires going through a heavy process. Recently, a new policy was implemented where new features can be added at v1beta1, but they must be actively developed over time or they will be automatically deprecated. There may also be specific upgrade paths required by other platforms, such as AWS for EKS.\n",
      "Prompt (737): That makes sense — the problem with k8s though was people were becoming so dependent on non-stable that the issues that came from it eventually got to the core sigs who never wanted to deal with those problems in the first place lol\n",
      "\n",
      "Once `stable` then yeah it can sit all day. But one cannot easily add functionality to stable without going through the process. Way to heavy for us, but .. to your point heh.\n",
      "\n",
      "Aaaaactually, dunno if you caught this but sometime in the last two years or so they implemented a new policy: you can add things at `v1beta1` but it’s got a lifecycle now. You _must_ bump version to demonstrate commitment to developing the feature over time, or it gets autodeprecated.\n",
      "\n",
      "ick XD\n",
      "\n",
      "Yeah I hear that. Makes sense.\n",
      "Response (348): The Kubernetes situation highlighted the challenges of dependency on non-stable components. However, Kubernetes has implemented a new policy where new features can be added at v1beta1, but they must demonstrate ongoing development commitment or they will be autodeprecated. It's a way to strike a balance between stability and adding functionality.\n",
      "Prompt (638): is it ok to leave the “” as non-clickable link? I assumed we may want it as a `mailto` but forgot to ask; first-pass has it as just static text for styling/timing reasons\n",
      "\n",
      "READY, Merge away! I’ll get monitoring _after_ your change since my changes will also delete the marlin pods.\n",
      "\n",
      "will proceed with MR landing whenever you give the  ,\n",
      "\n",
      "I’ve just about got the monitoring changes ready to roll.\n",
      "\n",
      "I think I’m ready to land the changes in `main`, will start with UI then Egret shortly after; only `/login`, `/landing` , and `/register` will be available after these changes\n",
      "\n",
      "(and `/callback`, but that’s internal for the Auth0 login stuff)\n",
      "Response (266): The link can be left as non-clickable text for now. The merge can proceed, and monitoring will be set up after the change. The UI changes will be landed first, followed by the Egret changes. Only a limited set of paths will be available after these changes are made.\n",
      "Prompt (522): did you ever send the post-register copy updates? I may have missed this last week\n",
      "\n",
      "welcome to hotel kaskada\n",
      "\n",
      "one thing that came up during the `main` pass: once you login, you can’t logout ; not sure if this matters, but could be strange?\n",
      "\n",
      "is it ok to leave the “” as non-clickable link? I assumed we may want it as a `mailto` but forgot to ask; first-pass has it as just static text for styling/timing reasons\n",
      "\n",
      "READY, Merge away! I’ll get monitoring _after_ your change since my changes will also delete the marlin pods.\n",
      "Response (400): During the \"main\" pass, it was observed that users cannot logout once they have logged in. This may be a strange behavior and should be considered. Regarding the non-clickable link, it is currently displayed as empty quotes (\"\") for styling purposes but it was suggested to potentially make it a \"mailto\" link. Monitoring will be set up after the merge to account for the deletion of the marlin pods.\n",
      "Prompt (423): will proceed with MR landing whenever you give the  ,\n",
      "\n",
      "perfect thank you!\n",
      "\n",
      "I think I’m ready to land the changes in `main`, will start with UI then Egret shortly after; only `/login`, `/landing` , and `/register` will be available after these changes\n",
      "\n",
      "(and `/callback`, but that’s internal for the Auth0 login stuff)\n",
      "\n",
      "update on landing page: UI pieces are in place, just trying to sort out the redirects from previous pages\n",
      "Response (334): The changes will be landed in `main`, starting with the UI and then Egret shortly after. Only `/login`, `/landing`, and `/register` will be available after the changes, along with `/callback` for internal Auth0 login functionality. The landing page UI is in place, and the team is working on sorting out redirects from previous pages.\n",
      "Prompt (314): I know Jupyter uses codemirror, I’m not sure what readme uses\n",
      "\n",
      "I’ve been using the Rust syntax highlighter for code blocks - it’s not perfect but it does a decent job of differentiating functions from literals, and shows `let` as a different color.\n",
      "\n",
      "It would definitely be nice to have a custom highlighter though.\n",
      "Response (277): The README editor may or may not use codemirror, but Jupyter does use it. In the meantime, you have been using the Rust syntax highlighter for code blocks, which does a good job differentiating functions from literals. However, having a custom highlighter would be even better.\n"
     ]
    }
   ],
   "source": [
    "file = open(f'{work_dir}/conversation_next_message_examples_out_good.jsonl', 'r')\n",
    "out_file = open(f'{work_dir}/conversation_next_message_examples_out_good_summarized.jsonl', 'w')\n",
    "\n",
    "prompt_suffix = \"\\n\\n###\\n\\n\"\n",
    "\n",
    "count = 0\n",
    "while True:\n",
    "    line = file.readline()\n",
    "    count +=1\n",
    "\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "    data = json.loads(line)\n",
    "\n",
    "    prompt = data[\"prompt\"].removesuffix(prompt_suffix)\n",
    "\n",
    "    print(f'Prompt ({len(prompt)}): {prompt}')\n",
    "\n",
    "    msgs = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that provides consise summaries of messages.  The response must be significantly shorter than the input\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"just read about this as part of Pinterests intfrastructure. It basically ingests Kafka events and guarantees they end up in exactly one partitioned file on S3. Supports writing Parquet, etc.\\nCan take JSON, Proto, Avro, Thrift etc. to columns as well.\\nMay not be necessary with the current less-streaming-first approach, but thought it may be useful to have on our radar if/when that becomes important again.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Pinterest's infrastructure ingests Kafka events and ensures they are stored in one partitioned file on S3. It supports writing Parquet and can convert JSON, Proto, Avro, and Thrift to columns. It may not be immediately necessary for our current approach but could be valuable in the future.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Good read for some background when we start looking at this\\n\\nThis article has a good summary of the difficulties (and how pinterest solved them) of writing a pipeline on both streaming and historic data\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"The Pinterest article provides insights on the challenges and solutions in building a pipeline for both streaming and historic data.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Sounds good -- ideally we can let things evolve/move independently, and then just have some mechanism for making those breaking changes (auth, etc.) in a sensible way.\\n\\nWhat I've seen done is basically we have a page that says \\\"The Kaskada service supports versions X through Z of the Python Client. Versions X through Y are deprecated and will be unsupported on DATE.\\\"\\n\\nSo then we could do something like:\\n\\n1. Let the Python client evolve as we see fit, as long as the API is evolving in compatible ways.\\n2. When certain things change that we want to eventually support exclusively (like change to auth), we can bump the major version.\\n3. Then we can just list which major versions of the client the service supports.\\n\\nSure -- some of auth may show up as fields in the protos, but especially things on headers don't fit to that well.\\n\\n(Also happy to discuss more about how I've seen protobufs used)\\n\\nSo I *think* I'm saying that we could rely on the protos as the contract and then we don't need to keep the versions themselves as synchronized\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"ideally we can move independently, and handle breaking changes in a sensible way. we can have a map between features and supported versions. some changes that would create breaking changes are auth, protos, protobufs, api, python changes.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "    ]\n",
    "\n",
    "    res = openai.ChatCompletion.create(\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "        messages = msgs\n",
    "    )\n",
    "    response = res[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    print(f'Response ({len(response)}): {response}')\n",
    "\n",
    "    data[\"prompt\"] = f'{response}{prompt_suffix}'\n",
    "\n",
    "    out_file.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "\n",
    "file.close()\n",
    "out_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next use the summarized helpful messages (good) and un-helpful messsages (bad) with few-shot learning to figure out which training examples will actually be helfpul.  \n",
    "\n",
    "First we will make a messages list with our learning examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = open(f'{work_dir}/conversation_next_message_examples_out_good_summarized.jsonl', 'r')\n",
    "bad = open(f'{work_dir}/conversation_next_message_examples_out_bad.jsonl', 'r')\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a helpful assistant. Your job is to determine if a prompt will be helpful for fine-tuning a model. All prompts start with 'start -->' and end with: '\\\\n\\\\n###\\\\n\\\\n'. You should respond 'yes' if you think the prompt has enough context to be helpful, or 'no' if not. No explanation is needed. You should only respond with 'yes' or 'no'.\"\n",
    "}]\n",
    "\n",
    "count = 0\n",
    "while True:\n",
    "\n",
    "    good_line = good.readline()\n",
    "    bad_line = bad.readline()\n",
    "    count += 1\n",
    "\n",
    "    if not good_line or not bad_line:\n",
    "        break\n",
    "\n",
    "    good_data = json.loads(good_line)\n",
    "    bad_data = json.loads(bad_line)\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f'start -->{good_data[\"prompt\"]}'\n",
    "    })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"yes\"\n",
    "    })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f'start -->{bad_data[\"prompt\"]}'\n",
    "    })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"no\"\n",
    "    })\n",
    "\n",
    "good.close()\n",
    "bad.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use those example messages to predict if a prompt will be helpful for training or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, openai, time, logging, backoff\n",
    "\n",
    "logging.getLogger('backoff').addHandler(logging.StreamHandler())\n",
    "\n",
    "file = open(f'{work_dir}/conversation_next_message_examples.jsonl', 'r')\n",
    "out_file = open(f'{work_dir}/conversation_next_message_examples_cleaned.jsonl', 'a')\n",
    "\n",
    "@backoff.on_exception(backoff.expo, (openai.error.RateLimitError, openai.error.ServiceUnavailableError))\n",
    "def chat_with_backoff(**kwargs):\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        return openai.ChatCompletion.create(**kwargs)\n",
    "    except openai.error.InvalidRequestError:\n",
    "        return None\n",
    "\n",
    "count = 0\n",
    "while True:\n",
    "    line = file.readline()\n",
    "    count +=1\n",
    "\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "    # helpful for restarting after issue\n",
    "    if count < 8243:\n",
    "        continue\n",
    "\n",
    "    data = json.loads(line)\n",
    "\n",
    "    # skip users that have few examples\n",
    "    if data[\"completion\"] not in [\" 1\", \" 2\", \" 5\", \" 10\"]:\n",
    "        continue\n",
    "\n",
    "    prompt = data[\"prompt\"]\n",
    "\n",
    "    if len(prompt) < 100:\n",
    "        continue\n",
    "\n",
    "    msgs = messages.copy()\n",
    "\n",
    "    msgs.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f'start -->{prompt}'\n",
    "    })\n",
    "\n",
    "    res = chat_with_backoff(\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "        messages = msgs\n",
    "    )\n",
    "    if not res:\n",
    "        continue\n",
    "    response = res[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    print(f'Result was `{response}` for prompt: {prompt}')\n",
    "\n",
    "    print(f'Currently processing line: {count}')\n",
    "\n",
    "    if response == \"yes\":\n",
    "        out_file.write(line)\n",
    "        out_file.flush()\n",
    "\n",
    "file.close()\n",
    "out_file.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next grab the examples from outside the cleaned set to be the `nil` examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_prompts = []\n",
    "\n",
    "with open(f'{work_dir}/conversation_next_message_examples_cleaned_cleaned.jsonl', 'r') as file:\n",
    "    while True:\n",
    "\n",
    "        line = file.readline()\n",
    "\n",
    "        if not line:\n",
    "            break\n",
    "\n",
    "        data = json.loads(line)\n",
    "        cleaned_prompts.append(data[\"prompt\"])\n",
    "\n",
    "\n",
    "with open(f'{work_dir}/conversation_next_message_examples.jsonl', 'r') as file:\n",
    "    with open(f'{work_dir}/conversation_next_message_examples_nils.jsonl', 'w') as out_file:\n",
    "        while True:\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "\n",
    "            prompt = data[\"prompt\"]\n",
    "            if prompt not in cleaned_prompts:\n",
    "                data[\"completion\"] = \" nil\"\n",
    "                out_file.write(json.dumps(data) + '\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then get a random subset of nil examples that matches the length of cleaned_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "df = pandas.read_json(f'{work_dir}/conversation_next_message_examples_nils.jsonl', lines=True, orient='records')\n",
    "df = df.sample(len(cleaned_prompts))\n",
    "df.to_json(f'{work_dir}/conversation_next_message_examples_nils_sample.jsonl', lines=True, orient='records')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then combine the nils and the cleaned samples sets, and randomize the order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "nils = pandas.read_json(f'{work_dir}/conversation_next_message_examples_nils_sample.jsonl', lines=True, orient='records')\n",
    "cleaned = pandas.read_json(f'{work_dir}/conversation_next_message_examples_cleaned_cleaned.jsonl', lines=True, orient='records', dtype=False)\n",
    "\n",
    "df = pandas.concat([nils, cleaned])\n",
    "df = df.sample(frac=1)\n",
    "df.to_json(f'{work_dir}/conversation_next_message_examples_joined.jsonl', lines=True, orient='records')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Verification & Split Stage\n",
    "\n",
    "* make sure prompts end with same suffix\n",
    "* remove too long examples\n",
    "* remove duplicated examples\n",
    "\n",
    "Note, we aren't doing classification, so don't start a fine-tune as suggested by the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Your file contains 2410 prompt-completion pairs\n",
      "- Based on your data it seems like you're trying to fine-tune a model for classification\n",
      "- For classification, we recommend you try one of the faster and cheaper models, such as `ada`\n",
      "- For classification, you can estimate the expected model performance by keeping a held out dataset, which is not used for training\n",
      "- There are 7 duplicated prompt-completion sets. These are rows: [1190, 1376, 1433, 1455, 1657, 1775, 1879]\n",
      "- All prompts end with suffix `\\n\\n###\\n\\n`\n",
      "\n",
      "Based on the analysis we will perform the following actions:\n",
      "- [Recommended] Remove 7 duplicate rows [Y/n]: Y\n",
      "- [Recommended] Would you like to split into training and validation set? [Y/n]: Y\n",
      "\n",
      "\n",
      "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
      "\n",
      "Wrote modified files to `/Users/eric.pinzur/Documents/slackbot2000/conversation_next_message_examples_joined_prepared_train.jsonl` and `/Users/eric.pinzur/Documents/slackbot2000/conversation_next_message_examples_joined_prepared_valid.jsonl`\n",
      "Feel free to take a look!\n",
      "\n",
      "Now use that file when fine-tuning:\n",
      "> openai api fine_tunes.create -t \"/Users/eric.pinzur/Documents/slackbot2000/conversation_next_message_examples_joined_prepared_train.jsonl\" -v \"/Users/eric.pinzur/Documents/slackbot2000/conversation_next_message_examples_joined_prepared_valid.jsonl\" --compute_classification_metrics --classification_n_classes 5\n",
      "\n",
      "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string `\\n\\n###\\n\\n` for the model to start generating completions, rather than continuing with the prompt.\n",
      "Once your model starts training, it'll approximately take 1.0 hours to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
     ]
    }
   ],
   "source": [
    "args = SimpleNamespace(file=f'{work_dir}/conversation_next_message_examples_joined.jsonl', quiet=True)\n",
    "cli.FineTune.prepare_data(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Stage\n",
    "\n",
    "* First need to upload the training & validation files to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 855k/855k [00:00<00:00, 1.42Git/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/eric.pinzur/Documents/slackbot2000/conversation_next_message_examples_joined_prepared_train.jsonl: file-BJQcy1YwkbWVBSqiwzac8M47\n",
      "Status (training_file): uploaded \n",
      "Status (training_file): processed \n"
     ]
    }
   ],
   "source": [
    "training_file_name = f'{work_dir}/conversation_next_message_examples_joined_prepared_train.jsonl'\n",
    "\n",
    "def check_status(training_id):\n",
    "    train_status = openai.File.retrieve(training_id)[\"status\"]\n",
    "    print(f'Status (training_file): {train_status} ')\n",
    "    return (train_status)\n",
    "\n",
    "# Upload the training and validation dataset files to Azure OpenAI.\n",
    "training_id = cli.FineTune._get_or_upload(training_file_name, True)\n",
    "\n",
    "# Check on the upload status of the training dataset file.\n",
    "(train_status) = check_status(training_id)\n",
    "\n",
    "# Poll and display the upload status once a second until both files have either\n",
    "# succeeded or failed to upload.\n",
    "while train_status not in [\"succeeded\", \"failed\", \"processed\"]:\n",
    "    time.sleep(1)\n",
    "    (train_status) = check_status(training_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a fine-tuning Job\n",
    "\n",
    "* no validation file since this is not a classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning model with job ID: \"ft-uiNxwLFc8s8e23lNY4COOF70\"\n"
     ]
    }
   ],
   "source": [
    "# This example defines a fine-tune job that creates a customized model based on curie, \n",
    "# with just a single pass through the training data. The job also provides classification-\n",
    "# specific metrics, using our validation data, at the end of that epoch.\n",
    "create_args = {\n",
    "    \"training_file\": training_id,\n",
    "    \"model\": \"davinci\",\n",
    "    \"n_epochs\": 4,\n",
    "    \"learning_rate_multiplier\": 0.02,\n",
    "    \"suffix\": \"coversation_next_message_dav_4\"\n",
    "}\n",
    "# Create the fine-tune job and retrieve the job ID\n",
    "# and status from the response.\n",
    "resp = openai.FineTune.create(**create_args)\n",
    "job_id = resp[\"id\"]\n",
    "status = resp[\"status\"]\n",
    "\n",
    "# You can use the job ID to monitor the status of the fine-tune job.\n",
    "# The fine-tune job may take some time to start and complete.\n",
    "print(f'Fine-tuning model with job ID: \"{job_id}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "dav_job_id = \"ft-eazX8z01fyfVgIGlqZq1tu8W\" #eric-datastax api key\n",
    "dav4_job_id = \"ft-uiNxwLFc8s8e23lNY4COOF70\" #eric-datastax api key\n",
    "ada_job_id = \"ft-gd4YPNhCsiat1VyqT12UlT9p\" #ryan's api key\n",
    "cur_job_id = \"ft-sKxsiaHteiR1AK6QhYITIXME\" #ryan's api key\n",
    "cur4_job_id = \"ft-e3cyDnMLxbp9O7Mr1egqnouX\" #eric-datastax api key\n",
    "cur8_job_id = \"ft-oXnZLB4PSKcJuPMC9GzWSEQx\" #eric-datastax api key\n",
    "\n",
    "job_id = dav_job_id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for the fine-tuning to start\n",
    "\n",
    "* Note that it can take several hours for the job to move from the `pending` state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job not in terminal status: running. Waiting.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mJob not in terminal status: \u001b[39m\u001b[39m{\u001b[39;00mstatus\u001b[39m}\u001b[39;00m\u001b[39m. Waiting.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39mwhile\u001b[39;00m status \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39msucceeded\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfailed\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m----> 8\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m5\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m     status \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mFineTune\u001b[39m.\u001b[39mretrieve(\u001b[39mid\u001b[39m\u001b[39m=\u001b[39mjob_id)[\u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mStatus: \u001b[39m\u001b[39m{\u001b[39;00mstatus\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get the status of our fine-tune job.\n",
    "status = openai.FineTune.retrieve(id=job_id)[\"status\"]\n",
    "\n",
    "# If the job isn't yet done, poll it every 2 seconds.\n",
    "if status not in [\"succeeded\", \"failed\"]:\n",
    "    print(f'Job not in terminal status: {status}. Waiting.')\n",
    "    while status not in [\"succeeded\", \"failed\"]:\n",
    "        time.sleep(5)\n",
    "        status = openai.FineTune.retrieve(id=job_id)[\"status\"]\n",
    "        print(f'Status: {status}')\n",
    "else:\n",
    "    print(f'Fine-tune job {job_id} finished with status: {status}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTune fine-tune id=ft-uiNxwLFc8s8e23lNY4COOF70 at 0x291ba1f90> JSON: {\n",
       "  \"object\": \"fine-tune\",\n",
       "  \"id\": \"ft-uiNxwLFc8s8e23lNY4COOF70\",\n",
       "  \"hyperparams\": {\n",
       "    \"n_epochs\": 4,\n",
       "    \"batch_size\": 2,\n",
       "    \"prompt_loss_weight\": 0.01,\n",
       "    \"learning_rate_multiplier\": 0.02\n",
       "  },\n",
       "  \"organization_id\": \"org-qHJsHkK4p0Jd51STTeKAm5fV\",\n",
       "  \"model\": \"davinci\",\n",
       "  \"training_files\": [\n",
       "    {\n",
       "      \"object\": \"file\",\n",
       "      \"id\": \"file-BJQcy1YwkbWVBSqiwzac8M47\",\n",
       "      \"purpose\": \"fine-tune\",\n",
       "      \"filename\": \"/Users/eric.pinzur/Documents/slackbot2000/conversation_next_message_examples_joined_prepared_train.jsonl\",\n",
       "      \"bytes\": 854884,\n",
       "      \"created_at\": 1692115490,\n",
       "      \"status\": \"processed\",\n",
       "      \"status_details\": null\n",
       "    }\n",
       "  ],\n",
       "  \"validation_files\": [],\n",
       "  \"result_files\": [\n",
       "    {\n",
       "      \"object\": \"file\",\n",
       "      \"id\": \"file-vOTRsJxhVhzZdVVxP7qwcNQL\",\n",
       "      \"purpose\": \"fine-tune-results\",\n",
       "      \"filename\": \"compiled_results.csv\",\n",
       "      \"bytes\": 173478,\n",
       "      \"created_at\": 1692122709,\n",
       "      \"status\": \"processed\",\n",
       "      \"status_details\": null\n",
       "    }\n",
       "  ],\n",
       "  \"created_at\": 1692116224,\n",
       "  \"updated_at\": 1692122710,\n",
       "  \"status\": \"succeeded\",\n",
       "  \"fine_tuned_model\": \"davinci:ft-datastax:coversation-next-message-dav-4-2023-08-15-18-05-07\",\n",
       "  \"events\": [\n",
       "    {\n",
       "      \"object\": \"fine-tune-event\",\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Created fine-tune: ft-uiNxwLFc8s8e23lNY4COOF70\",\n",
       "      \"created_at\": 1692116224\n",
       "    },\n",
       "    {\n",
       "      \"object\": \"fine-tune-event\",\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Fine-tune costs $23.66\",\n",
       "      \"created_at\": 1692117649\n",
       "    },\n",
       "    {\n",
       "      \"object\": \"fine-tune-event\",\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Fine-tune enqueued. Queue number: 5\",\n",
       "      \"created_at\": 1692117649\n",
       "    },\n",
       "    {\n",
       "      \"object\": \"fine-tune-event\",\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Fine-tune is in the queue. Queue number: 4\",\n",
       "      \"created_at\": 1692117816\n",
       "    },\n",
       "    {\n",
       "      \"object\": \"fine-tune-event\",\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Fine-tune is in the queue. Queue number: 3\",\n",
       "      \"created_at\": 1692117903\n",
       "    },\n",
       "    {\n",
       "      \"object\": \"fine-tune-event\",\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Fine-tune is in the queue. Queue number: 2\",\n",
       "      \"created_at\": 1692118071\n",
       "    },\n",
       "    {\n",
       "      \"object\": \"fine-tune-event\",\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Fine-tune is in the queue. Queue number: 1\",\n",
       "      \"created_at\": 1692118574\n",
       "    },\n",
       "    {\n",
       "      \"object\": \"fine-tune-event\",\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Fine-tune is in the queue. Queue number: 0\",\n",
       "      \"created_at\": 1692118650\n",
       "    },\n",
       "    {\n",
       "      \"object\": \"fine-tune-event\",\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Fine-tune started\",\n",
       "      \"created_at\": 1692118742\n",
       "    },\n",
       "    {\n",
       "      \"object\": \"fine-tune-event\",\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Completed epoch 1/4\",\n",
       "      \"created_at\": 1692119793\n",
       "    },\n",
       "    {\n",
       "      \"object\": \"fine-tune-event\",\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Completed epoch 3/4\",\n",
       "      \"created_at\": 1692121708\n",
       "    },\n",
       "    {\n",
       "      \"object\": \"fine-tune-event\",\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Uploaded model: davinci:ft-datastax:coversation-next-message-dav-4-2023-08-15-18-05-07\",\n",
       "      \"created_at\": 1692122708\n",
       "    },\n",
       "    {\n",
       "      \"object\": \"fine-tune-event\",\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Uploaded result file: file-vOTRsJxhVhzZdVVxP7qwcNQL\",\n",
       "      \"created_at\": 1692122710\n",
       "    },\n",
       "    {\n",
       "      \"object\": \"fine-tune-event\",\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Fine-tune succeeded\",\n",
       "      \"created_at\": 1692122710\n",
       "    }\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.FineTune.retrieve(dav4_job_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check fine-tuning events\n",
    "\n",
    "* Lets us know specifics about the fine-tuning job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the events of our fine-tune job.\n",
    "events = openai.FineTune.stream_events(id=job_id)\n",
    "\n",
    "for event in events:\n",
    "    print(event)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at Training Results\n",
    "\n",
    "* download the results file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_prefix = \"coversation_next_message\"\n",
    "\n",
    "result = openai.FineTune.retrieve(id=job_id)\n",
    "count = 0\n",
    "for result_file in result[\"result_files\"]:\n",
    "    file_name = f'{work_dir}/{file_prefix}_{count}.csv'\n",
    "    file = open(file_name, 'wb')\n",
    "    file.write(openai.File.download(id=result_file[\"id\"]))\n",
    "    file.close()\n",
    "    print(f'Outputted results to: {file_name}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_model = \"ada:ft-personal:coversation-next-message-ada-2023-08-15-12-27-13\"\n",
    "cur_model = \"curie:ft-personal:coversation-next-message-ada-2023-08-15-12-45-56\"\n",
    "cur4_model = \"curie:ft-datastax:coversation-next-message-cur-4-2023-08-15-16-49-08\"\n",
    "dav_model = \"davinci:ft-datastax:coversation-next-message-dav-2023-08-15-16-40-40\"\n",
    "dav4_model = \"davinci:ft-datastax:coversation-next-message-dav-4-2023-08-15-18-05-07\"\n",
    "cur8_model = \"curie:ft-datastax:coversation-next-message-cur-8-2023-08-15-18-01-18\"\n",
    "\n",
    "\n",
    "model_id = cur_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnm_train = pandas.read_json(f'{work_dir}/conversation_next_message_examples_joined_prepared_train.jsonl', lines=True)\n",
    "cnm_valid = pandas.read_json(f'{work_dir}/conversation_next_message_examples_joined_prepared_valid.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Should maybe include in the release process?\n",
      "\n",
      "As a separate question -- how come we didn't know that #119 was broken until I ran into it? Shouldn't the integration tests have picked it up? My concern is that we could have easily said \"we have fixes in main, let's deploy\", and would have done so, and starting Friday this will break people evaluating the product.\n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "Completion:  nil\n",
      "Prediction:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7nnTyFMor9U9nfVcIX94tWCMG7L8O at 0x293a2f130> JSON: {\n",
       "  \"id\": \"cmpl-7nnTyFMor9U9nfVcIX94tWCMG7L8O\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1692102638,\n",
       "  \"model\": \"ada:ft-personal:coversation-next-message-ada-2023-08-15-12-27-13\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \" nil\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": {\n",
       "        \"tokens\": [\n",
       "          \" nil\"\n",
       "        ],\n",
       "        \"token_logprobs\": [\n",
       "          -1.2021104\n",
       "        ],\n",
       "        \"top_logprobs\": [\n",
       "          {\n",
       "            \" nil\": -1.2021104,\n",
       "            \" 1\": -1.5456975,\n",
       "            \" 2\": -2.3454945,\n",
       "            \" 5\": -1.8755838,\n",
       "            \" 10\": -1.4878904\n",
       "          }\n",
       "        ],\n",
       "        \"text_offset\": [\n",
       "          370\n",
       "        ]\n",
       "      },\n",
       "      \"finish_reason\": \"length\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 84,\n",
       "    \"completion_tokens\": 1,\n",
       "    \"total_tokens\": 85\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 6\n",
    "prompt = cnm_train['prompt'][i]\n",
    "completion = cnm_train['completion'][i]\n",
    "\n",
    "print(f'Prompt: {prompt}')\n",
    "print(f'Completion: {completion}')\n",
    "print(f'Prediction:')\n",
    "\n",
    "openai.Completion.create(model=model_id, prompt=prompt, max_tokens=1, stop=' end', n=1, logprobs=5, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Yep it seems like almost the exact same process between the articles.\n",
      "\n",
      "Let me know if theres anything we need to do around the lfs files. I don’t think I have a good mental model of how that stuff is used now (generic s3 bucket for file storage that all engineers have access to maybe? we could grant permission to gitlab runners for CI if necessary … but at that point, maybe gitlab lfs is the right thing?)\n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "Completion:  1\n",
      "Prediction:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7nnnFBYEsvDJ7I0oI0ehbkJ7yh0Zy at 0x293d9d590> JSON: {\n",
       "  \"id\": \"cmpl-7nnnFBYEsvDJ7I0oI0ehbkJ7yh0Zy\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1692103833,\n",
       "  \"model\": \"curie:ft-personal:coversation-next-message-ada-2023-08-15-12-45-56\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \" 1\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": {\n",
       "        \"tokens\": [\n",
       "          \" 1\"\n",
       "        ],\n",
       "        \"token_logprobs\": [\n",
       "          -0.7076458\n",
       "        ],\n",
       "        \"top_logprobs\": [\n",
       "          {\n",
       "            \" nil\": -1.7763753,\n",
       "            \" 1\": -0.7076458,\n",
       "            \" 2\": -2.6300569,\n",
       "            \" 5\": -1.5701545\n",
       "          }\n",
       "        ],\n",
       "        \"text_offset\": [\n",
       "          415\n",
       "        ]\n",
       "      },\n",
       "      \"finish_reason\": \"length\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 99,\n",
       "    \"completion_tokens\": 1,\n",
       "    \"total_tokens\": 100\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 220\n",
    "prompt = cnm_valid['prompt'][i]\n",
    "completion = cnm_valid['completion'][i]\n",
    "\n",
    "print(f'Prompt: {prompt}')\n",
    "print(f'Completion: {completion}')\n",
    "print(f'Prediction:')\n",
    "\n",
    "res = openai.Completion.create(model=model_id, prompt=prompt, max_tokens=1, stop=' end', n=1, logprobs=4, temperature=0)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  nil, Prob: 27\n",
      "User:  1, Prob: 37\n",
      "User:  2, Prob: 10\n",
      "User:  5, Prob: 17\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "logprobs = res[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0]\n",
    "for user in logprobs:\n",
    "    print(f'User: {user}, Prob: {round(math.exp(logprobs[user])*100)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6277269450106397"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "file_0 = 'conversation_next_message_examples_joined_prepared_valid_with_pred_cur8'\n",
    "\n",
    "df = pandas.read_json(f'{work_dir}/{file_0}.jsonl', lines=True)\n",
    "df[\"test\"] = None\n",
    "df[\"pred\"] = None\n",
    "\n",
    "for i in range(len(df)):\n",
    "    completions = df['completion'][i].strip().split()\n",
    "    df.at[i, \"test\"] = completions\n",
    "    prediction = df['prediction'][i]\n",
    "    if \"choices\" in prediction:\n",
    "        predictions = prediction[\"choices\"][0][\"text\"].strip().split()\n",
    "        df.at[i, \"pred\"] = predictions\n",
    "df = df[df.pred.notnull()]\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([['nil', '1', '2', '5', '10']])\n",
    "y_test_transformed = mlb.transform(df['test'])\n",
    "y_pred_transformed = mlb.transform(df['pred'])\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_test_transformed, y_pred_transformed, average='weighted')  # Or 'micro', 'weighted' based on need\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>prediction</th>\n",
       "      <th>test</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There may be a new one? Let me look\\n\\nThst al...</td>\n",
       "      <td>nil</td>\n",
       "      <td>{'id': 'cmpl-7nspPCMEGKQhvYYiYK93ncEZnFSMp', '...</td>\n",
       "      <td>[nil]</td>\n",
       "      <td>[nil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is there a way to configure k8s to automatical...</td>\n",
       "      <td>5</td>\n",
       "      <td>{'id': 'cmpl-7nspQ65xTtaIDzezrv1aUTMpO3hGJ', '...</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Store `computePb.Table` within `SnapshotMetada...</td>\n",
       "      <td>5</td>\n",
       "      <td>{'id': 'cmpl-7nspROlLDt3KvlWOoWZdccMeZPoeY', '...</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>?\\n\\n###\\n\\n</td>\n",
       "      <td>nil</td>\n",
       "      <td>{'id': 'cmpl-7nspRxEU2nQVbNDHgSdAbMe0zXugk', '...</td>\n",
       "      <td>[nil]</td>\n",
       "      <td>[nil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>approved, but left 2 minor comments\\n\\n###\\n\\n</td>\n",
       "      <td>nil</td>\n",
       "      <td>{'id': 'cmpl-7nspSX1Bl7ZyFSgaLSuf9iiLQaJ7b', '...</td>\n",
       "      <td>[nil]</td>\n",
       "      <td>[nil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>studio is just a blob of javascript… it can’t ...</td>\n",
       "      <td>nil</td>\n",
       "      <td>{'id': 'cmpl-7nsqMqYBcn7vEBH1wafmthCrO1Vtj', '...</td>\n",
       "      <td>[nil]</td>\n",
       "      <td>[nil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>This should be an integration/CI test, right? ...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'id': 'cmpl-7nsqM6kLgzpLxyN8jxsX2OnV3V0Vk', '...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[nil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>Next week please, ideally after Wednesday morn...</td>\n",
       "      <td>nil</td>\n",
       "      <td>{'id': 'cmpl-7nsqM36oNPWlOauaIgpTIKeWmnM62', '...</td>\n",
       "      <td>[nil]</td>\n",
       "      <td>[nil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>I believe the extra row was the first row?\\n\\n...</td>\n",
       "      <td>nil</td>\n",
       "      <td>{'id': 'cmpl-7nsqN7wCFkk84EVW7N3VVcFBrvsz2', '...</td>\n",
       "      <td>[nil]</td>\n",
       "      <td>[nil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>seems to be fine on the auth’d side so far\\n\\n...</td>\n",
       "      <td>nil</td>\n",
       "      <td>{'id': 'cmpl-7nsqNiaTokTRpw4p2C46XGqVWxkSL', '...</td>\n",
       "      <td>[nil]</td>\n",
       "      <td>[nil]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>481 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prompt completion  \\\n",
       "0    There may be a new one? Let me look\\n\\nThst al...        nil   \n",
       "1    Is there a way to configure k8s to automatical...          5   \n",
       "2    Store `computePb.Table` within `SnapshotMetada...          5   \n",
       "3                                         ?\\n\\n###\\n\\n        nil   \n",
       "4       approved, but left 2 minor comments\\n\\n###\\n\\n        nil   \n",
       "..                                                 ...        ...   \n",
       "476  studio is just a blob of javascript… it can’t ...        nil   \n",
       "477  This should be an integration/CI test, right? ...          1   \n",
       "478  Next week please, ideally after Wednesday morn...        nil   \n",
       "479  I believe the extra row was the first row?\\n\\n...        nil   \n",
       "480  seems to be fine on the auth’d side so far\\n\\n...        nil   \n",
       "\n",
       "                                            prediction   test   pred  \n",
       "0    {'id': 'cmpl-7nspPCMEGKQhvYYiYK93ncEZnFSMp', '...  [nil]  [nil]  \n",
       "1    {'id': 'cmpl-7nspQ65xTtaIDzezrv1aUTMpO3hGJ', '...    [5]    [5]  \n",
       "2    {'id': 'cmpl-7nspROlLDt3KvlWOoWZdccMeZPoeY', '...    [5]   [10]  \n",
       "3    {'id': 'cmpl-7nspRxEU2nQVbNDHgSdAbMe0zXugk', '...  [nil]  [nil]  \n",
       "4    {'id': 'cmpl-7nspSX1Bl7ZyFSgaLSuf9iiLQaJ7b', '...  [nil]  [nil]  \n",
       "..                                                 ...    ...    ...  \n",
       "476  {'id': 'cmpl-7nsqMqYBcn7vEBH1wafmthCrO1Vtj', '...  [nil]  [nil]  \n",
       "477  {'id': 'cmpl-7nsqM6kLgzpLxyN8jxsX2OnV3V0Vk', '...    [1]  [nil]  \n",
       "478  {'id': 'cmpl-7nsqM36oNPWlOauaIgpTIKeWmnM62', '...  [nil]  [nil]  \n",
       "479  {'id': 'cmpl-7nsqN7wCFkk84EVW7N3VVcFBrvsz2', '...  [nil]  [nil]  \n",
       "480  {'id': 'cmpl-7nsqNiaTokTRpw4p2C46XGqVWxkSL', '...  [nil]  [nil]  \n",
       "\n",
       "[481 rows x 5 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7583751948726832"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "file_0 = 'conversation_next_message_examples_joined_prepared_valid_with_pred_cur8'\n",
    "\n",
    "df = pandas.read_json(f'{work_dir}/{file_0}.jsonl', lines=True)\n",
    "df[\"test\"] = None\n",
    "df[\"pred\"] = None\n",
    "\n",
    "for i in range(len(df)):\n",
    "    completions = df['completion'][i].strip().split()\n",
    "    df.at[i, \"test\"] = [(completions != ['nil'])]\n",
    "    prediction = df['prediction'][i]\n",
    "    if \"choices\" in prediction:\n",
    "        predictions = prediction[\"choices\"][0][\"text\"].strip().split()\n",
    "        df.at[i, \"pred\"] = [(predictions !=  ['nil'])]\n",
    "df = df[df.pred.notnull()]\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([[False, True]])\n",
    "y_test_transformed = mlb.transform(df['test'])\n",
    "y_pred_transformed = mlb.transform(df['pred'])\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_test_transformed, y_pred_transformed, average='macro')  # Or 'micro', 'weighted' based on need\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>prediction</th>\n",
       "      <th>test</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There may be a new one? Let me look\\n\\nThst al...</td>\n",
       "      <td>nil</td>\n",
       "      <td>{'id': 'cmpl-7nsnY5vthDVTKvhWXohS7pnEPzta4', '...</td>\n",
       "      <td>[False]</td>\n",
       "      <td>[False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is there a way to configure k8s to automatical...</td>\n",
       "      <td>5</td>\n",
       "      <td>{'id': 'cmpl-7nsnaB97iEIp207T78h3fNm28F1rB', '...</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Store `computePb.Table` within `SnapshotMetada...</td>\n",
       "      <td>5</td>\n",
       "      <td>{'id': 'cmpl-7nsnbB7p9kCQRKzwPyaEZqElkfl2r', '...</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>?\\n\\n###\\n\\n</td>\n",
       "      <td>nil</td>\n",
       "      <td>{'id': 'cmpl-7nsnct5smI3DqUAQ6Qb5kQzYOFYyt', '...</td>\n",
       "      <td>[False]</td>\n",
       "      <td>[False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>approved, but left 2 minor comments\\n\\n###\\n\\n</td>\n",
       "      <td>nil</td>\n",
       "      <td>{'id': 'cmpl-7nsnctAeHYdzEv9eXzUGBUbITDMnp', '...</td>\n",
       "      <td>[False]</td>\n",
       "      <td>[False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>studio is just a blob of javascript… it can’t ...</td>\n",
       "      <td>nil</td>\n",
       "      <td>{'id': 'cmpl-7nsojyoUtbUqzq4ltRxTADCXxVeX4', '...</td>\n",
       "      <td>[False]</td>\n",
       "      <td>[True]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>This should be an integration/CI test, right? ...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'id': 'cmpl-7nsojbejfTFuVsbb2vIOGjcCvbtmP', '...</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>Next week please, ideally after Wednesday morn...</td>\n",
       "      <td>nil</td>\n",
       "      <td>{'id': 'cmpl-7nsojrbNxStMTpOb45i09kbjyhYNy', '...</td>\n",
       "      <td>[False]</td>\n",
       "      <td>[False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>I believe the extra row was the first row?\\n\\n...</td>\n",
       "      <td>nil</td>\n",
       "      <td>{'id': 'cmpl-7nsokjNZRbvMhcUFIoTSqNmKowzMJ', '...</td>\n",
       "      <td>[False]</td>\n",
       "      <td>[False]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>seems to be fine on the auth’d side so far\\n\\n...</td>\n",
       "      <td>nil</td>\n",
       "      <td>{'id': 'cmpl-7nsokSobFjonmmI69ogyf4KDKOpQm', '...</td>\n",
       "      <td>[False]</td>\n",
       "      <td>[False]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>481 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prompt completion  \\\n",
       "0    There may be a new one? Let me look\\n\\nThst al...        nil   \n",
       "1    Is there a way to configure k8s to automatical...          5   \n",
       "2    Store `computePb.Table` within `SnapshotMetada...          5   \n",
       "3                                         ?\\n\\n###\\n\\n        nil   \n",
       "4       approved, but left 2 minor comments\\n\\n###\\n\\n        nil   \n",
       "..                                                 ...        ...   \n",
       "476  studio is just a blob of javascript… it can’t ...        nil   \n",
       "477  This should be an integration/CI test, right? ...          1   \n",
       "478  Next week please, ideally after Wednesday morn...        nil   \n",
       "479  I believe the extra row was the first row?\\n\\n...        nil   \n",
       "480  seems to be fine on the auth’d side so far\\n\\n...        nil   \n",
       "\n",
       "                                            prediction     test     pred  \n",
       "0    {'id': 'cmpl-7nsnY5vthDVTKvhWXohS7pnEPzta4', '...  [False]  [False]  \n",
       "1    {'id': 'cmpl-7nsnaB97iEIp207T78h3fNm28F1rB', '...   [True]   [True]  \n",
       "2    {'id': 'cmpl-7nsnbB7p9kCQRKzwPyaEZqElkfl2r', '...   [True]   [True]  \n",
       "3    {'id': 'cmpl-7nsnct5smI3DqUAQ6Qb5kQzYOFYyt', '...  [False]  [False]  \n",
       "4    {'id': 'cmpl-7nsnctAeHYdzEv9eXzUGBUbITDMnp', '...  [False]  [False]  \n",
       "..                                                 ...      ...      ...  \n",
       "476  {'id': 'cmpl-7nsojyoUtbUqzq4ltRxTADCXxVeX4', '...  [False]   [True]  \n",
       "477  {'id': 'cmpl-7nsojbejfTFuVsbb2vIOGjcCvbtmP', '...   [True]  [False]  \n",
       "478  {'id': 'cmpl-7nsojrbNxStMTpOb45i09kbjyhYNy', '...  [False]  [False]  \n",
       "479  {'id': 'cmpl-7nsokjNZRbvMhcUFIoTSqNmKowzMJ', '...  [False]  [False]  \n",
       "480  {'id': 'cmpl-7nsokSobFjonmmI69ogyf4KDKOpQm', '...  [False]  [False]  \n",
       "\n",
       "[481 rows x 5 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find High User Probs inside validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "Although, if we're generating windows, maybe it should be named differently?\n",
      "\n",
      "`between(bool) -&gt; window` - non overlapping window between true values.\n",
      "\n",
      "`hourly() -&gt; window` - non overlapping window between hours\n",
      "\n",
      "`sliding(segment_window, duration) -&gt; window` -&gt; sliding window of `duration` `segmentwindow`\n",
      "\n",
      "We could allow an implicit conversion from a boolean to a window. Then the final one could be `count(foo, window=foo.n &gt; 10)`. But... that even seems a bit weird, doesn't it since you're using a boolean predicate as a window... so maybe `since` is nice for that, and just does the *explicit* conversion of a boolean to a window?\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.9513615280494057}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "a  one thing that occurred to me with the current incremental -- we still need to *download* all the files to get the min/max time, even if we don't need the file. Seems like that will become a long pole pretty quick. Once we get this iteration out, we'll probably want to think about having wren pass the stored `min/max` times for the prepared files, so that Sparrow can just skip even looking at the file stats (and also downloading it).\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.9387995259110659}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "assuming we still think that ticks should cover all entities/times in the tables for the given entity type (which I think is where we left things), I think the following steps will get us there:\n",
      "\n",
      "1. Refactor AST to DFG so that it knows the \"entity type\" of the expression being compiled. This will be a bit tricky since we'll need to first figure out the primary entity type, and then pass it down. But, I think this is already basically done as part of slicing, it just needs to be cleaned up / run as a pass before AST to DFG so it can be passed in, etc. We'll also need to figure out how to associate it with each expression so when we go in a lookup we still know it. It may just get added to `ResolvedExpr` or be a new struct similar to it.\n",
      "\n",
      "2. In the pass containing ticks, for now, connect it to the scan tables for the given entity types' tables.\n",
      "\n",
      "I think longer term, there may be more changes to make like we've been discussing to make the merge operations part of the DFG rather than the passes, but I'm not sure it's necessary.\n",
      "\n",
      "Let's discuss more tomorrow to see if we believe those are the right steps / more needs to be done.\n",
      "\n",
      "Completions: ['ryan']\n",
      "Predictions: {'jordan': 0.9734398348571481}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "I’m fine requiring non-null keys, we wouldn’t be the first DB to do so.\n",
      "\n",
      "s/avoid/reduce likelihood of/\n",
      "\n",
      "Sounds good. Long term it’s probably worth putting this check back in though to avoid the hot key issues you mentioned\n",
      "\n",
      "Completions: ['nil']\n",
      "Predictions: {'ben': 0.916596138287195}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "I think that's good wording, and lets us explain what \"the system knows about\" separately. Although we still have to solve that\n",
      "\n",
      "I wonder if we can just say that ticks create their own set of rows. Rather than making it dependent on the merged set of data (which could happen under the hood), why can't we just say \"there is a set of entities that the system knows about (...) and ticks enumerate those on each tick\". At that point, the rows the tick produces merge with the other rows when used in operations.\n",
      "\n",
      "It potentially avoids a weird problem where we say \"we tick over the union of data\" and \"the union of data includes what we tick over\", leading to\n",
      "\n",
      "`ALL_DATA = union(TABLES..., ticks(ALL_DATA))`\n",
      "\n",
      "(Which granted we could in theory solve by saying we wanted the \"smallest set satisfying that relation\"... but seems painful.\n",
      "\n",
      "Completions: ['ryan']\n",
      "Predictions: {'jordan': 0.9034801042002569}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "FYI I think I found a commitable step towards explicit operations. My thinking is:\n",
      "\n",
      "• Step 1. Separate the `StepKind` to indicate operations and expressions. Do *no* immediate changes to the actual structure of the DFG. At this point, everything behaves as it does now, but we've separated the enums to indicate which is which.\n",
      "• Step 2. Start adding assertions (expressions should have an operation argument, etc.). This is moving us towards the DFG we want, without actually changing any scheduling / planning / execution.\n",
      "• Step 3 (a little fuzzy). At some point, make a switch to using the operation ID on expressions to do the scheduling. At this point, the existing scheduling code may go away.\n",
      "Step 3 could be implemented (possibly) using the existing plans (although we'd need to figure out what to do with things like merge operations), or it may mean rebasing the passless work and having explicit operation/expression in the plan proto, and executing that. Either way, I think step 1 and 2 can be committed to main (to minimize bit-rot), and then step 3 should be in distance. Sound reasonable?\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.9792045084665937}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "Yes, agreed. No longer have implicit newness that affects whether a subsequent aggregation is updated in this example\n",
      "\n",
      "Completions: ['nil']\n",
      "Predictions: {'ben': 0.8714369527608611}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "Oh -- the MR that deletes old code paths from the `operations` branch is also probably ready to go in (on the branch) so that we can start doing the MRs that aren't compatible with `main `on it. I'll be rebasing that branch regularly as we make changes on `main`.\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.9732271663547793}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "That explanation misses the fact that you can only omit one required argument to use `$input`.\n",
      "\n",
      "(Trying to clean up some of the weirdness in substring and shifts, specifically)\n",
      "\n",
      "do you remember why we didn't just allow `$input` as a default value in the signature? I guess there are cases where it could be useful as any of the arguments, but since we prefer the default value over `$input` it never seems to apply there.\n",
      "\n",
      "Right now, I'm thinking there could be a few ways to simplify arguments:\n",
      "\n",
      "1. If the parameter has a default value, you must pass it's argument by name. Since the default value indicates it is optional and likely configures the behavior, the name serves to document the option.\n",
      "2. (I'm thinking) about whether we can get rid of the implicit `$input` case, and just specify that as the default where appropriate. After that, the only arguments without default values would be \"required parameters\" and would come first. The only function I see that would be weird with that is `powf(base f64)` since it's not clear what the defaults would be.\n",
      "\n",
      "Completions: ['ryan']\n",
      "Predictions: {'jordan': 0.9490472759657517}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "I don't feel like that should be very common - the only use case I can think of would be better solved with a results-at execution config\n",
      "\n",
      "Completions: ['ben']\n",
      "Predictions: {'ben': 0.949026000462397}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "What do we mean by \"don't pass through\" -- if the minimum time in the batch is greater than the literal we're shifting to, then none of the values can be output.\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.9471371163924699}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "Merging. I'll look at sending heartbeats during execution when I get a chance, but I believe once it goes in we can start looking at the Wren changes necessary to consume it.\n",
      "\n",
      "Completions: ['eric']\n",
      "Predictions: {'jordan': 0.8464069556406059}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "two ideas for how we could make it less verbose to pass things to notes. The basic idea is to create a trait for things we want to be able to include, and then either explicitly use that, or create a `note!` macro that is like `format!` but implicitly uses that.\n",
      "\n",
      "\n",
      "\n",
      "Mocked up there -- if either seem useful, I can flesh them out / document / add more tests / etc.\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.885677827811286}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "have we decided what we are calling the limit/preview feature?  I’d like to start plumbing it through wren &amp; python today.\n",
      "\n",
      "Completions: ['nil']\n",
      "Predictions: {'ben': 0.9688861463707357}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "Stores reporting profitable revenue in the past month? And they only emit data when they are profitable =&gt; we would drop stores that aren't profitable \"recently\"\n",
      "\n",
      "Although, the above example may not really make sense to use windows\n",
      "\n",
      "Counter: absence of a value for an entity is just as significant a data point.\n",
      "\n",
      "If a user wants to see when the last log in time for their customers was, and wants the \"final\" values, they'll only see customers that have logged in \"recently\"\n",
      "\n",
      "Completions: ['ben']\n",
      "Predictions: {'ben': 0.8791769968178144}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "`table-id` is uuid\n",
      "\n",
      "table metadata is stored by `table-id`, not by name\n",
      "\n",
      "maybe helpful to have the id only… if table was replaced could help us invalidate old snapshots\n",
      "\n",
      "maybe also include a map of `table-id` to `table-name` ?  in case table is replaced with one of the same name but different data?\n",
      "\n",
      "actually I think this is a non-issue, because replacing the table would change the datatoken\n",
      "\n",
      "if we're worried about space at all, I think storing UUID as bytes is smaller than storing the string encoded version... but\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'ben': 0.801140621415051}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "from docs, looks like cross-region copy is possible, at least with the CLI\n",
      "\n",
      "Could complicate the communication paths though\n",
      "\n",
      "That may not be a huge deal, given that we need to DL the file anyway to check the schema\n",
      "\n",
      "maybe… but that won’t help us in the cross-region case\n",
      "\n",
      "Could we assume a role provided by users rather than using credentials they provide?\n",
      "\n",
      "Completions: ['ben']\n",
      "Predictions: {'ben': 0.8018823801805239}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "Number 1 is nice. Kind of in the opposite direction, it would be interesting if we could dump the partially created plan up until the time it fails, to see how we were attempting to wire things up\n",
      "\n",
      "Completions: ['ben']\n",
      "Predictions: {'ben': 0.8527379061152638}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "Do we run the old and new wren at the same time? Or support both in the same service?\n",
      "\n",
      "Keep in mind that versioning the API means creating a new set of Protos and having all the code to convert between them, so introduces a lot of boilerplate to maintain\n",
      "\n",
      "I can see arguments either way.\n",
      "\n",
      "We've looked at googles policies before, and for channel based releases (recommended) they update alpha and beta in place:\n",
      "\n",
      "Completions: ['eric']\n",
      "Predictions: {'eric': 0.8538404665181764}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "Is it just pushing the output channel ownership to the `query_executor` layer rather than the `compute_executor`?\n",
      "\n",
      "Completions: ['ben']\n",
      "Predictions: {'ben': 0.8848658352977876}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "Right. Basically both versions are saying \"treat `transform(value in operation N, to operation N)` the same as the `value`\".\n",
      "\n",
      "In one case, we handle it by rewriting the DFG expr to remove such `transform` nodes. In the other case we just add special handling that says \"if you encounter this transform node, just reference the plan you made for the value\"\n",
      "\n",
      "Benefits:\n",
      "\n",
      "1. It seems easier / less likely to have weird affects compared to having the plan \"skip them\" (although I could try that if we wanted to avoid having an extra pass rewrite things).\n",
      "\n",
      "2. Even if simplification didn't saturate, it would still get rid of the transform nodes\n",
      "\n",
      "Risk:\n",
      "\n",
      "If we run it *after* simplification, and simplification doesn't include a similar rule, then simplification may miss opportunities that the equivalence would have enabled. Doesn't seem a problem because we could have both points do it.\n",
      "\n",
      "Thoughts on whether we should have a separate \"remove useless transforms\" pass vs. have the DFG-&gt;plan try to remove them?\n",
      "\n",
      "Actually... one easy way to handle the \"useless transforms\" would be to just iterate over the nodes in an expression and \"skip\" them. It should be pretty easy to handle that.\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.9652879773865113}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "I think the bigger win here would be if we didn't need to operate on the string, and just had that map from string -&gt; key hash. Then we wouldn't plumb the key around, and would only need to use it in the final output.\n",
      "\n",
      "Not yet, and for *many* instructions it wouldn't be a significant win (often the threading overhead is higher than for instance doing the arithmetic). I think the problem is that these are something like 30-40 character strings, so each row has to allocate that and such.\n",
      "\n",
      "(eg., more than half of the time in compute is *just* plumbing the msno string through every row)\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.8373731662794669}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "Shouldn't the merge train be built as if they were on main (and thus have the right sha)?\n",
      "\n",
      "Could we build and tag with the git hash in the merge train, and then just tag `latest` and push?\n",
      "\n",
      "Considering the CI process: what if we moved all the build/deploy stuff to run on the mergetrain? That means we’ll be building and pushing things before they’re _actually_ merged, but in this case we can run the integration test on actual merge to main — and let the merge train handle order of deployment operations.\n",
      "\n",
      "Completions: ['eric']\n",
      "Predictions: {'eric': 0.8087195698713312}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "Yep it seems like almost the exact same process between the articles.\n",
      "\n",
      "Let me know if theres anything we need to do around the lfs files. I don’t think I have a good mental model of how that stuff is used now (generic s3 bucket for file storage that all engineers have access to maybe? we could grant permission to gitlab runners for CI if necessary … but at that point, maybe gitlab lfs is the right thing?)\n",
      "\n",
      "Completions: ['ben']\n",
      "Predictions: {'ben': 0.8088075471711403}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "then we'd be guaranteed that simplification could never eliminate stuff from the domain\n",
      "\n",
      "and a separate \"operation\" for each distinct domain\n",
      "\n",
      "not fully flushed out, but yeah -- we create a separate e-graph managed DFG for each \"operation\"\n",
      "\n",
      "Partly inspired from picking apart (a) how datafusion and other systems (eg., substrait) represent SQL plans and (b) looking at how datafusion-tokamak (their egg based optimizer) handles expressions\n",
      "\n",
      "Thought. We may not need to make domains part of the DFG explicitly -- maybe we just use them when producing the DFG to determine where we need to merge things. Something like the following (only partly baked).\n",
      "\n",
      "• A domain is a non-empty set of unique IDs that represent the logical contents of the domain.\n",
      "• The domain of a table scan node `dom(table_scan)` is `{ table_scan }`. Any operation that creates a new \"domain\" I believe this applies to any of the instructions that correspond to operations (eg., `shift`, `when`, `lookup`, `tick`, etc.).\n",
      "• The domain of a binary operation `dom(a + b)` is `dom(a) U dom(b)`. Note that if both expressions in the same domain, then the result is in that domain. Otherwise, the domain is a new set which has to union (merge) the corresponding domains. In this case, the resulting expressions are added to a different \"domain specific DFG\".\n",
      "At the end of creating the DFG, we now have a list of DFGs, each associated with a unique domain. We can order them topologically (a domain must be in an operation before any domain it is either a part of or a subset of).\n",
      "\n",
      "Note: This fixes the `x - x =&gt; 0` problem, since it only simplifies expressions *within* a given domain.\n",
      "\n",
      "The result of this would be a plan with explicit operations (scan, merge, etc.). Each \"operation\" would run on a separate thread.\n",
      "\n",
      " thoughts? also sent over some pictures of various ways of showing the DFG / plan to see if any of them feel like they fit the best.\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.8947332095796798}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "I think the complexity of 1 is limited since we already have that logic for gathering\n",
      "\n",
      "We (sometimes) still do the concatenation, but only of the slice that needs to be merged.\n",
      "\n",
      "Thoughts on approach for updating the merge?\n",
      "\n",
      "Right now, we pass the merge kernel a list of list of batches. Each outer list represents an input stream that can be concatenated to get an order set of input for merging.\n",
      "\n",
      "The question is where do we implement the limit -- where do we slice to \"less than the next tick\".\n",
      "\n",
      "Path 1: The \"merge driver\" slices the list of list of batches. This will generally involve taking 0 or more batches, plus (possibly) part of a batch. For the partial batch, it will likely need a binary search. Benefit of this path is that it keeps complexity out of the merge kernel (which would get harder to test if we add complexity)\n",
      "\n",
      "Path 2: The \"merge driver\" passes the whole list to the merger which concatenates and then slices. This concatenation is likely expensive (allocating the entire result) and unnecessary. Not many apparent benefits?\n",
      "\n",
      "Path 3: The \"merge driver\" does the concatenation and slices on that. This is similar to Path 1, but maybe a bit simpler (operating on one array per input). Downside compared to Path 1 is needing to allocate the entire concatenation even when the tick-rate may make it unnecessary.\n",
      "\n",
      "I'm *thinking* that path 1 is likely the best. And we already have that logic (basically) in gather, so just need to figure out how to split that out. Sound reasonable?\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.8121811439643413}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "Seems to be missing the operation for the transform\n",
      "\n",
      "For arguments to an operation... it may depend on the operation... that said, we may be able to figure out how to infer / transform in `add_operation`, at which point `add_pattern` should get the right behavior, and then the implementation patterns could be simpler\n",
      "\n",
      "For an experssion, it's relatively easy since we know we want to merge everything to be in the same operation.\n",
      "\n",
      "so the `transform` inside of the select would not be added (since that is part of adding the `select` which is an operaiton)\n",
      "\n",
      "But, the other reason is that the logic for adding stuff only applies to expressions\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.847667007622111}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "So there is one thread for the \"table\", and one thread for the \"pass merge\".\n",
      "\n",
      "For a table, we also need to merge the batches from each file.\n",
      "\n",
      "That's true of the pass merger.\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.8785002383853868}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "I think the use of exclusive intervals at the top in the gatherer could potentially lead to problems if we tried to emit rows at the *maximum* timestamp. I think it's OK to ignore that for now (we could even assert that input timestamps are less than `u64MAX` for now, it would complicate the logic to handle:\n",
      "\n",
      "`None` -&gt; not ready to emit\n",
      "`Some(None)` -&gt; ready to emit everything\n",
      "`Some(Some(t))` -&gt; ready to emit values &lt; t\n",
      "\n",
      "(Or a custom enum).\n",
      "\n",
      "Thoughts on just adding an assert that the `max_time &lt; u64MAX` for now?\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.8589662909226073}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "Think it's better to leave the tests failing, or mark the ones that need to be fixed as `ignored` based on the bug?\n",
      "\n",
      "Decision: we'll lean towards a branch; we'll ignore CI for now (will still make things pass before merge-to-main).\n",
      "\n",
      "Pro: We could ignore trying to make the changes \"backwards compatibly\" to the old code path\n",
      "\n",
      "Con: Would (probably) want to figure out how to update CI to run tests on the branch\n",
      "\n",
      "Con(???): Would (probably) need to delete old code path on the branch to avoid compile errors from new code\n",
      "\n",
      "want to discuss whether we should work on a branch this week? trying to figure out how to get the operation stuff through faster\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.8595270008939638}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "Glad I wrote out the question -- my initial assumption was that we'd do path (2) since that is closer to \"implement as-of join\"... but I think path (1) is actually a lot cleaner. Writing it up / discussing is often helpful\n",
      "\n",
      "And I think if we basically add `(last (transform ?value ?dest_op) ?dest_op)` (instead of just `(transform ?value ? dest_op)`, then we preserve continuity. And, any rewrites we do should preserve continuity unless they (a) drop the `last` (which seems like it should never be correct) or (b) add a transform after the last... but ideally simplifications don't add transforms (they just rewrite equivalent operations / change the operation within the transform).\n",
      "\n",
      "So I think we should have the \"correct\" behavior here.\n",
      "\n",
      "Yeah. My thoughts are roughly along those lines too. If we can do it during AST -&gt; DFG, that would be ideal. We already have continuity information there, so it would mean we wouldn't need to recompute it or carry it through things. The one risk here is that if we change the DFG heavily, it may mean that we need to be more careful (eg., if we translate to the DFG in a way that preserves continuity, but then rewrite in a way that loses it). But, we should just take that as an invariant -- rewriting the DFG shouldn't change continuity behavior.\n",
      "\n",
      "Looking at adding the `last` aggregation to preserve continuity. There seem to be two ways to do it. Thoughts on:\n",
      "\n",
      "1. While creating the DFG, whenever we add a `transform`, if the `value` is continuous we need to wrap it in `last`.\n",
      "2. While converting the DFG to the plan, we can add `last` in operations after pulling things in.\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.9189184117016034}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "Right -- this is why we had to come up with the concept of `else` as changing the default (rather than changing the actual value). Because in this case, there are *no* values in the stream, so the `last` is just outputting the \"default\" (`null`).\n",
      "\n",
      "One way to think of this is from a streams perspective:\n",
      "\n",
      "• `e` is a stream containing `[null]`\n",
      "• `sum(e)` is a stream containing `[]` because it hasn't seen any new input\n",
      "• `sum(e) | else(0)` is a stream containing `[]`\n",
      "• `sum(e) | else(0) | last()` is a stream that hasn't seen anything ,thus it outputs `null`\n",
      "\n",
      "The `else` seems like it changes things\n",
      "\n",
      "^^^ that\n",
      "\n",
      "The problem is that the `last` hasn't \"seen\" any values, so there is no \"last value\" -- the `sum` hasn't output anything.\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.8948741724788546}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "re:Window`. We definitely *could* remove it. We would just need to handle aggregations before we process arguments. I'm gonna see if there is an easy way to do something for now. But I also wrote down a short thought on handling generators (ticks and windows) more holistically in case we want to revisit that at some point.\n",
      "\n",
      "\n",
      "\n",
      "The basic idea would be to just indicate which argument position expects a generator, and determine the merged-operation of all other arguments, and use *that* as the value of the `$condition_input` while evaluating the generator. That would basically be the generalization -- the generator would automatically range over the nearest enclosing generator parameter containing it.\n",
      "\n",
      "I probably won't go so far as to play with that, but I'm going to do some of the work to tear out windows / normalize how we're handling arguments a little.\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.960674038938163}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "Path 1 seems the most complex. Does it feel like that'll be open to implementing the other options we discussed in the future?\n",
      "\n",
      "For path 3, clarify for me - right now we don't need to do the outer allocation because we already have the list of record batches from a source, and we just iterate over that list of batches instead of concatenating the into one larger batch\n",
      "\n",
      "Completions: ['ben']\n",
      "Predictions: {'ben': 0.879593508841178}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "yes it would fail if the command had problems… but if that fails we have bigger problems\n",
      "\n",
      "the only assertion inside a go-routine inside a test is asserting that we killed wren correctly.  If this assertion fails, something major in the test run didn’t work correctly.\n",
      "\n",
      "Completions: ['ben']\n",
      "Predictions: {'ben': 0.9458974023380001}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "we could have something that lets a merge join with a literal simplify away, but that wouldn't help with the \"no simplifier\" tests, so it's not a great option (also if simplification times out, etc.)\n",
      "\n",
      "not sure about the error in merge join... we could probably add a check in the DFG builder that basically says if we try to create a merge join node for something with `none` it just returns the operation... but that would require more interception of the pattern -- can maybe look on monday?\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.9815410710059943}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "the dropping is just a temporary thing until we can handle schema merging in our system. \n",
      "\n",
      "thou i could include everything we currently have in the schema, and just drop any newly added fields. \n",
      "\n",
      "lets chat tomorrow\n",
      "\n",
      "Completions: ['nil']\n",
      "Predictions: {'ben': 0.918697157275321}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "Are there currently cases where aggregations produce discrete values? I remember there was some talk about it but I think we ended up not going that route?\n",
      "\n",
      "I think you’re asking if sliding windows are continuous or discrete, no?\n",
      "\n",
      "Completions: ['nil']\n",
      "Predictions: {'ben': 0.8965642508147451}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "3rd option would be to add all the edges while creating the DFG, not allow simplification to remove them (only simplify within an operation), and have some separate pass that tries to combine/simplify operations.\n",
      "\n",
      "Ok. I think I got the approximately right plan for lookups, but now I'm going to need to work a little more on merge. Specifically, in this plan, note that the merge operation (operation 6) only inputs from operation 0 and 5 (left and right), but attempts to read from operation 0, 1, and 4. I think this is reasonable, but the issue is that operations 1 and 4 should be read indirectly through the operation 5 which is `merge(1, 4)`.\n",
      "\n",
      "The problem is that right now, a single `transform` can lift a value up through multiple layers of `merge_join`, but at plan/execution time, we can only read from the direct inputs to the operation. I'll think a bit, but two strategies seem to be:\n",
      "\n",
      "1. Update the DFG so transforms only go through one layer. This could be a post-simplification change. The idea being to let the DFG start with these \"short-cuts\" (which has benefits so that simplification doesn't need to maintain all the links), but then we create the necessary edges as part of getting ready to make the plan. This would be similar to how we do the rewriting for slices (and may even be combined with it).\n",
      "2. Have the plan building code handle this. I think that could get messy (would need to track the chain of operations that a given input needs to be added to in order to be available at some point).\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.9932019276522289}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "I’m ooo today but that’s my understanding\n",
      "\n",
      "A simplistic explanation would be there are two types of output from a sliding window (common in other tools too) emit incomplete windows or only emit complete windows. The above combined with a when would emit only complete windows.\n",
      "\n",
      "Yes, this is the current behavior\n",
      "\n",
      "Completions: ['nil']\n",
      "Predictions: {'ben': 0.8533295532257709}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "I extracted the pieces of `spread_zip` I had in progress (to fix the problem in `shift_to` using `zip`) and created this draft MR -- \n",
      "\n",
      "There are still definitely opportunities for improving it -- test coverage is lacking, probably could write some special cases when there are no null values that avoid logic inside loops, etc. but it hopefully gives you some place to start with for testing `shift_to`.\n",
      "\n",
      "LMK if there any discussions would be helpful on `shift_to` or anything else.\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.8044572601015739}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "Oh, I think you said a) is requiring explicit `$input` though\n",
      "\n",
      "So as long as `$input` is used within the pipe, but not making it required in the `coalesce` function\n",
      "\n",
      "Requiring `$input` for varargs arguments makes sense to me. It does suggest we might want to add the “free variables” analysis we used to have that throws an error if you assign a name and then don’t use it.\n",
      "\n",
      "Completions: ['ben']\n",
      "Predictions: {'ben': 0.8558903301773695}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "sounds good.  I’m probably still 30-60 mins out from having my MR.  I’ll probably submit it as-as if integration tests are passing, and then move to a new MR for adding slice-based integration tests.\n",
      "\n",
      "Completions: ['ben']\n",
      "Predictions: {'ben': 0.814250582047111}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "In my mind, when used for label time, I have already defined the events I want to be included I am just shifting them forward in time to compute a label. I wouldn't expect to reduce the set that I have. i.e. if I have 200 unique subscription start dates and I shift them all to January 1, 2021 I expect 200 unique predictors shifted to the label time and then the target feature computed .\n",
      "\n",
      "Completions: ['ben']\n",
      "Predictions: {'ben': 0.8292863063175236}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "I assume I need to wait for the results to come back from a query before i can do multiple successive queries\n",
      "\n",
      "is it possible that queries are taking a long time because we turned on prepare and i haven't queried this data set yet?\n",
      "\n",
      "Completions: ['ben']\n",
      "Predictions: {'ben': 0.9062182789171829}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "I wonder if it would be helpful to allow using a record with fewer fields as a record with more fields implicitly. For instance, the case where we need to have the \"timestamp\" field as `null` could just be omitted, and since the field is missing we could treat it as null.\n",
      "\n",
      "Completions: ['nil']\n",
      "Predictions: {'jordan': 0.8171980681730846}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "pushes `anyhow` further through the `sparrow-main` code. I *think* this will allow me to use to start using it in the MR for using table metadata in analysis, since it's now one step away from the top-levels of the runtime and compiler crates.\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.9565226694160326}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "FWIW, I would prefer `sliding(5)` over `sliding(5, true)` - if we could make the window an optional argument it would align better with my expectations\n",
      "\n",
      "Completions: ['ben']\n",
      "Predictions: {'ben': 0.9611621766781695}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "My understanding is that we can communicate results by either returning a value or raising an exception. I haven’t seen any mechanisms for producing warnings.\n",
      "\n",
      "maybe  has thoughts on this.  I don’t have a good understanding on how fenl magic works yet\n",
      "\n",
      "Completions: ['nil']\n",
      "Predictions: {'ben': 0.873366041060854}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "let me know when you've had a chance to see if the fix works out. If it does, we can try removing the environment variable to re-enable projection pushdown.\n",
      "\n",
      "Completions: ['ryan']\n",
      "Predictions: {'jordan': 0.9562487815866068}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "I like this in general.  I’m concerned a bit about now storing additional TableMetdata (schema) for each prepared set of files.\n",
      "\n",
      "Completions: ['ben']\n",
      "Predictions: {'ben': 0.9801408952384986}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "In other news, I just signed up for an account to use for the “real” load and I’m seeing CORS failures in the UI when it tries to get my credentials\n",
      "\n",
      "Completions: ['eric']\n",
      "Predictions: {'ben': 0.8791862985596468}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "sometime we should follow up on the error discussion -- I think we were leaning towards changing it so that a query with invalid Fenl returned a response, but included fenl errors in the response, rather than error details.\n",
      "\n",
      "Completions: ['eric']\n",
      "Predictions: {'jordan': 0.960683633237699}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "FYI I'm possibly wrecking some of the ignored resumable tests to get Kevin's prepare changes working. I *think* it makes sense though. Basically, where we previously had two separate files that we sent, I'm concatenating that to be a single CSV, since we'll first test (a) fake resumable using the entire data set and a cutoff and (b) later want to do something like \"first run on this CSV and checkpoint\", then \"run on that CSV\". I think the version flagging what was new is likely \"deprecated\" / \"unsupported\" anyway, so this doesn't really break anything that was *working*\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.8357999573477246}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "Changing them to `i64` may be the easiest for now. I think we should look at something better, but don't think it makes sense to pull that in right now. Basically, Cockroach is somewhat similar to Marlin:\n",
      "\n",
      "1. Bidirectional inference that passes down \"expected types\" and passes up \"inferred type\". This largely resolves ambiguity around literals.\n",
      "2. Something that tracks the set of \"possible types\" for variables. Eg., `let x = 5` needs to figure out the type of `x` similarly to if the literal `5` were inlined there.\n",
      "Totally do-able, but more than a \"quick fix\" for the arguments to sliding.\n",
      "\n",
      "Found an article about how CockroachDB did type checking for SQL, which is similar. I'll see if there is anything there that could make be easily incorporated to allow us to omit this annotation.\n",
      "\n",
      "There are problems with 4.\n",
      "\n",
      "1. The type checker doesn't know the input is a literal / or it's value, so that is extra info. This could be by having a special `literal(57)` type, or by passing the `Option&lt;ScalarValue&gt;` or something to type checking.\n",
      "2. That kind of implicit conversion would allow `sliding(5)` to convert the 5 to `u64` to do the addition. But, it may take more to handle `add(5, 1u64)`. Specifically, in the latter the current default behavior is to widen both the `i64` and `u64` to `f64`. But, we should be able to use the same `Option&lt;ScalarValue&gt;` information to avoid.\n",
      "3. How far to go? Do we only do it for explicit literals? What about `sliding(1 + 1)`? Do we treat that as the literal \"2\"?\n",
      "The alternative (option 5) is to do some limited bi-directional type inference.\n",
      "\n",
      "Free to discuss? I think we intentionally did this, so we have a few options for how to fix:\n",
      "\n",
      "1. Change the signature to take `i64` since that is the \"default\" type for numeric literals\n",
      "2. Write `0u64` or something like that\n",
      "3. Using the narrowest type possible based on the number and relying on implicit widening\n",
      "4. Allow implicit numeric conversions of constants as long as we don't lose precision\n",
      "I *think* 4 is the most complete solution.\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.9185487959904254}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "The argument to the select itself is just the condition because multiple values may be filtered (transformed) to the same condition\n",
      "\n",
      "Yeah. I think the value should be used with a transform and the select operation\n",
      "\n",
      "Completions: ['nil']\n",
      "Predictions: {'ben': 0.8453833634098205}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "This could be especially nice, because then we don't have to deal with anything like \"need all input from the current slice\" type behaviors -- a file just indicates \"I have nothing to contribute until my minimum time\", but it does so *immediately*, so the logic is \"wait for all input streams to either produce something or say they have nothing\".\n",
      "\n",
      "Similarly, that `EmptyTo(...)` ability allows shifts to unblock downstream stages by saying \"everything I'm going to output is later... so downstream can go ahead\".\n",
      "\n",
      "Completions: ['ryan']\n",
      "Predictions: {'jordan': 0.883264576167924}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "yeah, on board with that\n",
      "\n",
      "Agreed, I'd be happy if we were very strict with our simplifications of the dfg\n",
      "\n",
      "Suppose this ties in with our earlier conversation on the logical/physical split. I think we're always going to want to be doing some forms of simplification/rewriting in the `dfg`, so I'd lean towards keeping that complexity within the `dfg` , and reducing the amount of changes we make as we build the `plan`.\n",
      "\n",
      "Completions: ['ben']\n",
      "Predictions: {'ben': 0.9196399814843376}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "We can change this in the future but it's how I've treated leads in the past\n",
      "\n",
      "In my mind, until a lead expresses interest in Kaskada they aren't really a contact at an Account. While they are warm because we know them, they haven't expressed interest yet.\n",
      "\n",
      "Completions: ['nil']\n",
      "Predictions: {'ben': 0.8137789581827947}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "First part of query execution out for review. This extends the slice analysis (and the compiler frontend) to rewrite the expression to scan tables with the needed slice plans.\n",
      "\n",
      "Completions: ['nil']\n",
      "Predictions: {'jordan': 0.9899310873635381}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "Once the literal stuff goes in, should I try to pick up that notebook and get things working, or would it be more helpful to look at projection pushdown / shift_until performance type stuff?\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'jordan': 0.97916701642294}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "This is how we were running Spark at RP, fwiw. I remember it taking about a week to get working the first time - hopefully it’s in a less “alpha” state by now\n",
      "\n",
      "Completions: ['nil']\n",
      "Predictions: {'ben': 0.8512537382882591}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "Option 1 - Look into transform to plan to see if we can get smarter about removing transforms that are transforming expressions into the current operation (also, what happens if we just allow that transform to stay?)\n",
      "\n",
      "Option 2 - We want to do explicit merging for ticks, since we need to solve . I suspect that will solve this issue, since we do some rather weird changes to the dfg to remove merges when we tick.\n",
      "\n",
      "Completions: ['ben']\n",
      "Predictions: {'jordan': 0.8740078689428731}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "mostly about how / where to store it… but I can figure that out…\n",
      "\n",
      "I like this in general.  I’m concerned a bit about now storing additional TableMetdata (schema) for each prepared set of files.\n",
      "\n",
      "Completions: ['ben']\n",
      "Predictions: {'ben': 0.9751502385836895}\n",
      "\n",
      "Found example with high probability:\n",
      "\n",
      "Prompt:\n",
      "I don’t see how that changes things - the point is that the expression produced a `null` value at a particular time for a particular entity; we should be able to specify the context, no?\n",
      "\n",
      "Yeah, I’d expect the time, subsort, and entity to be populated\n",
      "\n",
      "Completions: ['jordan']\n",
      "Predictions: {'ben': 0.9722681306348201}\n",
      "\n",
      "Total Examples: 65 of 481 lines.\n",
      "User example count: {'jordan': 34, 'ben': 29, 'eric': 2}\n",
      "F1 score (weighted): 0.5458958432871477\n",
      "F1 score (macro): 0.4134161490683231\n"
     ]
    }
   ],
   "source": [
    "file_0 = 'conversation_next_message_examples_joined_prepared_valid_with_pred_dav4'\n",
    "\n",
    "min_prob = 0.80\n",
    "\n",
    "user_name = {\"1\": \"ben\", \"2\": \"ryan\", \"3\": \"marcial\", \"4\": \"charna\", \"5\": \"eric\", \"6\": \"kevinn\", \"7\": \"tina\", \"8\": \"davor\", \"9\": \"karina\", \"10\": \"jordan\", \"11\": \"brian\", \"12\": \"janoo\", \"13\": \"theo\", \"15\": \"darci\", \"19\": \"bradley\"}\n",
    "\n",
    "example_count = 0\n",
    "line_count = 0\n",
    "user_example_count = {}\n",
    "\n",
    "test = []\n",
    "pred = []\n",
    "with open(f'{work_dir}/{file_0}.jsonl') as file:\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "\n",
    "        if not line:\n",
    "            break\n",
    "\n",
    "        data = json.loads(line)\n",
    "        if 'choices' not in data['prediction']:\n",
    "            continue\n",
    "\n",
    "        line_count += 1\n",
    "        logprobs = data['prediction']['choices'][0]['logprobs']['top_logprobs'][0]\n",
    "\n",
    "        high_users = {}\n",
    "        for user in logprobs:\n",
    "            logprob = logprobs[user]\n",
    "            user = user.strip()\n",
    "            if user == 'nil':\n",
    "                continue\n",
    "            prob = math.exp(logprob)\n",
    "            if prob > min_prob:\n",
    "                name = user_name[user] if user in user_name else user\n",
    "                high_users[name] = prob\n",
    "                if name in user_example_count:\n",
    "                    user_example_count[name] += 1 \n",
    "                else:\n",
    "                    user_example_count[name] = 1\n",
    "\n",
    "\n",
    "        if len(high_users) > 0:\n",
    "            example_count += 1\n",
    "            print(\"\\nFound example with high probability:\\n\")\n",
    "\n",
    "            comp_users = []\n",
    "            completions = data[\"completion\"].removesuffix(\"end\").strip().split(\" \")\n",
    "            for completion in completions:\n",
    "                comp_name = user_name[completion] if completion in user_name else completion\n",
    "                comp_users.append(comp_name)\n",
    "\n",
    "            prompt = data[\"prompt\"].removesuffix('\\n\\n###\\n\\n')\n",
    "\n",
    "            print(f'Prompt:\\n{prompt}\\n')\n",
    "            print(f'Completions: {comp_users}')\n",
    "            print(f'Predictions: {high_users}')\n",
    "            test.append(comp_users)\n",
    "            pred.append(high_users.keys())\n",
    "\n",
    "print()\n",
    "print(f'Total Examples: {example_count} of {line_count} lines.')\n",
    "print(f'User example count: {user_example_count}')\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([['nil', 'ben', 'ryan', 'eric', 'jordan']])\n",
    "y_test_transformed = mlb.transform(test)\n",
    "y_pred_transformed = mlb.transform(pred)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_weighted = f1_score(y_test_transformed, y_pred_transformed, average='weighted')  # Or 'micro', 'weighted' based on need\n",
    "f1_macro = f1_score(y_test_transformed, y_pred_transformed, average='macro')\n",
    "\n",
    "print(f'F1 score (weighted): {f1_weighted}')\n",
    "print(f'F1 score (macro): {f1_macro}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
